{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "xie6nOwc44_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "6aj1GjDH46fk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNsf9UMw5D31",
        "outputId": "4f4e22e5-4b85-4725-dd8f-f791a0c72d1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0UgGqGi5Gk0",
        "outputId": "317274c0-0255-45ba-9833-905206d72254"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.23-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.7/521.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=9a3dc271dbb58881be6d3538cc4fd7ca138bc78d6e0dfe828249d33a593728dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.23 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.109.2 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.9.14 overrides-7.7.0 posthog-3.4.2 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.22-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.26 (from langchain)\n",
            "  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.9 langchain-community-0.0.22 langchain-core-0.1.26 langsmith-0.1.5 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install llama-index\n",
        "!pip3 install pypdf\n",
        "!pip3 install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpA8Lyxg5lgU",
        "outputId": "80817718-395c-42cc-906e-2213fb7f1461"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.12-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.5-py3-none-any.whl (25 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.12 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.12-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl (6.6 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.5-py3-none-any.whl (33 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
            "Collecting llama-index-vector-stores-chroma<0.2.0,>=0.1.1 (from llama-index-cli<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_index_vector_stores_chroma-0.1.4-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.12->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDF-1.23.25-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.3.4-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.12->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.14.1)\n",
            "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.4.23)\n",
            "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.17.0)\n",
            "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.15.2)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.12->llama-index) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.12->llama-index)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.12->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.12->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.7.0)\n",
            "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.12->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.12->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.12->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.12->llama-index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.12->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.12->llama-index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.109.2)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.27.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.4.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.4.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.60.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (29.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.9.14)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.12->llama-index) (23.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.12->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.12->llama-index) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.12->llama-index) (1.16.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.20.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.36.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (6.11.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.43b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (67.7.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.6)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (12.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.5.1)\n",
            "Installing collected packages: dirtyjson, pypdf, PyMuPDFb, httpcore, tiktoken, pymupdf, httpx, bs4, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-vector-stores-chroma, llama-index-program-openai, llama-index-question-gen-openai, llama-index-cli, llama-index\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed PyMuPDFb-1.23.22 bs4-0.0.2 dirtyjson-1.0.8 httpcore-1.0.4 httpx-0.27.0 llama-index-0.10.12 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.5 llama-index-core-0.10.12 llama-index-embeddings-openai-0.1.6 llama-index-indices-managed-llama-cloud-0.1.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.6 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.5 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-chroma-0.1.4 llama-parse-0.3.4 llamaindex-py-client-0.1.13 openai-1.12.0 pymupdf-1.23.25 pypdf-4.0.2 tiktoken-0.6.0\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.19.2-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.109.2)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.10.1 (from gradio)\n",
            "  Downloading gradio_client-0.10.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.1->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=69082fde062be0f12806ab4d7f6e2dd628ecb7c629391fb5528b78d7e7bdf9fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, colorama, aiofiles, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 12.0\n",
            "    Uninstalling websockets-12.0:\n",
            "      Successfully uninstalled websockets-12.0\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 ffmpy-0.3.2 gradio-4.19.2 gradio-client-0.10.1 pydub-0.25.1 python-multipart-0.0.9 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szVsPyJw54gz",
        "outputId": "00f4b866-fc8a-436c-c74e-d876b894a912"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.22)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.26)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CH59m85599t",
        "outputId": "38617ef1-4ba4-4e01-b136-a48cefcc6b52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = \"/content/drive/MyDrive/CMPE258_Assignment02/\""
      ],
      "metadata": {
        "id": "Cuh80HSx7nac"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "import os"
      ],
      "metadata": {
        "id": "CuWmdCzY7uR7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "90LShau47yPk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code**"
      ],
      "metadata": {
        "id": "pIQZgCyd71Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section encompasses the code that we wrote for this project. Please note that the section below is all the helper functions that we created for this project, and that each helper function will be documented in regards to what it does.\n",
        "\n",
        "## Helper Functions"
      ],
      "metadata": {
        "id": "gJdlSeCs754Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_filenames(directory):\n",
        "    try:\n",
        "        # List all files and directories in the given directory\n",
        "        filenames = os.listdir(directory)\n",
        "        # Filter out directories if you want only files\n",
        "        filenames = [file for file in filenames if os.path.isfile(os.path.join(directory, file))]\n",
        "        return filenames\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "5W8a0oY778MQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "def extract_text(file_path):\n",
        "  pdf_file_path = file_path\n",
        "  with open(pdf_file_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "  # Create a PDF object\n",
        "    pdf_text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      pdf_text += page.extract_text()\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "M9bnDGlK7-Xp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_all_PDF_files(file_list):\n",
        "  all_text = ''\n",
        "  full_directory_prefix = directory_path\n",
        "  for each_file_name in file_list:\n",
        "    print_name = full_directory_prefix + each_file_name\n",
        "    print(print_name)\n",
        "    all_text = all_text + extract_text(full_directory_prefix + each_file_name)\n",
        "  return all_text"
      ],
      "metadata": {
        "id": "KzW-202r8AUJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = list_filenames(directory_path)"
      ],
      "metadata": {
        "id": "g-VXKIT28CND"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eD9tbDd8ETl",
        "outputId": "2d0f0a81-6337-4e16-b59a-2bae2955eb19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['agriculture-13-00540.pdf', '1707.07217.pdf', '1807.11809.pdf', 'Deep_learning_review_and_discussion_of_its_future_.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = extract_text_from_all_PDF_files(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEk66EQB8GdN",
        "outputId": "998d9ac1-28fc-42b2-9bea-25f07e857c3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CMPE258_Assignment02/agriculture-13-00540.pdf\n",
            "/content/drive/MyDrive/CMPE258_Assignment02/1707.07217.pdf\n",
            "/content/drive/MyDrive/CMPE258_Assignment02/1807.11809.pdf\n",
            "/content/drive/MyDrive/CMPE258_Assignment02/Deep_learning_review_and_discussion_of_its_future_.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "\"\"\"\n",
        "function clean_text\n",
        "  input: text - text to be cleaner\n",
        "  output: returns text after performing the following actions:\n",
        "    1. lowercase text\n",
        "    2. remove special chars and numbers\n",
        "    3. Strip extra white spaces\n",
        "\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Function to clean the text data. This includes:\n",
        "    - Lowercasing the text\n",
        "    - Removing special characters and numbers\n",
        "    - Stripping extra white spaces\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "6AmdVmHu8MKu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "function tokenize_and_remove_stopwords\n",
        "input: text to remove stop words from\n",
        "output: returns the text without any stop words within the text.\n",
        "Please note that this only removes the english stopwords.\n",
        "\"\"\"\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    return [word for word in tokens if word not in stop_words]"
      ],
      "metadata": {
        "id": "HoGyaGot8NIk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLa_lcm38PHp",
        "outputId": "31b56b5d-8ee7-4d87-c67b-ca2ec488bb74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citation: Albahar, M. A Survey on\n",
            "Deep Learning and Its Impact on\n",
            "Agriculture: Challenges and\n",
            "Opportunities. Agriculture 2023 ,13,\n",
            "540. https://doi.org/10.3390/\n",
            "agriculture13030540\n",
            "Academic Editor: Jiangbo Li\n",
            "Received: 7 January 2023\n",
            "Revised: 6 February 2023\n",
            "Accepted: 11 February 2023\n",
            "Published: 23 February 2023\n",
            "Copyright: © 2023 by the author.\n",
            "Licensee MDPI, Basel, Switzerland.\n",
            "This article is an open access article\n",
            "distributed under the terms and\n",
            "conditions of the Creative Commons\n",
            "Attribution (CC BY) license (https://\n",
            "creativecommons.org/licenses/by/\n",
            "4.0/).\n",
            "agriculture \n",
            "Review\n",
            "A Survey on Deep Learning and Its Impact on Agriculture:\n",
            "Challenges and Opportunities\n",
            "Marwan Albahar\n",
            "College of Computer Science in Al-Leith, Umm Al Qura University, Mekkah 21955, Saudi Arabia;\n",
            "mabahar@uqu.edu.sa\n",
            "Abstract: The objective of this study was to provide a comprehensive overview of the recent ad-\n",
            "vancements in the use of deep learning (DL) in the agricultural sector. The author conducted a\n",
            "review of studies published between 2016 and 2022 to highlight the various applications of DL in\n",
            "agriculture, which include counting fruits, managing water, crop management, soil management,\n",
            "weed detection, seed classiﬁcation, yield prediction, disease detection, and harvesting. The author\n",
            "found that DL’s ability to learn from large datasets has great promise for the transformation of the\n",
            "agriculture industry, but there are challenges, such as the difﬁculty of compiling datasets, the cost of\n",
            "computational power, and the shortage of DL experts. The author aimed to address these challenges\n",
            "by presenting his survey as a resource for future research and development regarding the use of DL\n",
            "in agriculture.\n",
            "Keywords: agriculture; deep learning; crop management; weed detection\n",
            "1. Introduction\n",
            "In the contemporary era of globalization, the role and contributions of agriculture are\n",
            "crucially important. Over the years, agriculture has suffered from different challenges in\n",
            "fulﬁlling the ever-increasing needs of the world population, which has increased by twofold\n",
            "in the past ﬁve decades. There are different predictions for this unprecedented growth\n",
            "in the population, which is likely to reach approximately 9 billion people in the world\n",
            "by 2050 [ 1]. In addition, the estimates show an increase in the number of people living\n",
            "within urban areas, along with a signiﬁcant drop in the percentage of the population who\n",
            "are retired or working [ 1,2]. This means that agricultural productivity around the world\n",
            "must be increased considerably, and there is a need for a human labor force. To address\n",
            "this sort of problem, technologies such as tractors were introduced into agriculture over a\n",
            "century ago. At present, mechanical technology is showing an incredible evolution, and\n",
            "a signiﬁcant number of technologies are available. Existing technologies, such as remote\n",
            "sensing [ 3], robotic platforms [ 4], and the Internet of Things (IoT) [ 2], have recently become\n",
            "widespread in industry, particularly in the agricultural sector, leading to the phenomenon\n",
            "of smart and efﬁcient farming [ 5,6]. Schmidhuber (2015) states that deep learning (DL) is a\n",
            "modern approach that is successfully being utilized as part of various machine-learning\n",
            "techniques [ 7]. It is comparable to artiﬁcial neutral networks (ANNs) but with enhanced\n",
            "learning capabilities; therefore, it has higher accuracy [ 8]. In recent years, DL technologies,\n",
            "such as generative adversarial networks (GANs), recurrent neutral networks (RNNs), and\n",
            "convolutional neutral networks (CNNs), have been widely implemented and investigated\n",
            "in different ﬁelds of research, including farming and agriculture. Agriculturalists and\n",
            "researchers often use different software systems without assessing the mechanisms and\n",
            "ideas of the techniques, such as GANs, RNNs, and CNNs, that are usually applied in DL\n",
            "algorithms. There are various sub-categories of DL algorithms, including deep convo-\n",
            "lutional generative adversarial networks (DCGANs), very deep convolutional networks\n",
            "(VGGNets), and long short-term memory (LSTM) networks; therefore, understanding these\n",
            "Agriculture 2023 ,13, 540. https://doi.org/10.3390/agriculture13030540 https://www.mdpi.com/journal/agricultureAgriculture 2023 ,13, 540 2 of 22\n",
            "sub-categories of DL algorithms is important for understanding common DL algorithms [ 9].\n",
            "According to Kamilaris and Prenafeta [ 10], DL is a modern and recent technique and\n",
            "system for data analysis and image processing with signiﬁcant potential and promising\n",
            "results [ 10]. DL has been successfully used in different domains and has recently entered\n",
            "the agriculture ﬁeld. Additionally, it has the potential to address complex problems more\n",
            "efﬁciently and quickly due to the use of complex models that enable massive parallelization.\n",
            "These multifaceted models used in DL are likely to increase classiﬁcation accuracy and\n",
            "limit faults in regression problems, but only if there are sufﬁciently large databases that can\n",
            "be used to explain such problems.\n",
            "The authors of [ 11] argued that DL using drone technology is signiﬁcant for agriculture\n",
            "and farming as it provides a convenient way to monitor, assess, and scan crops through the\n",
            "use of high-quality and high-resolution images [ 11]. Such technology aids in recognizing\n",
            "advancements in ﬁelds and assessing quality. For example, with the help of images\n",
            "provided by drone technology, agriculturalists and farmers can determine whether crops\n",
            "are fully ready for harvesting. DL, along with machine-learning (ML) techniques, can help\n",
            "farmers understand the nature of the soil, thus aiding them in making timely decisions\n",
            "regarding farming. DL is also applied to assess how nutrients and water are to be managed\n",
            "and to make decisions about the suitable time for cropping and harvesting. Yields are\n",
            "higher and more efﬁcient and the return on investment (ROI) for crops can be projected,\n",
            "considering the margin and cost in the market [ 12]. In addition, the efﬁciency of DL is\n",
            "recognized because it has been observed to outclass conventional methods, such as support\n",
            "vector machines (SVMs), random forest (RF) algorithms, and ANNs. Several technologies\n",
            "are being used together with DL to enhance performance in prediction and classiﬁcation\n",
            "related to agricultural problems. The RNN and LSTM models have memory and time\n",
            "dimensions. As a result, they can be used to project animal and plant growth based on\n",
            "recorded data, evaluate water needs, and determine crop yield using the mining time\n",
            "dimensions and memory function [ 13,14]. They can also be utilized to estimate plant\n",
            "and animal growth based on previously recorded data to evaluate fruit yields or water\n",
            "requirements. Ren et al. [ 15], for example, used both models to forecast various phenomena\n",
            "and climate changes. Using hyperspectral images and infrared thermal imaging to provide\n",
            "data and information is the right direction for the prompt detection of diseases in crops.\n",
            "With the subsequent exponential growth in this ﬁeld, it is necessary to provide an up-to-date\n",
            "review of the recent literature focusing on innovative research techniques implementing\n",
            "DL for agriculture. Hence, this study focused on providing an overview of the recent\n",
            "advancements linked to DL in the agricultural sector. The study aimed to describe the\n",
            "use of DL in agriculture; in particular, with respect to counting fruits, managing water,\n",
            "crop management, soil management, weed detection, seed classiﬁcation, yield prediction,\n",
            "disease detection, and harvesting. The adoption of technology in the agriculture sector\n",
            "during the recent period has signiﬁcantly transformed and assisted farming and crop\n",
            "cultivation. However, DL has been noted to have added efﬁciency to agriculture, which\n",
            "has motivated researchers to investigate how it can be helpful in farming and harvesting\n",
            "and in yield predictions.\n",
            "This paper is categorized in the following way: Section 2 presents the research method-\n",
            "ology. Section 3 provides a literature review by highlighting a brief history of the topic.\n",
            "Section 4 highlights the importance of DL in the agriculture sector. Section 5 discusses DL\n",
            "tools that can be used for model development. In addition, the same section describes the\n",
            "usage, purpose, signiﬁcance, and implementation of DL in the agriculture sector. Section 6\n",
            "provides the results and a discussion, drawing on previous studies that have discussed\n",
            "deep learning. Section 7 presents the overall conclusion of the study, together with some\n",
            "ideas for future research directions.\n",
            "2. Research Method\n",
            "The methodology of this study was based on secondary data and a comprehensive re-\n",
            "view of approaches linked to agricultural DL, including disease detection, yield prediction,Agriculture 2023 ,13, 540 3 of 22\n",
            "and weed prediction, using databases such as Research Gate, IEEE Explore, Springer, Else-\n",
            "vier, Google Scholar, Frontier, and Science Direct. For this study, research papers published\n",
            "between 2016 and early 2022 were considered due to the increasing advancements in DL\n",
            "and its increasing use in agriculture. Additionally, the emphasis for data collection was on\n",
            "journal articles and conference papers. The inclusion criteria for research papers were that\n",
            "they were available in English with full access and were relevant to the research objectives\n",
            "with themes including development and agriculture. Studies that were published before\n",
            "2016 were excluded from this study. The research method ﬂowchart for this study is shown\n",
            "in Figure 1.\n",
            "Figure 1. The research methodology.\n",
            "3. Literature Review\n",
            "3.1. Deep Learning\n",
            "Agriculture faces many challenges due to the increase in demand and the presence\n",
            "of fewer workers in the ﬁelds. In this context, smart farming can be used to address\n",
            "issues such as food security, sustainability, productivity, and environmental impact [ 16].\n",
            "As is known, agriculture plays a vital role in the global economy [ 17]. This is because\n",
            "it ensures food security for countries, and most companies rely on it for their external\n",
            "trade. In the world today, most home appliances, travel means, and other commonly\n",
            "used services are becoming automated through the adoption of artiﬁcial intelligence (AI);\n",
            "thus, farming practices should too, since they are the backbone of a country. Achieving\n",
            "understanding and making quick responses with the help of data provided by continuous\n",
            "monitoring, measuring, and analysis of different physical aspects and phenomena would be\n",
            "helpful to overcome the complex, multivariate, and unpredictable challenges of agricultural\n",
            "ecosystems [ 18]. This would require the analysis of huge amounts of agricultural data\n",
            "and the use of new information and communication technologies (ICTs) and would be\n",
            "necessary for both small-scale farms and large-scale ecosystem monitoring [ 19]. It could\n",
            "be achieved using DL with a large network. DL is basically an aspect of machine learning\n",
            "that aims to build neural networks that can analytically learn by simulating the human\n",
            "brain. It acts like the human brain in that it works by reading data, such as pictures, videos,\n",
            "text, and sounds. With its continued development, DL has already been implemented in\n",
            "various complex tasks, such as image segmentation, image recognition, natural language\n",
            "processing, object detection, and image classiﬁcation [ 18]. However, DL requires a huge\n",
            "dataset since the quality of the DL results entirely depends on the size of the dataset, and\n",
            "the model tends to learn from that data and then respond accordingly. Some computer and\n",
            "industrial advancements, such as image processing, IoT technologies, robotics, machine\n",
            "learning, deep learning, and computer vision, are very useful in the agricultural industry\n",
            "and even for local farmers. High-quality image processing makes AI based on drone\n",
            "technology a very helpful asset for farmers, since they can identify the progress of the crops\n",
            "and determine whether they are ready to harvest or not while sitting in one place rather\n",
            "than having to move long distances. This has been achieved just AI and a drone system;\n",
            "one can only imagine how beneﬁcial it would be to implement DL in agriculture [18].\n",
            "Figure 2 lists numerous advantages of using DL in agriculture. With the current\n",
            "increase in the population, there has also been an increase in the demand for agricultural\n",
            "goods [ 18]. The implementation of DL and other automation components could greatly\n",
            "beneﬁt production outcomes, reduce the chances of ripening, reduce production costs,\n",
            "and increase income due to the increased production. Moreover, it would also makeAgriculture 2023 ,13, 540 4 of 22\n",
            "it possible to forecast climatic changes—for instance, if there is an incoming rainstorm,\n",
            "cyclone, etc.—so that the farmers could be ready and prepare before a disaster.\n",
            "Figure 2. Deep learning applications in agriculture [18].\n",
            "3.2. Agriculture before Deep Learning\n",
            "Traditional agriculture was the main method of farming before the adoption of sci-\n",
            "entiﬁc advancements in agricultural industries. Traditional agriculture mainly involves\n",
            "the extensive use of traditional tools and organic fertilizers, indigenous knowledge of land\n",
            "use and natural resources, cultural beliefs, etc. [ 19]. Another way to describe it could be\n",
            "as the “primitive style” or “early style” of farming and food production. The traditional\n",
            "method of farming affects the environment in many ways, such as through the depletion of\n",
            "soil nutrients. For instance, slash and burn practices in traditional agriculture were one\n",
            "reason for decreased soil organic matter. Another problem caused by traditional farming\n",
            "was deforestation, as most deforestation has taken place in tropical rainforests to make\n",
            "space for other agricultural activities. Another signiﬁcant issue relating to soil erosion is\n",
            "the removal of topsoil by water or wind. This topsoil is the most fertile, and it may take\n",
            "decades to replenish it once it is removed [19].\n",
            "Agroforestry, crop rotation, intercropping, polycultures, and water harvesting are\n",
            "some of the most common types of traditional farming practices. The traditional farming\n",
            "method of constructing grain storage structures provided an incredible moisture-proof\n",
            "environment for grain storage. Compared to today’s colossal warehouses, these small\n",
            "structures were cheap to fabricate and maintain. However, many different pesticides were\n",
            "used to keep the grains safe during storage periods, which later resulted in very bad effects\n",
            "on the environment [ 20]. The implementation of technology and the investment of huge\n",
            "sums of money in agricultural industries have helped to control other diseases, thereby\n",
            "making the amount of money spent worthwhile.\n",
            "3.3. Deep Learning Architecture\n",
            "Several nonlinear transformations are used to model higher-level abstractions in\n",
            "data, and these are the foundations of DL [ 21]. One of the main beneﬁts of DL is theAgriculture 2023 ,13, 540 5 of 22\n",
            "automatic extraction of features from raw data or feature learning. Producing features\n",
            "from lower-level components yields features in higher-level components [ 22]. Recurrent\n",
            "neural networks (RNNs) and CNNs are two types of DL networks that are often used\n",
            "in agriculture.\n",
            "3.3.1. Convolutional Neural Networks (CNNs)\n",
            "The CNN is a type of DL algorithm [ 23] composed of multiple convolutional layers,\n",
            "pooling layers, and fully connected layers. Two of the most common applications for CNNs\n",
            "are the recognition of handwritten characters and image processing. In the domain of\n",
            "computer vision, CNNs have been used for a variety of tasks, including object detection,\n",
            "image classiﬁcation, voice recognition, image fragmentation, medical image analysis, and\n",
            "text and video processing. Convolutional, pooling, and fully connected layers are the\n",
            "typical architectural components of a CNN [ 24]. Figure 3 depicts the architecture of a CNN,\n",
            "and brief descriptions of each layer are provided below.\n",
            "Figure 3. Convolutional neural network architecture.\n",
            "In a CNN, the convolutional layer is the most fundamental and signiﬁcant. It stores all\n",
            "of the images’ distinguishing characteristics while making it possible to limit the amount of\n",
            "data that must be simultaneously processed. Then, pooling enables a CNN to aggregate all\n",
            "the different dimensions of an image and recognize the object, even if its form is distorted or\n",
            "it is positioned at an angle. Thus, the number of learnable features in the model is reduced,\n",
            "helping to address the overﬁtting issue. Pooling can be accomplished in a variety of ways,\n",
            "including average pooling, maximum pooling, and stochastic pooling. The fully connected\n",
            "layer is the ﬁnal layer, which is used to feed the neural network [24].\n",
            "3.3.2. Recurrent Neural Networks (RNNs)\n",
            "An RNN is a type of neural network model capable of performing exceptionally\n",
            "well in fundamental tasks such as machine translation, language modeling, and speech\n",
            "recognition [ 25]. Unlike traditional neural networks, RNNs use the network’s sequential\n",
            "information. This feature is essential in many applications because the data sequence’s\n",
            "inherent structure contains valuable information that can be extracted from it. Figure 4\n",
            "depicts the fundamental structure of a recurrent neural network [25].\n",
            "Figure 4. Recurrent neural network generic structure.Agriculture 2023 ,13, 540 6 of 22\n",
            "4. Signiﬁcance of Deep Learning in Agriculture\n",
            "4.1. Counting of Fruit\n",
            "Counting fruit is an essential task for growers because it makes it possible to estimate\n",
            "the yield, which can be helpful in the management of yards. According to [ 26], counting\n",
            "fruit using automated fruit detection and algorithms can optimize agriculture production\n",
            "and help in managing the harvest process effectively. For automated fruit counting and\n",
            "detection, the authors provided a method that uses a pipeline for the DL algorithms\n",
            "consisting of part 0, part 1, part 2, and part 3. In part 0, the algorithms learn ground\n",
            "truths. They use the Bob detection neural network in part 1 and count the fruit through the\n",
            "neural network in part 2. In part 3, the linear regression is run for the ﬁnal count [ 26]. The\n",
            "authors of [ 27] proposed automatic yield estimation using robotic agricultural techniques\n",
            "to improve the manual counting of fruit. The authors used Inception-ResNet to achieve a\n",
            "high accuracy ratio with a lower computational cost as a deep simulated learning technique.\n",
            "The signiﬁcance of the technique proposed in [ 27] is that it does not require thousands of\n",
            "images to train the neural networks. Instead, the network can be trained with synthetic\n",
            "images to test the authenticity of images, achieving an accuracy rate of 91%. This novel\n",
            "DL method can facilitate farmers’ abilities to efﬁciently count fruit and make decisions\n",
            "with great precision [ 27]. Similarly, the authors of [ 28] trained a Fast R-CNN DL model\n",
            "to detect, count, and predict the right size for citrus fruit. The authors also used the long\n",
            "short-term memory detection method to calculate the number of fruit on each tree, as\n",
            "shown in Figure 5. Hence, DL methods, such as automated yield detection, DL simulation,\n",
            "and Fast R-CNN, can be helpful in the counting of fruit. Table 1 presents the most recent\n",
            "methods for counting fruits.\n",
            "Figure 5. Flowchart for Faster R-CNN [28].Agriculture 2023 ,13, 540 7 of 22\n",
            "Table 1. Summary of different DL methods for counting fruit.\n",
            "Ref DL model Dataset Accuracy\n",
            "[29] Faster R-CNN TL + ﬁeld farm 0.83 F1-score\n",
            "[30] Inception-ResNet-v4 ILSVRC-2010&2012 N/A\n",
            "[31] VGG-16 Orchard 95%\n",
            "[32] CNN Kiwifruit 89.29%\n",
            "[33] YOLO V3 PT + WGISD —\n",
            "[34] Faster R-CNN + Iv2 Cherries 85%\n",
            "[35] E-Net Fruit 360 93.7%\n",
            "[36] 8-layer CNN model Self collecting 95.67%\n",
            "[37] M-Net Mango orchard 73.6%\n",
            "[38] YOLO V3 PT + WGISD 97.3% for test\n",
            "4.2. Management of Water\n",
            "Water is an essential natural resource for agriculture that needs recycling for the\n",
            "continued and sustainable development of agriculture. The authors of [ 39] stated that\n",
            "water is essential for agriculture production but the chemicals from industries and the\n",
            "wastewater from daily usage increase water pollution. Therefore, the agriculture ﬁeld\n",
            "needs a DL technique to protect agriculture from water pollution. The authors of [ 39]\n",
            "proposed a near-infrared (NIR) spectroscopy method that can be used to assess water\n",
            "demand, protection, and recycling. The NIR system is used as one layer along with an\n",
            "improved convolutional neural network (CNN) layer that employs decision tree analysis\n",
            "to depict informative data helpful for making decisions relevant to water management.\n",
            "The authors of [ 40] posited that agriculture is the backbone of the economy in India and\n",
            "requires water as a signiﬁcant resource. Traditional irrigation methods waste water due to\n",
            "excessive water use and unplanned water management. Therefore, the authors provided\n",
            "an integrative approach that uses DL methods to improve the irrigation system in India’s\n",
            "agriculture (see Figure 6). The system consists of sensors that detect the soil’s humidity\n",
            "and predict the irrigation needs of the soil. The authors of [ 41] stated that water is a critical\n",
            "resource for which evapotranspiration assessment is very beneﬁcial. Evapotranspiration\n",
            "assessment uses DL techniques to predict upcoming water needs and to provide clues that\n",
            "can be helpful for real-time irrigation management. Thus, DL techniques can help farmers\n",
            "precisely manage their irrigation systems.\n",
            "Figure 6. Deep learning approach for water management [40].Agriculture 2023 ,13, 540 8 of 22\n",
            "4.3. Crop Management\n",
            "The signiﬁcance of DL frameworks is increasing in crop management, a subﬁeld of\n",
            "agriculture. The authors of [ 42] asserted that DL technology is beneﬁcial for crop planting.\n",
            "Crop planting is the ﬁrst step in crop production and needs to be managed efﬁciently to\n",
            "increase crop production. The authors discussed the various DL crop planting techniques,\n",
            "including ViSeed, which has been used for soybean production; Fast R-CNN, which has\n",
            "been used to count and measure the stalks of sorghum plants; CNN, which has been used\n",
            "for the identiﬁcation of localized features of roots and shoots; and VGG-16, which has\n",
            "been used for categorization of crops and weeds. According to the authors of [ 43], there\n",
            "are different types of deep learning networks that can be used for crop prediction. These\n",
            "different types include ANNs, for which the regression method and crop species and images\n",
            "and climatic and soil properties can be used to predict wheat, barley, sugarcane, sunﬂower,\n",
            "and potato crop production. Other DL techniques discussed in [ 43] include two-layered\n",
            "DNN LSTM, which has been used to predict tomato, soybean, and corn crop production\n",
            "using the regression method, a vegetative index, environmental characteristics, and soil.\n",
            "In addition, the authors of [ 44] stated that intelligence plays a crucial role in precisely\n",
            "managing crops. The authors introduced the CropDeep approach, which classiﬁes and\n",
            "detects different classes of crops. CropDeep provides crop management services through\n",
            "cameras and models; it classiﬁes crops and provides analysis that is helpful for decision\n",
            "making and includes real-world challenges; e.g., weather uncertainty (see Figure 7). Table 2\n",
            "presents the most recent methods for crop management.\n",
            "Figure 7. CropDeep deep-learning detection and classiﬁcation models [44].\n",
            "Table 2. Summary of different DL methods for crop management.\n",
            "Ref DL Model Application Accuracy\n",
            "[45,46] FCN architecture/Stem-seg-SJoint stem detection and\n",
            "crop/weed classiﬁcationmAP , 85.4% (stem detection)\n",
            "69.7% (segmentation)\n",
            "[47] AgroAVNET Crop/weed classiﬁcation 98.23%\n",
            "[48]AlexNet, VGG-19, GoogLeNet,\n",
            "ResNet-50, ResNet-101,\n",
            "Inception-v3Crop/weed classiﬁcation 96% (VGG-19)\n",
            "[49] 1D/2D/3D CNN Crop mapping 94% (3D CNN)\n",
            "4.4. Soil Management\n",
            "Soil management refers to the practices, operations, and treatments that protect soil\n",
            "and increase an agricultural ﬁeld’s production. The authors of [ 50] stated that DL techniques\n",
            "can help manage soil moisture. According to the authors, developing a mathematical modelAgriculture 2023 ,13, 540 9 of 22\n",
            "is difﬁcult for soil moisture, so the accuracy, predictions, and generalization of existing\n",
            "models could be improved. The authors improved the DL regression model by ﬁtting it\n",
            "with large datasets, making it possible to precisely determine soil moisture.\n",
            "The authors of [ 51] proposed that agriculture has been an essential aspect of the lives of\n",
            "human beings since even before civilization. Soil yield plays a key role in crop production\n",
            "and efﬁcient agriculture. Therefore, the authors discussed how implementing the Keras\n",
            "API in Python can help protect soil from herbicide toxicity while also retaining moisture.\n",
            "Moreover, using a ﬁrst-order agriculture simulator that employs discrete time, the Richard\n",
            "equation can help determine the precise moisture level in soil [ 52]. The authors explained\n",
            "that using an agriculture simulator can help in obtaining aerial images with a particular soil\n",
            "moisture information dataset. The dataset was analyzed using seven methods, including\n",
            "constant prediction baseline, SVM, and NN, which showed that using a CNN could reduce\n",
            "water consumption by 52% [ 52]. Thus, the authors showed that DL techniques can help\n",
            "maintain soil moisture.\n",
            "4.5. Weed Detection\n",
            "Weeds are undesirable plants that can reduce crop production. DL techniques can\n",
            "help detect weeds. According to [ 51], a “weed” is a plant that grows in an unfavorable\n",
            "environment. They have negative effects on crop production because they compete with\n",
            "plants for water, sunlight, and soil minerals. Weed detection can be undertaken by using\n",
            "DL techniques, such as ﬁrst-order agriculture simulation with Richard’s equation. This\n",
            "approach reduces the use of weedicides, thus protecting the soil from toxicity and ensuring\n",
            "the plants achieve suitable production yields. In addition, the authors of [ 53] raised the\n",
            "concern that the production and utilization of herbicides have made weeds resistant to\n",
            "these herbicides. Therefore, developing precision techniques to detect weeds is important\n",
            "to increase crop production. Researchers have discussed the revolution in computing\n",
            "technology and how it can help in better understanding weed biology and ecology. DL is\n",
            "the most helpful technique, as it aids in categorizing weeds in crop categories and getting\n",
            "rid of them. In a similar context, the authors of [ 54] found that DL techniques, such as\n",
            "classiﬁcation SVMs and CNNs, can reduce the burden on farmers. The techniques can help\n",
            "farmers detect weeds. The authors described various weed detection and categorization\n",
            "techniques. In such techniques, the camera ﬁrst takes images of weeds and then uses a gray-\n",
            "level occurrence matrix to identify homogeneity among the images. The color identiﬁed\n",
            "through the hue saturation value (HSV) describes the mellowness of the weeds, as shown\n",
            "in Figure 8. Hence, DL techniques are helpful for weed detection, reducing the burden on\n",
            "farmers and increasing crop yield.\n",
            "4.6. Seed Classiﬁcation\n",
            "In the agriculture sector, crop production strongly relies on seeds. According to [ 55],\n",
            "seeds are a signiﬁcant part of crop production, without which production and harvesting\n",
            "of crops are impossible. The increased level of population growth has put pressure on\n",
            "agriculture growth due to the precision needed when identifying and classifying seeds.\n",
            "The authors proposed a CNN to increase the efﬁciency of the classiﬁcation of seeds. In this\n",
            "technique, the methods used included decayed learning points.\n",
            "4.7. Classiﬁcation of Plant Diseases\n",
            "Plants can produce decreased crop yields due to the presence of fungi, microbes, and\n",
            "bacteria. If the disease is not diagnosed in time, it can cause signiﬁcant economic losses\n",
            "for farmers. The pathogen-killing pesticides that are used to restore crop functionality\n",
            "and remove pests come at a high cost. Excessive use of pesticides is detrimental to the\n",
            "environment and can disrupt the water and soil cycles [ 56]. As plant diseases stunt growth,\n",
            "identifying them in their early stages is essential. DL models have been applied in the\n",
            "process of recognizing and categorizing various plant diseases. Several DL architectures\n",
            "have been proposed to improve detection accuracy [ 57]. In [ 58], the authors proposed aAgriculture 2023 ,13, 540 10 of 22\n",
            "method for identifying and categorizing banana diseases based on a CNN. The proposed\n",
            "model processes pictures of leaves to help farmers detect two banana diseases (sigatoka\n",
            "and speckle) quickly. In addition, the authors of [ 59] used AlexNet to accurately classify\n",
            "plant diseases based on leaf images. The DL hybrid model described in [ 60] identiﬁed\n",
            "and categorized diseases that affect sunﬂowers, such as Verticillium wilt, Phoma rot,\n",
            "downy mildew, and Alternaria leaf rot. The authors of [ 61] developed a mobile app that\n",
            "utilizes machine learning to diagnose diseases that affect plant leaves. The app can classify\n",
            "38 different diseases. The authors amassed 96,206 images, including both healthy and\n",
            "diseased plant leaf specimens, for training, testing, and validation of the model. In [ 62], the\n",
            "authors proposed a pre-trained, transfer-learning deep neural network model that could\n",
            "predict crop disease by learning leaf characteristics from input data. They extensively\n",
            "investigated different DL and CNN topologies, such as ResNet, MobileNet, Wide ResNet,\n",
            "and DenseNet. The result showed that the proposed method was superior in terms of\n",
            "accuracy and memory to previous methods in the literature. Another CNN-based method\n",
            "for detecting, classifying, and identifying plant diseases was proposed in [ 63]. The accuracy\n",
            "for the identiﬁcation of 13 different plant diseases ranged from 91 to 98%. Furthermore, the\n",
            "proposed model was able to distinguish between unhealthy and healthy leaves, as well as\n",
            "their backgrounds. Using 500 different leaf images, the authors of [ 64] proposed a model\n",
            "based on an SVM classiﬁer. The proposed model could accurately identify plant diseases,\n",
            "achieving an accuracy of 94%. Table 3 presents the most recent methods for classifying\n",
            "plant diseases.\n",
            "Figure 8. Flowchart for weed detection and classiﬁcation [54].Agriculture 2023 ,13, 540 11 of 22\n",
            "Table 3. Summary of different DL methods for classifying plant diseases.\n",
            "Ref. Leaf Type Method Accuracy\n",
            "[65] Rice VGGNet 92.00\n",
            "[66] Tomato S-CNN and F-CNN 98.30\n",
            "[67] Plant leaf EfﬁcientNet 96.18\n",
            "[68] Grape Hy-CNN 98.70\n",
            "[69] Grape United model 98.20\n",
            "[70] Plant leaf Whale and DL 95.10\n",
            "[71] Crop FCNN and SCNN 92.01\n",
            "[72] Coffee Deep CNN 98.00\n",
            "4.8. Yield Prediction\n",
            "It is essential to focus on and carefully manage yield predictions for each crop. Agri-\n",
            "cultural machine-learning and DL algorithms are primarily concerned with crop yield\n",
            "prediction. They inform the farmer about whether the crop is ready for cultivation and\n",
            "predict when it will be [ 73]. Manjula et al. [ 74] proposed a model based on an RF classiﬁer\n",
            "that could predict millet crop yield with an accuracy of 99.74%. The prediction of crop\n",
            "yield is notoriously difﬁcult due to the presence of numerous complex factors. For example,\n",
            "high-dimensional marker data are required to represent genotype information, consisting\n",
            "of the data for millions of markers for each individual plant. The ﬁnal effect of the genetic\n",
            "markers must be estimated because it is affected by numerous environmental conditions\n",
            "and ﬁeld management techniques. Recently, a variety of machine-learning models, includ-\n",
            "ing association rule mining, ANNs, decision trees, and multivariate regression, have been\n",
            "applied in the ﬁeld of crop yield prediction. The most notable characteristic of ML and DL\n",
            "may be that the output is treated as an implicit function of the input variables and may be\n",
            "a highly nonlinear and complex function [75].\n",
            "Extensive research has been conducted on crop yield prediction. Liu et al. [ 76] em-\n",
            "ployed a neural network with one hidden layer for the prediction of corn yield using input\n",
            "data on weather, soil, and management. In the same context, Drummond et al. [ 76] worked\n",
            "on predicting crop yield by using neural networks, projection pursuit regression, and\n",
            "stepwise multiple linear regression. As a result, they found that both regression methods\n",
            "were outperformed by the neural network method. In addition, Marko et al. [ 76] predicted\n",
            "the crop yields of different soybean varieties by using weighted histogram regression. They\n",
            "obtained better performance than the conventional regression algorithms.\n",
            "4.9. Disease Detection\n",
            "In the agricultural ﬁeld, one major threat to farmers is crop disease. With the devel-\n",
            "opments in the ﬁelds of AI and DL and their implementation in agricultural industries,\n",
            "crop disease detection has become one of the easiest processes. Before the adoption of\n",
            "advanced technology in agriculture, the detection of diseases in crops at an early stage was\n",
            "a time-consuming and tedious task that had to be performed manually [ 77]. Plant disease\n",
            "not only affects plant growth and the population but also seriously affects the economy\n",
            "of countries. Hence, it is essential to adopt automatic and accurate techniques for the\n",
            "prediction and detection of plant disease severity for disease management and food safety\n",
            "and to predict losses in returns. In most developing countries, farmers are usually required\n",
            "to travel huge distances to contact experts, which can consume huge amounts of money\n",
            "and time [ 76]. This could be handled by developing a robust and easy-to-use plant or crop\n",
            "disease detection system, which would require many sample images of crops that have\n",
            "diseases that could be uploaded to the cloud, and the system could run on IoT devices, such\n",
            "as smartphones and tablet PCs with appropriate computational capabilities. Some work\n",
            "has been done in order to tackle this issue of crop diseases. Nikhil Patil et al. [ 78] proposedAgriculture 2023 ,13, 540 12 of 22\n",
            "a crop disease detection system using a CNN. The system achieved an accuracy rate of 89%\n",
            "compared to the traditional crop disease detection system. Hence, when it comes to image\n",
            "processing, CNN systems can be relied upon due to the fact that they are widely used in\n",
            "agricultural research. Most DL applications in agriculture can be categorized as plant or\n",
            "crop classiﬁcation, which is important for disaster monitoring, robotic harvesting, pest\n",
            "control, and yield prediction. Plant and crop disease recognition models are mostly based\n",
            "on pattern recognition and leaf images [ 78]. Hence, DL and AI models could automatically\n",
            "determine which plants are diseased and send alerts to the farmer for early action. Figure 9\n",
            "shows an example of how DL and AI can detect plant diseases.\n",
            "Figure 9. Plant disease detection [79].\n",
            "5. Application of Deep-Learning Models in Agriculture\n",
            "According to [ 80], there are different points of view about the creation of DL tools\n",
            "for model development. Python tools are used to emphasize the concept of saliency in\n",
            "images. Saliency is typically deﬁned by unique features, including pixels, or the resolution\n",
            "of the image in visual processing. Another DL tool is the gradient explanation technique,\n",
            "which employs the gradient-based attribution method. In this method, each gradient\n",
            "quantiﬁes each of the input dimensions that can change the predictions around the input.\n",
            "The integrated gradient is a gradient-based attribution that allows predictions to be formed\n",
            "by a deep neural network. It is created via attributions related to the network’s input\n",
            "features [ 81]. Deep label-speciﬁc feature (DeepLIFT) is another tool designed to ensure\n",
            "the accuracy of deep neural network predictions. It is also known as the gradient + input\n",
            "method, and it is used to enhance the gradient with the input signal. It has quite a few\n",
            "advantages over gradient-based methods, especially when it is implemented into models\n",
            "that are usually trained with natural images and genomics data [ 82]. Typically, activation\n",
            "of each neuron takes place with reference to the contribution scores calculated by the\n",
            "system. These calculations are based on comparisons between various outputs and a\n",
            "certain benchmarked output and the differences in the inputs from their reference inputs.\n",
            "Another model is guided backpropagation, also known as guided saliency, which is a\n",
            "type of deconvolution approach. This tool and approach is often employed in a range of\n",
            "network structures, such as in max pooling in CNNs [ 83]. The purpose is to substitute the\n",
            "max-pooling layers with a convolutional layer. Similarly, the authors of [ 84] claim that\n",
            "deconvolution is a technique for visualizing CNNs that uses quite similar aspects, such as\n",
            "deconvolutional networks. Furthermore, the authors of [ 85] proposed class activation maps\n",
            "(CAMs) for the identiﬁcation of images. With this tool, analysts may inspect a particular\n",
            "image and then its speciﬁc parts or pixels are used to form the ﬁnal output. In otherAgriculture 2023 ,13, 540 13 of 22\n",
            "words, CAMs are used to study the discriminative regions of an image, as with a CNN. A\n",
            "ﬁnal softmax-loss layer is formed after obtaining the weighted sum of the vector. Finally,\n",
            "layer-wise relevance propagation (LRP) is a tool for decomposing nonlinear classiﬁers that\n",
            "aims to improve DL interpretability [ 86]. It is based on deep neural networks formed by\n",
            "propagating predictions backward, fulﬁlling the requirement for the conservation property.\n",
            "All the abovementioned DL tools are currently available for model development.\n",
            "According to [ 87], the deep neural network (DNN) can be employed using a CNN for\n",
            "the assessment of the quality of seeds in agriculture. The model can be used to study the\n",
            "quality of seeds in soybean pods, along with the sorting of haploid seeds. Assessments\n",
            "of the shape, phenotypic expression, and embryonic pose are undertaken [ 88]. CNNs\n",
            "have also been used in the classiﬁcation plant seedlings into 12 species. Furthermore, the\n",
            "authors of [ 89] used an image analysis technique to create a principal component analysis\n",
            "(PCA) that can be used to place seeds in different clusters in a cost- and time-effective\n",
            "manner. The authors of [ 90] claimed that DL algorithms, such as Inspection-v3, VGG-16,\n",
            "and VGG-19, are more efﬁcient in citrus plant disease detection than other innovations.\n",
            "In [91], the authors claimed that DL methods aid in the identiﬁcation of plant diseases\n",
            "from individual lesions and spots. This makes it possible to focus on other aspects rather\n",
            "than only considering the entire leaf in disease detection. This DL application is good\n",
            "at detecting multiple diseases on the same leaf and provides 12% greater accuracy. DL\n",
            "methods can also be used to identify various plant diseases [ 92]. The authors of [ 93]\n",
            "claimed that apple leaf and fruit diseases can be detected using a CNN model, implying\n",
            "that the use of DL models for disease detection is quite effective. There are also harvesting\n",
            "techniques that use DL. The authors of [ 94] formed a shot-detector (YOLO) algorithm for\n",
            "on-tree fruit detection and used the BBox-Label-Tool to label images. Likewise, two deep\n",
            "learning models were utilized for images of pears and apples fruits, and it was found that\n",
            "the deep learning models were quite effective for harvesting purposes. The authors of [ 95]\n",
            "revealed that having a robust DL model can be helpful in the harvesting process, as it\n",
            "showed promising results due to its employment of bio-inspired features. As shown by\n",
            "these studies, DL is becoming one of the most useful techniques and models in harvesting\n",
            "since it employs mature features in comparison to other agricultural techniques. This recent\n",
            "survey also shows that CNNs have promising results in agriculture and increase efﬁciency.\n",
            "In other words, CNNs have increased accuracy and improved learning capacities when DL\n",
            "mechanisms are employed in agriculture.\n",
            "6. Results and Discussions\n",
            "The ﬁndings from the studies show that DL mechanisms have helped farmers in\n",
            "different areas of agricultural production. These include counting fruit, management\n",
            "of water, crop management, soil management, weed detection, seed classiﬁcation, yield\n",
            "prediction, disease detection, and even harvesting. A summary of the key ﬁndings is\n",
            "presented in Table 4.\n",
            "Table 4. Summary of different DL methods used in agriculture.\n",
            "Ref Method Used Purpose of Employing Method Key Insights\n",
            "[20]Automated fruit detection and\n",
            "algorithms using a DL algorithm\n",
            "pipeline consisting of part 0, part 1,\n",
            "part 2, and part 3Counting fruitOptimization of agriculture production\n",
            "Promising harvesting results\n",
            "[27] Inception-ResNet Counting fruitProvides high accuracy in the counting of fruit\n",
            "Uses synthetic images to test authentic images,\n",
            "achieving a 91% accuracy rate\n",
            "[26] Near-infrared (NIR) spectroscopy Management of waterIncreased water protection and recycling\n",
            "Provides information that helps make effective\n",
            "decisions in water managementAgriculture 2023 ,13, 540 14 of 22\n",
            "Table 4. Cont.\n",
            "Ref Method Used Purpose of Employing Method Key Insights\n",
            "[41] Evapotranspiration Management of waterAllows the prediction of water speciﬁcations\n",
            "for real-time irrigation management\n",
            "[42]R-CNN for counting and measuring\n",
            "crop plantingsCrop managementThe CNN helps in identifying localized\n",
            "features of roots and shoots\n",
            "CGG-16 allows categorization of crops\n",
            "and weeds\n",
            "[43] Two-layered DNN LSTM Crop managementHighlights soil and environmental\n",
            "characteristics\n",
            "Prominent vegetative index used to create\n",
            "estimations of crop production for tomato,\n",
            "soybean, and corn\n",
            "[51] Keras API through Python Soil managementHelps in preventing the harmful effects of\n",
            "herbicides and toxicity in soil and\n",
            "in retaining moisture\n",
            "[52]First-order agriculture simulator using\n",
            "discrete timeSoil managementImproves aerial images and provides soil\n",
            "moisture information\n",
            "[51]First-order agriculture simulation\n",
            "with Richard equationWeed detectionIncreases protection of soil from toxicity and\n",
            "ensures plants achieve good production yields\n",
            "[54] SVM and CNN Weed detectionThe camera is used to take an image of a weed,\n",
            "and then the gray-level occurrence matrix is\n",
            "employed to determine the homogeneity\n",
            "among the images\n",
            "Reduces burden on farmers\n",
            "[55] CNN Seed classiﬁcation Increased efﬁciency in seed classiﬁcation\n",
            "[74] Random forest (RF) Yield prediction Provides the best yield prediction accuracy\n",
            "[76] Histogram regression Yield predictionOffers accuracy in the determination of\n",
            "soybean varieties\n",
            "[78] CNN Disease detectionAchieved an 89% accuracy rate when\n",
            "compared to other traditional crop disease\n",
            "detection methods\n",
            "Improves pest control and makes robotic\n",
            "harvesting possible with increased yield\n",
            "prediction and disaster monitoring\n",
            "abilities for crops\n",
            "[95] Bio-inspired methods HarvestingIncreases harvesting efﬁciency\n",
            "Improves accuracy for harvesting in\n",
            "agriculture\n",
            "[96]Canopy-attention-YOLOv4 Fruit detectionPrecision = 94.89%\n",
            "Recall = 90.08%\n",
            "F1 = 92.52%\n",
            "[97] YOLOv5-CS (citrus sort)Fruit detection and\n",
            "countingRecall = 97.66%\n",
            "Precision = 86.97%\n",
            "mAP = 98.23%\n",
            "The key ﬁndings from the literature review show that there are various ways in which\n",
            "DL has beneﬁted the agriculture industry. Agriculture faces numerous barriers as a result of\n",
            "increased demand and fewer workers in the sector. However, the implementation of smart\n",
            "farming can help address issues such as productivity, environmental impact, food security,\n",
            "and sustainability and increase the efﬁciency of agricultural production [ 16]. Agriculture,\n",
            "as is well-known, plays an important part in the global economy [ 17] as it ensures food\n",
            "security for regions and is used by the majority of businesses for external commerce. The\n",
            "implementation of deep-learning methods has helped the agricultural sector grow and\n",
            "develop, employing the latest prediction analyses and tools. There are various tools that\n",
            "have been used by scholars to prove the efﬁciency of deep-learning methods in agriculture.\n",
            "According to [ 18], the size of the dataset used for deep-learning methods may determine\n",
            "the quality of the results. The accuracy of the results obtained using deep learning in\n",
            "agricultural production and processes may lead to improved decisions. Traditional farmingAgriculture 2023 ,13, 540 15 of 22\n",
            "practices result in a wide range of environmental consequences, including soil nutrient\n",
            "depletion. Deforestation is another issue produced by traditional farming, as most defor-\n",
            "estation has occurred in tropical rainforests to make room for other agricultural activities.\n",
            "Soil erosion, which occurs as a result of the erosion of topsoil by water or wind, is another\n",
            "issue associated with traditional farming. This topsoil is the most productive portion of the\n",
            "soil and, once removed, it can take decades to restore it [ 19]. This implies that traditional\n",
            "agricultural production methods are insufﬁcient to promote the efﬁciency of the sector.\n",
            "Agriculture must become more brilliant and progressive in order to meet future demands\n",
            "and utilize emerging technologies, such as deep learning, remote sensors, and distributed\n",
            "computing [ 98]. The current study’s key ﬁndings highlight that the use of various DL tools\n",
            "in agriculture has improved farming outcomes and production. According to [ 95], the\n",
            "real parameters for agriculture are now obtained through cutting-edge technology and DL\n",
            "mechanisms. DL methods have proven their ability to increase efﬁciency in agriculture,\n",
            "showing improved results and accuracy in all domains. Despite the employment of DL\n",
            "mechanisms in agriculture, it was identiﬁed that there are also certain challenges that\n",
            "accompany the use of this technology, such as dataset creation, the time required to train\n",
            "staff, and having skilled labor to increase production. Furthermore, system development\n",
            "and hardware maintenance, as well as the deployment of large models and software on\n",
            "small devices, such as mobile phones, may have an impact on system efﬁciency [ 99]. More-\n",
            "over, developing awareness among staff members when DL methods are employed is\n",
            "also challenging in agriculture. According to [ 100], transfer learning is a technique that\n",
            "can be employed to minimize the emerging challenges that may arise when employing\n",
            "deep learning in agriculture. It is typically used to address problems when there is a\n",
            "small dataset and minimal time is required to test the accuracy of the model. As shown\n",
            "in [101,102], AI is useful to minimize the challenges affecting agricultural production when\n",
            "DL and robotics are used. In the same context, automated machine learning (AutoML) is\n",
            "another technique that helps increase agriculture production through innovation. When\n",
            "DL methods are employed in agricultural production mechanisms, AutoML can be use-\n",
            "ful to minimize the challenges. Thus, the literature shows that DL methods have been\n",
            "extremely useful in increasing production in the agricultural sector. However, in order\n",
            "to minimize the challenges that come with employing DL techniques, it is important to\n",
            "consider employing other emerging technologies, such as robotics, the Internet of Things,\n",
            "and distributed computing.\n",
            "It was found that most DL-based farming techniques use very simple algorithms and\n",
            "network structures. The primary cause of this is that the combination of deep learning and\n",
            "precision agriculture is still in its infancy. The lack of collaboration between the computer-\n",
            "science and agriculture communities is also a contributing factor. Table 1 shows that many\n",
            "of the deep-learning algorithms tested had an accuracy of 90% or more with some datasets,\n",
            "but it should be noted that these results are not generalizable. The accuracy and speed\n",
            "of these networks typically fall short of the benchmarks when they are employed with\n",
            "other datasets or a real farmland environment. This is mainly due to the fact that the\n",
            "complexity, quality, and quantities of agricultural datasets are still very different from\n",
            "actual farmland environments. Many novel approaches have been proposed to lessen\n",
            "the reliance of DL models on agricultural datasets, including transfer learning [ 103,104],\n",
            "few-shot learning [ 105,106], graph convolutional networks [ 107], and semi-supervised\n",
            "learning [ 108]. However, their entire performances are still unavailable. Only a few\n",
            "recent studies have focused on tailoring deep-learning algorithms and neural network\n",
            "architectures to the needs of agricultural applications. Some studies, for instance, have\n",
            "aimed to ﬁnd the most effective ways to optimize the parameters used in DL models.\n",
            "Enhancing DL algorithms and frameworks has been the focus of other research. With the\n",
            "aim of large-scale dense semantic segmentation of weeds using aerial images, the authors\n",
            "of [109,110] presented WeedMap and WeedNet. They made changes to the decoder that\n",
            "allowed them to use a modiﬁed version of the VGG16 architecture in place of the original\n",
            "encoder. Jiao et al. [ 111] created an anchor-free convolutional neural network (AF-RCNN)Agriculture 2023 ,13, 540 16 of 22\n",
            "to achieve a balance between speed and accuracy in deep-learning algorithms applied\n",
            "to the detection of multiclass agricultural pests. To improve recognition accuracy in leaf\n",
            "disease detection, the authors of [ 112] utilized convolutional neural networks (CNNs) and\n",
            "pre-trained models to identify plant diseases. The study focused on ﬁne-tuning popular\n",
            "pre-trained models, such as DenseNet-121, ResNet-50, VGG-16, and Inception V4, using\n",
            "the PlantVillage dataset, which contains 54,305 images of plant diseases in 38 classes . The\n",
            "performance of the models was evaluated through various metrics. The results showed that\n",
            "DenseNet-121 achieved the highest classiﬁcation accuracy of 99.81%, outperforming other\n",
            "state-of-the-art models. In the same context, the authors of [ 113] proposed a new method for\n",
            "data augmentation utilizing generative adversarial networks (GANs) for tomato leaf disease\n",
            "recognition. By utilizing deep convolutional generative adversarial networks (DCGANs) to\n",
            "augment the original images and GoogLeNet as the input, the proposed model was able to\n",
            "achieve the top average identiﬁcation accuracy of 94.33%. The model was further improved\n",
            "by adjusting the hyper-parameters, modifying the architecture of the convolutional neural\n",
            "networks, and experimenting with different GANs. The use of DCGAN to augment\n",
            "the dataset not only increased its size but also improved its diversity, leading to better\n",
            "generalization of the recognition model. In addition, the authors of [ 114] proposed the use\n",
            "of a deep convolutional generative adversarial network (DCGAN) to augment an original\n",
            "dataset and trained a convolutional neural network (CNN) in the task of regression by\n",
            "utilizing the DCGAN to generate synthetic images that were realistic enough to be included\n",
            "in the training set. They employed a two-stage scheme where the baseline CNN, trained\n",
            "with the original dataset, was utilized to predict the regression vectors for each image\n",
            "generated by the DCGAN. These regression vectors served as the ground truth for the\n",
            "augmented dataset, enabling the CNN to make more accurate predictions.\n",
            "7. Future Challenges and Opportunities in the Agricultural Domain\n",
            "Deep learning has the potential to revolutionize the agricultural industry by enabling\n",
            "more efﬁcient crop production, precision agriculture, and improved crop monitoring and\n",
            "forecasting. However, there are several challenges that need to be addressed to fully\n",
            "realize the potential of deep learning in agriculture. One major challenge is the lack of\n",
            "high-quality labeled data in the ﬁeld of agriculture. This can be addressed by developing\n",
            "new data collection methods and creating large labeled datasets that can be used to train\n",
            "deep-learning models [ 114]. Another challenge is the high computational cost associated\n",
            "with deep learning, which can make it difﬁcult to implement these models in resource-\n",
            "constrained environments, such as rural areas [115].\n",
            "Additionally, there is a need for deep-learning models in agriculture to attain robust-\n",
            "ness and adaptability to different environments, crop types, imaging conditions, and sensor\n",
            "modalities. This requires the development of models that can be generalized well across\n",
            "different scenarios and are robust to variations in the data [ 116,117]. Furthermore, as the\n",
            "data in the agricultural domain is often incomplete, noisy, or corrupted, there is a need for\n",
            "methods that can handle missing or incomplete data [ 118]. In recent years, research on\n",
            "robust deep-learning techniques, such as robust optimization, adversarial training [ 118],\n",
            "and meta-learning [ 119,120], has been undertaken to address this challenge, but more\n",
            "research is still needed in this area.\n",
            "Another important challenge is the need for deep-learning models in agriculture\n",
            "to be interpretable and explainable, as this is essential for decision making and to gain\n",
            "trust from stakeholders. There is a growing interest in developing methods that can\n",
            "provide insight into the inner workings of deep-learning models, such as explainable AI\n",
            "techniques—for instance, Local Interpretable Model-Agnostic Explanations (LIME) [ 121]\n",
            "and Shapley Additive Explanations (SHAP) [ 122]—and interpretable deep learning—for\n",
            "instance, decision trees and rule-based systems [ 123]. Moreover, the integration of multiple\n",
            "modalities of data, such as image, sensor, and weather data, is crucial to improving the\n",
            "performance of deep-learning models in agriculture. There is a need for multistream neuralAgriculture 2023 ,13, 540 17 of 22\n",
            "networks, such as those based on attention [ 124], that can handle multiple modalities of\n",
            "data and provide a more comprehensive understanding of the agricultural system.\n",
            "Few-shot learning is a form of machine learning that allows models to be generalized to\n",
            "previously unseen classes with a small number of examples. This is beneﬁcial in agriculture,\n",
            "as it allows such models to learn more quickly and accurately from a limited amount of\n",
            "data. Furthermore, it is more data-efﬁcient, meaning that fewer data points are required to\n",
            "train the model [125].\n",
            "In conclusion, deep learning has the potential to revolutionize the agricultural industry,\n",
            "but there are several areas that need to be addressed to fully realize its potential. These\n",
            "include the need for robustness, interpretability, integration of multiple modalities of data,\n",
            "and few-shot learning. Further research in these areas is crucial to overcoming these\n",
            "challenges and fully harnessing the power of deep learning in agriculture.\n",
            "8. Conclusions and Future Work\n",
            "The primary aim of the present study was to provide an overview of the recent ad-\n",
            "vancements linked to DL in the agricultural sector. This study employed a comprehensive\n",
            "review of approaches linked to agricultural DL, including disease detection, yield predic-\n",
            "tion, and weed prediction, in studies published between 2016 and early 2022. The key\n",
            "ﬁndings of this paper indicated that a range of DL tools, employed for counting fruit,\n",
            "managing water, crop management, soil management, weed detection, seed classiﬁcation,\n",
            "yield prediction, disease detection, and even harvesting, have been identiﬁed. It was also\n",
            "revealed that, despite the employment of DL processes in agriculture, there are certain\n",
            "obstacles that come with the use of this technology. It involves difﬁculties such as the\n",
            "compilation of datasets, the time necessary for staff training, and the presence of DL experts\n",
            "to ensure high production. Furthermore, system development and hardware requirements,\n",
            "as well as the deployment of large models and software on small devices, such as smart-\n",
            "phones, may have impacts on system efﬁciency. Thus, it can be concluded that, with the\n",
            "support of the most recent prediction analyses and tools, the application of DL methods\n",
            "has assisted the agriculture industry in growing and developing. It can be noted that DL\n",
            "has great potential in the ﬁeld of agriculture. However, the high cost of hardware and\n",
            "software makes its application very limited. Therefore, there is a need to work on research\n",
            "and development around cost-effective DL methods in the future to enhance large-scale\n",
            "applications. Furthermore, DL methods are limited in terms of accuracy and effectiveness.\n",
            "Thus, more research is needed in this direction to enhance the accuracy and effectiveness of\n",
            "DL methods.\n",
            "The availability of computational capacity is yet another barrier that inhibits the\n",
            "implementation of DL in agriculture. The training of models necessitates ever-increasing\n",
            "amounts of computing resources due to the growing number of datasets, as well as the\n",
            "increasing complexity of deep-learning neural networks. It is very important that the\n",
            "performance of graphics processing units (GPUs) and central processing units (CPUs)\n",
            "continues to improve, as this is a very important accelerator for the widespread popularity\n",
            "of deep learning. In addition, the development of DL has been sped up by the availability of\n",
            "cloud computing services, such as the Google Cloud Platform, offered by private businesses.\n",
            "However, because of stringent requirements regarding computation capacity, current uses\n",
            "of DL in agriculture are typically ofﬂine. This means that the collection and analysis of\n",
            "images of farmland is undertaken in an asynchronous manner. Consequently, companies\n",
            "have paid some attention to this issue.\n",
            "Funding: This research received no external funding.\n",
            "Data Availability Statement: The data used are included within the manuscript.\n",
            "Conﬂicts of Interest: The author declares no conﬂict of interest.Agriculture 2023 ,13, 540 18 of 22\n",
            "References\n",
            "1. Santos, L.; Santos, F.; Mendes, J.; Costa, P .; Lima, J.; Reis, R.; Shinde, P . Path planning aware of robot’s center of mass for steep\n",
            "slope vineyards. Robotica 2020 ,38, 684–698. [CrossRef]\n",
            "2. Patil, K.A.; Kale, N.R. A model for smart agriculture using IoT. In Proceedings of the 2016 International Conference on Global\n",
            "Trends in Signal Processing, Information Computing and Communication (ICGTSPICC), Jalgaon, India, 22–24 December 2016;\n",
            "pp. 543–545.\n",
            "3. Atzberger, C. Advances in remote sensing of agriculture: Context description, existing operational monitoring systems and major\n",
            "information needs. Remote Sens. 2013 ,5, 949–981. [CrossRef]\n",
            "4. Santos, L.; Ferraz, N.; dos Santos, F.N.; Mendes, J.; Morais, R.; Costa, P .; Reis, R. Path planning aware of soil compaction for steep\n",
            "slope vineyards. In Proceedings of the 2018 IEEE International Conference on Autonomous Robot Systems and Competitions\n",
            "(ICARSC), Torres Vedras, Portugal, 25–27 April 2018. [CrossRef]\n",
            "5. Dhanaraju, M.; Chenniappan, P .; Ramalingam, K.; Pazhanivelan, S.; Kaliaperumal, R. Smart Farming: Internet of Things\n",
            "(IoT)-Based Sustainable Agriculture. Agriculture 2022 ,12, 1745. [CrossRef]\n",
            "6. Walter, A.; Finger, R.; Huber, R.; Buchmann, N. Opinion: Smart farming is key to developing sustainable agriculture. Proc. Natl.\n",
            "Acad. Sci. USA 2017 ,114, 6148–6150. [CrossRef] [PubMed]\n",
            "7. Schmidhuber, J. Deep learning in neural networks: An overview. Neural Netw. 2015 ,61, 85–117. [CrossRef] [PubMed]\n",
            "8. Kamilaris, A.; Prenafeta-Bold ú, F.X. A review of the use of convolutional neural networks in agriculture. J. Agric. Sci. 2018 ,156,\n",
            "312–322. [CrossRef]\n",
            "9. Bouguettaya, A.; Zarzour, H.; Kechida, A.; Taberkit, A.M. A survey on deep learning-based identiﬁcation of plant and crop\n",
            "diseases from UAV-based aerial images. In Cluster Computing ; Springer Science and Business Media LLC: Berlin/Heidelberg,\n",
            "Germany, 2022. [CrossRef]\n",
            "10. Khan, A.; Vibhute, A.D.; Mali, S.; Patil, C.H. A systematic review on hyperspectral imaging technology with a machine and deep\n",
            "learning methodology for agricultural applications. In Ecological Informatics ; Elsevier BV: Amsterdam, The Netherlands, 2022;\n",
            "Volume 69, p. 101678.\n",
            "11. Kashyap, P . Machine Learning for Decision-Makers: Cognitive Computing Fundamentals for Better Decision Making ; Apress: Bangalore,\n",
            "India, 2017; pp. 227–228.\n",
            "12. Magomadov, V .S. Deep Plearning and its role in smart agriculture. J. Phys. Conf. Ser. 2019 ,1399 , 044109. [CrossRef]\n",
            "13. Graves, A.; Schmidhuber, J. Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures.\n",
            "Neural Netw. 2005 ,18, 602–610. [CrossRef]\n",
            "14. Jain, A.; Zamir, A.R.; Savarese, S.; Saxena, A. Structural-run: Deep learning on Spatio-temporal graphs. In Proceedings of the\n",
            "IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27–30 June 2016; pp. 5308–5317.\n",
            "15. Ren, C.; Kim, D.K.; Jeong, D. A survey of deep learning in agriculture: Techniques and their applications. J. Inf. Process. Syst.\n",
            "2020 ,16, 1015–1033.\n",
            "16. Santos, L.; Santos, F.N.; Oliveira, P .M.; Shinde, P . Deep learning applications in agriculture: A short review. In Proceedings of\n",
            "the Fourth Iberian Robotics Conference: Advances in Robotics, Robot 2019: Porto, Portugal, 20–22 November 2019; Silva, M.F.,\n",
            "Lima, J.L., Reis, L.P ., Sanfeliu, A., Tardioli, D., Eds.; Springer: Cham, Switzerland; pp. 139–151. Available online: https:\n",
            "//doi.org/10.1007/978-3-030-35990-4_12 (accessed on 23 February 2022). [CrossRef]\n",
            "17. Kamilaris, A.; Prenafeta-Bold ú, F.X. Deep learning in agriculture: A survey. Comput. Electron. Agric. 2018 ,147, 70–90. Available\n",
            "online: https://doi.org/10.1016/j.compag.2018.02.016 (accessed on 23 February 2022). [CrossRef]\n",
            "18. Thai-Nghe, N.; Tri, N.T.; Hoa, N.H. Deep Learning for Rice Leaf Disease Detection in Smart Agriculture. In Artiﬁcial Intelligence in\n",
            "Data and Big Data Processing ; Springer International Publishing: Berlin/Heidelberg, Germany, 2022; pp. 659–670.\n",
            "19. Traditional Agriculture: An Efﬁcient and Sustainable Farming Method. Stories.pinduoduo-global.com . 2021. Available online:\n",
            "https://stories.pinduoduo-global.com/agritech-hub/traditionalagriculture#:~:text=Traditional%20agriculture%20can%20be%\n",
            "20deﬁned,cultural%20beliefs%20of%20the%20farmers (accessed on 23 February 2022).\n",
            "20. Shaila, M.; Begum, N. Ancient farming methods of seed storage and pest management practices in India—A review. Plant Arch.\n",
            "2021 ,21, 499–509.\n",
            "21. Dargan, S.; Kumar, M.; Ayyagari, M.R.; Kumar, G. A survey of deep learning and its applications: A new paradigm to machine\n",
            "learning. Arch. Comput. Methods Eng. 2020 ,27, 1071–1092. [CrossRef]\n",
            "22. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015 ,521, 436–444. [CrossRef] [PubMed]\n",
            "23. Abdullahi, H.S.; Sheriff, R.; Mahieddine, F. Convolution neural network in precision agriculture for plant image recognition and\n",
            "classiﬁcation. In 2017 Seventh International Conference on Innovative Computing Technology (INTECH), Luton, UK, 16–18 August 2017 ;\n",
            "IEEE: Piscataway, NJ, USA, 2017; Volume 10, pp. 256–272.\n",
            "24. Ajit, A.; Acharya, K.; Samanta, A. A review of convolutional neural networks. In Proceedings of the 2020 International Conference\n",
            "on Emerging Trends in Information Technology and Engineering (ic-ETITE), Vellore, India, 24–25 February 2020.\n",
            "25. Zaremba, W.; Sutskever, I.; Vinyals, O. Recurrent neural network regularization. arXiv 2014 , arXiv:1409.2329. in press.\n",
            "26. Chen, S.W.; Shivakumar, S.S.; Dcunha, S.; Das, J.; Okon, E.; Qu, C.; Taylor, C.J.; Kumar, V . Counting apples and oranges with deep\n",
            "learning: A data-driven approach. IEEE Robot. Autom. Lett. 2017 ,2, 781–788. [CrossRef]\n",
            "27. Rahnemoonfar, M.; Sheppard, C. Deep count: Fruit counting based on deep simulated learning. Sensors 2017 ,17, 905. [CrossRef]Agriculture 2023 ,13, 540 19 of 22\n",
            "28. Apolo-Apolo, O.E.; Mart ínez-Guanter, J.; Egea, G.; Raja, P .; P érez-Ruiz, M. Deep learning techniques for estimation of the yield\n",
            "and size of citrus fruits using a UAV . Eur. J. Agron. 2020 ,115, 126030. [CrossRef]\n",
            "29. Sa, I.; Ge, Z.; Dayoub, F.; Upcroft, B.; Perez, T.; McCool, C. Deepfruits: A fruit detection system using deep neural networks.\n",
            "Sensors 2016 ,16, 1222. [CrossRef]\n",
            "30. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural\n",
            "Information Processing Systems, Proceedings of the Neural Information Processing Systems Conference, Lake Tahoe, NV , USA, 3–6 December\n",
            "2012 ; Neural Information Processing Systems Foundation, Inc.: Ljubljana, Slovenia, 2012; pp. 1097–1105.\n",
            "31. Bargoti, S.; Underwood, J. Deep fruit detection in orchards. In Proceedings of the 2017 IEEE International Conference on Robotics\n",
            "and Automation (ICRA), Singapore, 29 May–3 June 2017.\n",
            "32. Fu, L.; Feng, Y.; Elkamil, T.; Liu, Z.; Li, R.; Cui, Y. Image recognition method of multi-cluster kiwifruit in ﬁeld based on\n",
            "convolutional neural networks. Nongye Gongcheng Xuebao/Trans. Chin. Soc. Agric. Eng. 2018 ,34, 205–211.\n",
            "33. Katarzyna, R.; Paweł, M. A vision-based method utilizing deep convolutional neural networks for fruit variety classiﬁcation in\n",
            "uncertainty conditions of retail sales. Appl. Sci. 2019 ,9, 3971. [CrossRef]\n",
            "34. Villacr és, J.F.; Auat Cheein, F. Detection and characterization of cherries: A deep learning usability case study in Chile. Agronomy\n",
            "2020 ,10, 835. [CrossRef]\n",
            "35. Chung, D.T.P .; Van Tai, D. A fruits recognition system based on a modern deep learning technique. J. Phys. Conf. Ser. 2019 ,\n",
            "1327 , 012050. [CrossRef]\n",
            "36. Wang, S.H.; Chen, Y. Fruit category classiﬁcation via an eight-layer convolutional neural network with parametric rectiﬁed linear\n",
            "unit and dropout technique. Multim. Tools Appl. 2018 ,79, 15117–15133. [CrossRef]\n",
            "37. Kestur, R.; Meduri, A.; Narasipura, O. MangoNet: A deep semantic segmentation architecture for a method to detect and count\n",
            "mangoes in an open orchard. Eng. Appl. Artif. Intell. 2019 ,77, 59–69. [CrossRef]\n",
            "38. Santos, T.T.; de Souza, L.L.; dos Santos, A.A.; Avila, S. Grape detection, segmentation, and tracking using deep neural networks\n",
            "and three-dimensional association. Comput. Electron. Agric. 2020 ,170, 105247. [CrossRef]\n",
            "39. Chen, H.; Chen, A.; Xu, L.; Xie, H.; Qiao, H.; Lin, Q.; Cai, K. A deep learning CNN architecture applied in smart near-infrared\n",
            "analysis of water pollution for agricultural irrigation resources. Agric. Water Manag. 2020 ,240, 106303. [CrossRef]\n",
            "40. Garg, D.; Khan, S.; Alam, M. Integrative use of IoT and deep learning for agricultural applications. In Proceedings of ICETIT\n",
            "2019: Emerging Trends in Information Technology; Springer: Cham, Switzerland, 2020; pp. 521–531.\n",
            "41. Mohan, P .; Patil, K.K. Deep learning based weighted SOM to forecast weather and crop prediction for agriculture application. Int.\n",
            "J. Intell. Eng. Syst. 2018 ,11, 167–176. [CrossRef]\n",
            "42. Yang, X.; Sun, M. A survey on deep learning in crop planting. IOP Conf. Ser. Mater. Sci. Eng. 2019 ,490, 062053. [CrossRef]\n",
            "43. Dharani, M.K.; Thamilselvan, R.; Natesan, P .; Kalaivaani, P .C.D.; Santhoshkumar, S. Review Pon crop prediction using deep\n",
            "learning techniques. J. Phys. Conf. Ser. 2021 ,1767 , 012026. [CrossRef]\n",
            "44. Zheng, Y.Y.; Kong, J.L.; Jin, X.B.; Wang, X.Y.; Su, T.L.; Zuo, M. CropDeep: The crop vision dataset for deep-learning-based\n",
            "classiﬁcation and detection in precision agriculture. Sensors 2019 ,19, 1058. [CrossRef]\n",
            "45. Lottes, P .; Behley, J.; Chebrolu, N.; Milioto, A.; Stachniss, C. Robust joint stem detection and crop-weed classiﬁcation using image\n",
            "sequences for plant-speciﬁc treatment in precision farming. J. Field Robot. 2020 ,37, 20–34. [CrossRef]\n",
            "46. Lottes, P .; Behley, J.; Milioto, A.; Stachniss, C. Fully convolutional networks with sequential information for robust crop and weed\n",
            "detection in precision farming. IEEE Robot. Autom. Lett. 2018 ,3, 2870–2877. [CrossRef]\n",
            "47. Chavan, T.R.; Nandedkar, A.V . AgroAVNET for crops and weeds classiﬁcation: A step forward in automatic farming. Comput.\n",
            "Electron. Agric. 2018 ,154, 361–372. [CrossRef]\n",
            "48. Suh, H.K.; Ijsselmuiden, J.; Hofstee, J.W.; van Henten, E.J. Transfer learning for the classiﬁcation of sugar beet and volunteer\n",
            "potato under ﬁeld conditions. Biosyst. Eng. 2018 ,174, 50–65. [CrossRef]\n",
            "49. Meng, S.; Wang, X.; Hu, X.; Luo, C.; Zhong, Y. Deep learning-based crop mapping in the cloudy season using one-shot\n",
            "hyperspectral satellite imagery. Comput. Electron. Agric. 2021 ,186, 106188. [CrossRef]\n",
            "50. Cai, Y.; Zheng, W.; Zhang, X.; Zhangzhong, L.; Xue, X. Research on soil moisture prediction model based on deep learning.\n",
            "PLoS ONE 2019 ,14, e0214508. [CrossRef]\n",
            "51. Yashwanth, M.; Chandra, M.L.; Pallavi, K.; Showkat, D.; Kumar, P .S. Agriculture automation using deep learning methods\n",
            "implemented using keras. In Proceedings of the 2020 IEEE International Conference for Innovation in Technology (INOCON),\n",
            "Bangalore, India, 6–8 November 2020.\n",
            "52. Tseng, D.; Wang, D.; Chen, C.; Miller, L.; Song, W.; Viers, J.; Vougioukas, S.; Carpin, S.; Ojea, J.A.; Goldberg, K. Towards\n",
            "automating precision irrigation: Deep learning to infer local soil moisture conditions from synthetic aerial agricultural images.\n",
            "In Proceedings of the 2018 IEEE 14th International Conference on Automation Science and Engineering (CASE), Munich, Germany,\n",
            "20–24 August 2018; pp. 284–291.\n",
            "53. Westwood, J.H.; Charudattan, R.; Duke, S.O.; Fennimore, S.A.; Marrone, P .; Slaughter, D.C.; Swanton, C.; Zollinger, R. Weed\n",
            "management in 2050: Perspectives on the future of weed science. Weed Sci. 2018 ,66, 275–285. [CrossRef]\n",
            "54. Mishra, A.M.; Gautam, V . Weed species identiﬁcation in different crops using precision weed management: A review. Proc. CEUR\n",
            "Workshop 2021 , 180–194. Available online: https://www.semanticscholar.org/paper/Weed-Species-Identiﬁcation-in-Different-\n",
            "Crops-Weed-Mishra-Gautam/8710e1a04eada39809b159ea8650f4c639c9bf19 (accessed on 3 July 2022).Agriculture 2023 ,13, 540 20 of 22\n",
            "55. Gulzar, Y.; Hamid, Y.; Soomro, A.B.; Alwan, A.A.; Journaux, L. A convolution neural network-based seed classiﬁcation system.\n",
            "Symmetry 2020 ,12, 2018. [CrossRef]\n",
            "56. Sharma, A.; Jain, A.; Gupta, P .; Chowdary, V . Machine learning applications for precision agriculture: A comprehensive review.\n",
            "IEEE Access 2020 ,9, 4843–4873. [CrossRef]\n",
            "57. Saleem, M.H.; Potgieter, J.; Arif, K.M. Plant disease detection and classiﬁcation by deep learning. Plants 2019 ,8, 468. [CrossRef]\n",
            "58. Amara, J.; Bouaziz, B.; Algergawy, A. A deep learning-based approach for banana leaf diseases classiﬁcation. In Proceedings of\n",
            "the Datenbanksysteme für Business, Technologie und Web (BTW 2017)-Workshopband, Stuttgart, Germany, 6–7 March 2017;\n",
            "Gesellschaft für Informatik e.V .: Bonn, Germany, 2017; pp. 79–88.\n",
            "59. Dipali, M.; Deepa, D. Automation, and integration of growth monitoring in plants (with disease prediction) and crop prediction.\n",
            "Mater. Today Proc. 2021 ,43, 3922–3927.\n",
            "60. Akash, S.; Malik, A. A hybrid model for the classiﬁcation of sunﬂower diseases using deep learning. In Proceedings of the 2021\n",
            "2nd International Conference on Intelligent Engineering and Management (ICIEM), London, UK, 28–30 April 2021; pp. 58–62.\n",
            "61. Ahmed, A.A.; Reddy, G.H. A mobile-based system for detecting plant leaf diseases using deep learning. AgriEngineering 2021 ,3,\n",
            "478–493. [CrossRef]\n",
            "62. Pallagani, V .; Khandelwal, V .; Chandra, B.; Udutalapally, V .; Das, D.; Mohanty, S.P . DCrop: A deep-learning-based framework for\n",
            "accurate prediction of diseases of crops in smart agriculture. In Proceedings of the 2019 IEEE International Symposium on Smart\n",
            "Electronic Systems (iSES) (Formerly iNiS), Rourkela, India, 16–18 December 2019; pp. 29–33.\n",
            "63. Sladojevic, S.; Arsenovic, M.; Anderla, A.; Culibrk, D.; Stefanovic, D. Deep neural networks based recognition of plant diseases by\n",
            "leaf image classiﬁcation. Comput. Intell. Neurosci. 2016 ,2016 , 3289801. [CrossRef] [PubMed]\n",
            "64. Arivazhagan, S.; Shebiah, R.N.; Ananthi, S.; Varthini, S.V . Detection of unhealthy region of plant leaves and classiﬁcation of plant\n",
            "leaf diseases using texture features. Agric. Eng. Int. CIGR J. 2013 ,15, 211–217.\n",
            "65. Chen, J.; Chen, J.; Zhang, D.; Sun, Y.; Nanehkaran, Y.A. Using deep transfer learning for image-based plant disease identiﬁcation.\n",
            "Comput. Electron. Agric. 2020 ,173, 105393. [CrossRef]\n",
            "66. Sharma, P .; Berwal, Y.P .S.; Ghai, W. Performance analysis of deep learning CNN models for disease detection in plants using\n",
            "image segmentation. Inf. Process. Agric. 2020 ,7, 566–574. [CrossRef]\n",
            "67. Atila, Ü.; Uçar, M.; Akyol, K.; Uçar, E. Plant leaf disease classiﬁcation using EfﬁcientNet deep learning model. Ecol. Inform. 2021 ,\n",
            "61, 101182. [CrossRef]\n",
            "68. Kaur, P .; Harnal, S.; Tiwari, R.; Upadhyay, S.; Bhatia, S.; Mashat, A.; Alabdali, A.M. Recognition of leaf disease using hybrid\n",
            "convolutional neural network by applying feature reduction. Sensors 2022 ,22, 575. [CrossRef]\n",
            "69. Ji, M.; Zhang, L.; Wu, Q. Automatic grape leaf diseases identiﬁcation via UnitedModel based on multiple convolutional neural\n",
            "networks. Inf. Process. Agric. 2020 ,7, 418–426. [CrossRef]\n",
            "70. Gadekallu, T.R.; Rajput, D.S.; Reddy, M.P .K.; Lakshmanna, K.; Bhattacharya, S.; Singh, S.; Jolfaei, A.; Alazab, M. A novel\n",
            "PCA–whale optimization-based deep neural network model for classiﬁcation of tomato plant diseases using GPU. J. Real-Time\n",
            "Image Process. 2021 ,18, 1383–1396. [CrossRef]\n",
            "71. Azimi, S.; Kaur, T.; Gandhi, T.K. A deep learning approach to measure stress level in plants due to nitrogen deﬁciency. Measurement\n",
            "2021 ,173, 108650. [CrossRef]\n",
            "72. Joshi, R.C.; Kaushik, M.; Dutta, M.K.; Srivastava, A.; Choudhary, N. VirLeafNet: Automatic analysis and viral disease diagnosis\n",
            "using deep-learning in Vigna mungo plant. Ecol. Inform. 2021 ,61, 101197. [CrossRef]\n",
            "73. Kavitha, A. Deep Learning for Smart Agriculture. Int. J. Eng. Res. Technol. 2021 ,9. Available online: https://www.\n",
            "semanticscholar.org/paper/Deep-Learning-for-Smart-Agriculture-Kavitha/0a272722fe4838cce5af0bb907310bf76927406d (ac-\n",
            "cessed on 3 July 2022).\n",
            "74. Josephine, B.; Ramya, K.; Rao, K.V .S.N.; Kuchibhotla, P .; Venkata, K.; Rahamathulla, S. Crop yield prediction using machine\n",
            "learning. Int. J. Sci. Technol. Res. 2020 ,9. Available online: http://www.ijstr.org/paper-references.php?ref=IJSTR-0120-29576\n",
            "(accessed on 3 July 2022).\n",
            "75. Khaki, S.; Wang, L. Crop yield prediction using deep neural networks. Front. Plant Sci. 2019 ,10, 621. Available online:\n",
            "https://doi.org/10.3389/fpls.2019.00621 (accessed on 23 February 2022). [CrossRef]\n",
            "76. Deepika, P .; Kaliraj, S. A survey on pest and disease monitoring of crops. In Proceedings of the 2021 3rd International Conference\n",
            "on Signal Processing and Communication (ICPSC), Coimbatore, India, 13–14 May 2021; Available online: https://doi.org/10.110\n",
            "9/icspc51351.2021.9451787 (accessed on 23 February 2022).\n",
            "77. Ale, L.; Sheta, A.; Li, L.; Wang, Y.; Zhang, N. Deep learning based plant disease detection for smart agriculture. In Proceedings\n",
            "of the 2019 IEEE Globecom Workshops (GC Wkshps), Waikoloa, HI, USA, 9–13 December 2019; Available online: https:\n",
            "//doi.org/10.1109/gcwkshps45667.2019.9024439 (accessed on 23 February 2022).\n",
            "78. Zhu, N.; Liu, X.; Liu, Z.; Hu, K.; Wang, Y.; Tan, J.; Huang, M.; Zhu, Q.; Ji, X.; Jiang, Y.; et al. Deep learning for smart agriculture:\n",
            "Concepts, tools, applications, and opportunities. Int. J. Agric. Biol. Eng. 2018 ,11, 32–44. [CrossRef]\n",
            "79. Deep Learning for Image-Based Plant Disease Detection. Let’s Nurture—An IT Company Nurturing Ideas into Reality.\n",
            "2022. Available online: https://www.letsnurture.com/blog/using-deep-learning-for-image-based-plant-disease-detection.html\n",
            "(accessed on 23 February 2022).\n",
            "80. Zhao, Q.; Koch, C. Learning saliency-based visual attention: A Review. Signal Process. 2013 ,93, 1401–1407. [CrossRef]Agriculture 2023 ,13, 540 21 of 22\n",
            "81. Kümmerer, M.; Theis, L.; Bethge, M. Deep Gaze I: Boosting saliency prediction with feature maps trained on ImageNet. In\n",
            "Proceedings of the 2015 International Conference on Learning Representations (ICLR), San Diego, CA, USA, 8 May 2014; Available\n",
            "online: https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_2160776 (accessed on 24 February 2022).\n",
            "82. Shrikumar, A.; Greenside, P .; Kundaje, A. Reverse-complement parameter sharing improves deep learning models for genomics.\n",
            "bioRxiv 2017, in press.\n",
            "83. Springenberg, J.T.; Dosovitskiy, A.; Brox, T.; Riedmiller, M. Striving for simplicity: The all convolutional net. arXiv . 2015. in press.\n",
            "Available online: https://arxiv.org/abs/1412.6806 (accessed on 24 February 2022).\n",
            "84. Zeiler, M.D.; Fergus, R. Visualizing and understanding Convolutional Networks. In Proceedings of Computer Vision–ECCV\n",
            "2014: 13th European Conference, Part I 13, Zurich, Switzerland, 6–12 September 2014; Springer International Publishing: Cham,\n",
            "Switzerland, 2014; pp. 818–833.\n",
            "85. Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; Torralba, A. Learning deep features for discriminative localization. In Proceedings\n",
            "of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 26 June–1 July 2016.\n",
            "86. Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; Müller, K.-R.; Samek, W. On pixel-wise explanations for non-linear classiﬁer\n",
            "decisions by layer-wise relevance propagation. PLoS ONE 2015 ,10, e0130140. [CrossRef]\n",
            "87. Uzal, L.C.; Grinblat, G.L.; Nam ías, R.; Larese, M.G.; Bianchi, J.S.; Morandi, E.N.; Granitto, P .M. Seed-per-pod estimation for plant\n",
            "breeding using deep learning. Comput. Electron. Agric. 2018 ,150, 196–204. [CrossRef]\n",
            "88. Nkemelu, D.K.; Omeiza, D.; Lubalo, N. Deep convolutional neural network for plant seedlings classiﬁcation. arXiv . 2018. in press.\n",
            "Available online: https://arxiv.org/abs/1811.08404?source=post_page (accessed on 24 February 2022).\n",
            "89. Amiryouseﬁ, M.R.; Mohebbi, M.; Tehranifar, A. Pomegranate seed clustering by machine vision. Food Sci. Nutr. 2017 ,6, 18–26.\n",
            "[CrossRef] [PubMed]\n",
            "90. Sujatha, R.; Chatterjee, J.M.; Jhanjhi, N.Z.; Brohi, S.N. Performance of deep learning vs machine learning in plant leaf disease\n",
            "detection. Microprocess. Microsyst. 2021 ,80, 103615. [CrossRef]\n",
            "91. Arnal Barbedo, J.G. Plant disease identiﬁcation from individual lesions and spots using deep learning. Biosyst. Eng. 2019 ,180,\n",
            "96–107. [CrossRef]\n",
            "92. Liu, P .; Mahmood, T.; Khan, Q. Multi-attribute decision-making based on prioritized aggregation operator under hesitant\n",
            "intuitionistic fuzzy linguistic environment. Symmetry 2017 ,9, 270. [CrossRef]\n",
            "93. Bresilla, K.; Perulli, G.D.; Boini, A.; Morandi, B.; Grappadelli, L.; Manfrini, L. Single-shot convolution neural networks for\n",
            "real-time fruit detection within the tree. Front. Plant Sci. 2019 ,10, 611. [CrossRef] [PubMed]\n",
            "94. Altaheri, H.; Alsulaiman, M.; Muhammad, G. Date fruit classiﬁcation for robotic harvesting in a natural environment using deep\n",
            "learning. IEEE Access 2019 ,7, 117115–117133. [CrossRef]\n",
            "95. Meshram, V .; Patil, K.; Meshram, V .; Hanchate, D.; Ramkteke, S.D. Machine learning in agriculture domain: A state-of-art survey.\n",
            "Artif. Intell. Life Sci. 2021 ,1, 100010. [CrossRef]\n",
            "96. Lu, S.; Chen, W.; Zhang, X.; Karkee, M. Canopy-attention-YOLOv4-based immature/mature apple fruit detection on dense-foliage\n",
            "tree architectures for early crop load estimation. Comput. Electron. Agric. 2022 ,193, 106696. [CrossRef]\n",
            "97. Lyu, S.; Li, R.; Zhao, Y.; Li, Z.; Fan, R.; Liu, S. Green citrus detection and counting in orchards based on YOLOv5-CS and AI edge\n",
            "system. Sensors 2022 ,22, 576. [CrossRef]\n",
            "98. Khan, R.; Dhingra, N.; Bhati, N. Role of Artiﬁcial Intelligence in Agriculture: A Comparative Study. In Transforming Management\n",
            "with AI, Big-Data, and IoT ; Springer International Publishing: Cham, Switzerland, 2022; pp. 73–83.\n",
            "99. Wang, C.; Liu, B.; Liu, L.; Zhu, Y.; Hou, J.; Liu, P .; Li, X. A review of deep learning used in the hyperspectral image analysis for\n",
            "agriculture. Artif. Intell. Rev. 2021 ,54, 5205–5253. [CrossRef]\n",
            "100. Coulibaly, S.; Kamsu-Foguem, B.; Kamissoko, D.; Traore, D. Deep neural networks with transfer learning in Millet crop images.\n",
            "Comput. Ind. 2019 ,108, 115–120. [CrossRef]\n",
            "101. Sahni, V .; Srivastava, S.; Khan, R. Modelling techniques to improve the quality of food using artiﬁcial intelligence. J. Food Qual.\n",
            "2021 ,2021 , 2140010. [CrossRef]\n",
            "102. Khan, R.; Tyagi, N.; Chauhan, N. Safety of food and food warehouse using VIBHISHAN. J. Food Qual. 2021 ,2021 , 1328332.\n",
            "[CrossRef]\n",
            "103. Kaya, A.; Keceli, A.S.; Catal, C.; Yalic, H.Y.; Temucin, H.; Tekinerdogan, B. Analysis of transfer learning for deep neural network\n",
            "based plant classiﬁcation models. Comput. Electron. Agric. 2019 ,158, 20–29. [CrossRef]\n",
            "104. Sharma, M.; Nath, K.; Sharma, R.K.; Kumar, C.J.; Chaudhary, A. Ensemble Averaging of Transfer Learning Models for Identiﬁca-\n",
            "tion of Nutritional Deﬁciency in Rice Plant. Electronics 2022 ,11, 148. [CrossRef]\n",
            "105. Argüeso, D.; Picon, A.; Irusta, U.; Medela, A.; San-Emeterio, M.G.; Bereciartua, A.; Alvarez-Gila, A. Few-shot learning approach\n",
            "for plant disease classiﬁcation using images taken in the ﬁeld. Comput. Electron. Agric. 2020 ,175, 105542. [CrossRef]\n",
            "106. Zhong, F.M.; Chen, Z.K.; Zhang, Y.C.; Xia, F. Zero-and few-shot learning for diseases recognition of Citrus aurantium L. using\n",
            "conditional adversarial autoencoders. Comput. Electron. Agric. 2020 ,179, 105828. [CrossRef]\n",
            "107. Jiang, H.H.; Zhang, C.Y.; Qiao, Y.L.; Zhang, Z.; Zhang, W.J.; Song, C.Q. CNN feature-based graph convolutional network for\n",
            "weed and crop recognition in smart farming. Comput. Electron. Agric. 2020 ,174, 105450. [CrossRef]\n",
            "108. Khaki, S.; Pham, H.; Han, Y.; Kuhl, A.; Kent, W.; Wang, L. Deepcorn: A semi-supervised deep learning method for high-throughput\n",
            "image-based corn kernel counting and yield estimation. Knowl. Based Syst. 2021 ,218, 106874. [CrossRef]Agriculture 2023 ,13, 540 22 of 22\n",
            "109. Sa, I.; Chen, Z.; Popovi´ c, M.; Khanna, R.; Liebisch, F.; Nieto, J.; Siegwart, R. WeedNet: Dense semantic weed classiﬁcation using\n",
            "multispectral images and MAV for smart farming. IEEE Robot. Autom. Lett. 2017 ,3, 588–595. [CrossRef]\n",
            "110. Sa, I.; Popovi´ c, M.; Khanna, R.; Chen, Z.; Lottes, P .; Liebisch, F.; Nieto, J.; Stachniss, C.; Walter, A.; Siegwart, R. WeedMap: A\n",
            "large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming.\n",
            "Remote Sens. 2018 ,10, 1423. [CrossRef]\n",
            "111. Jiao, L.; Dong, S.; Zhang, S.; Xie, C.; Wang, H. AF-RCNN: An anchor-free convolutional neural network for multi-categories\n",
            "agricultural pest detection. Comput. Electron. Agric. 2020 ,174, 105522. [CrossRef]\n",
            "112. Eunice, J.; Popescu, D.E.; Chowdary, M.K.; Hemanth, J. Deep learning-based leaf disease detection in crops using images for\n",
            "agricultural applications. Agronomy 2022 ,12, 2395.\n",
            "113. Wu, Q.; Chen, Y.; Meng, J. DCGAN-based data augmentation for tomato leaf disease identiﬁcation. IEEE Access 2020 ,8,\n",
            "98716–98728. [CrossRef]\n",
            "114. Hammouch, H.; El-Yacoubi, M.; Qin, H.; Berrahou, A.; Berbia, H.; Chikhaoui, M. A two-stage deep convolutional generative\n",
            "adversarial network-based data augmentation scheme for agriculture image regression tasks. In Proceedings of the 2021\n",
            "International Conference on Cyber-Physical Social Intelligence (ICCSI), Beijing, China, 18–20 December 2021.\n",
            "115. Sourav, A.I.; Emanuel, A.W.R. Recent Trends of Big Data in Precision Agriculture: A Review. In IOP Conference Series: Materials\n",
            "Science and Engineering ; IOP Publishing: Bristol, UK, 2021; Volume 1096, p. 012081.\n",
            "116. Bharman, P .; Ahmad Saad, S.; Khan, S.; Jahan, I.; Ray, M.; Biswas, M. Deep Learning in Agriculture: A Review. Asian J. Res.\n",
            "Comput. Sci. 2022 ,13, 28–47. [CrossRef]\n",
            "117. Goodfellow, I.; Bengio, Y.; Courville, A. Deep Learning ; MIT Press: Cambridge, MA, USA, 2014.\n",
            "118. Liu, J.; Cheng, L.; Yan, S. Deep learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
            "Recognition, Long Beach, CA, USA, 16–20 June 2019.\n",
            "119. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv\n",
            "2018 , arXiv:1706.06083. in press.\n",
            "120. Finn, C.; Abbeel, P .; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th\n",
            "International Conference on Machine Learning, Sydney, Australia, 6–11 August 2017; Volume 70, pp. 1126–1135.\n",
            "121. Ribeiro, M.T.; Singh, S.; Guestrin, C. “Why should I trust you?”: Explaining the predictions of any classiﬁer. In Proceedings of the\n",
            "22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco CA, USA, 13–17 August\n",
            "2016; Association for Computing Machinery: New York, NY, USA, 2016; pp. 1135–1144.\n",
            "122. Lundberg, S.M.; Lee, S.I. A uniﬁed approach to interpreting model predictions. Adv. Neural Inf. Process. Syst. 2017 ,30, 4765–4774.\n",
            "123. Caruana, R. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the\n",
            "21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, Australia, 10–13 August 2015;\n",
            "pp. 1721–1730.\n",
            "124. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need. Adv.\n",
            "Neural Inf. Process. Syst. 2017 ,30, 5998–6008.\n",
            "125. Snell, J.; Swersky, K.; Zemel, R. Prototypical networks for few-shot learning. Adv. Neural Inf. Process. Syst. 2017 ,30, 4077–4087.\n",
            "Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\n",
            "author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\n",
            "people or property resulting from any ideas, methods, instructions or products referred to in the content.Deep Learning in Robotics: A Review of Recent Research\n",
            "Harry A. Pierson (corresponding author)\n",
            "Department of Industrial Engineering, University of Arkansas, Fayetteville, AR, USA\n",
            "4207 Bell Engineering Center\n",
            "1 University of Arkansas\n",
            "Fayetteville, AR  72701\n",
            "hapierso@uark.edu\n",
            "+1 (479) 575-6034\n",
            "Michael S. Gashler\n",
            "Department of Computer Science and Computer Engineering, University of Arkansas, \n",
            "Fayetteville, AR, USA\n",
            "504 J. B. Hunt Building\n",
            "1 University of Arkansas\n",
            "Fayetteville, AR  72701\n",
            "mgashler@uark.edu\n",
            "1Deep Learning in Robotics: A Review of Recent Research\n",
            "Advances in deep learning over the last decade have led to a flurry of research in \n",
            "the application of deep artificial neural networks to robotic systems, with at least \n",
            "thirty papers published on the subject between 2014 and the present.  This review\n",
            "discusses the applications, benefits, and limitations of deep learning vis-à-vis \n",
            "physical robotic systems, using contemporary research as exemplars.  It is \n",
            "intended to communicate recent advances to the wider robotics community and \n",
            "inspire additional interest in and application of deep learning in robotics.\n",
            "Keywords: deep neural networks; artificial intelligence; human-robot interaction \n",
            "1.  Introduction\n",
            "Deep learning is the science of training large artificial neural networks.  Deep neural \n",
            "networks (DNNs) can have hundreds of millions of parameters [1, 2], allowing them to \n",
            "model complex functions such as nonlinear dynamics.  They form compact \n",
            "representations of state from raw, high-dimensional, multimodal sensor data commonly \n",
            "found in robotic systems [3], and unlike many machine learning methods, they do not \n",
            "require a human expert to hand-engineer feature vectors from sensor data at design time.\n",
            "DNNs can, however, present particular challenges in physical robotic systems, where \n",
            "generating training data is generally expensive, and sub-optimal performance in training\n",
            "poses a danger in some applications.  Yet, despite such challenges, roboticists are \n",
            "finding creative alternatives, such as leveraging training data via digital manipulation, \n",
            "automating training, and employing multiple DNNs to improve performance and reduce\n",
            "training time.\n",
            "Applying deep learning to robotics is an active research area, with at least thirty \n",
            "papers published on the subject from 2014 through the time of this writing.  This review\n",
            "presents a summary of this recent research with particular emphasis on the benefits and \n",
            "challenges vis-à-vis robotics.  A primer on deep learning is followed by a discussion of \n",
            "2how common DNN structures are used in robotics and in examples from the recent \n",
            "literature.  Practical considerations for roboticists wishing to use DNNs are also \n",
            "provided.  Finally, limitations of and strategies that mitigate these as well as future \n",
            "trends are discussed.\n",
            "2.  Deep learning\n",
            "2.1 A brief history of deep learning\n",
            "The basic principles of linear regression were used by Gauss and Legendre  [4], and \n",
            "many of those same principles still cover what researchers in deep learning study.  \n",
            "However, several important advances have slowly transformed regression into what we \n",
            "now call deep learning.  First, the addition of an activation function enabled regression \n",
            "methods to fit to nonlinear functions.  It also introduced some biological similarity with \n",
            "brain cells [5].\n",
            "Next, nonlinear models were stacked in “layers” to create powerful models, \n",
            "called multi-layer perceptrons .  In the 1960s a few researchers independently figured \n",
            "out how to differentiate multi-layer perceptrons [6], and by the 1980s, it evolved into a \n",
            "popular method for training them, called backpropagation [7, 8].  It was soon proven \n",
            "that multi-layer perceptrons were universal function approximators [9], meaning they \n",
            "could fit to any data, no matter how complex, with arbitrary precision, using a finite \n",
            "number of regression units.  In many ways, backpropagation marked the beginning of \n",
            "the deep learning revolution; however, researchers still mostly limited their neural \n",
            "networks to a few layers because of the problem of vanishing gradients [10, 11].  \n",
            "Deeper neural networks took exponentially longer to train.\n",
            "Neural networks were successfully applied for robotics control as early as the \n",
            "1980s [12].  It was quickly recognized that nonlinear regression provided the \n",
            "3functionality that was needed for operating dynamical systems in continuous spaces [13,\n",
            "14], and closely related fuzzy systems seemed well suited for nominal logical control \n",
            "decisions [15].  Even as early as 1989, Pomerleau’s ALVINN [16] famously \n",
            "demonstrated that neural networks were effective for helping vehicles to stay in their \n",
            "lanes.  However, neural networks were still generally too slow to digest entire images, \n",
            "or perform the complex tasks necessary for many robotics applications.\n",
            "In the 2000s, researchers began using graphical processing units (GPUs) to \n",
            "parallelize implementations of artificial neural networks [17].  The largest bottleneck in \n",
            "training neural networks is a matrix-vector multiplication step, which can be \n",
            "parallelized using GPUs.  In 2006, Hinton presented a training method that he \n",
            "demonstrated to be effective with a many-layered neural network [18].  The near-\n",
            "simultaneous emergence of these technologies triggered the flurry of research interest \n",
            "that is now propelling deep learning forward at an unprecedented rate [19].\n",
            "As hardware improved, and as neural networks began to become more practical, \n",
            "they were increasingly found to be effective with real robotics applications.  In 2004 \n",
            "RNNPB showed that neural networks could self-organize high-level control schema that\n",
            "generalized effectively with several robotics test problems [20].  In 2008, \n",
            "neuroscientists made advances in recognizing how animals achieved locomotion, and \n",
            "were able to extend this knowledge all the way to neural networks for experimental \n",
            "control of robots [21].  In 2011, TNLDR demonstrated that deep neural nets could \n",
            "effectively model both state and dynamics from strictly unsupervised training with raw \n",
            "images of a simulated robot [22].  Another relevant work is Pomerleau’s 2012 book \n",
            "surveying applications for neural networks in perception for robot guidance [23].\n",
            "In hindsight, we see that chess was considered in the early years of artificial \n",
            "intelligence to be representative of human intelligence over machines [24].  After \n",
            "4machines beat world-class chess players [25], a new emblematic task was needed to \n",
            "represent the superior capabilities of human intelligence.  Visual recognition was largely\n",
            "accepted to be something easy for humans but difficult for machines [26].  But now, \n",
            "with the emergence of deep learning, humans will not be able to claim that as an \n",
            "advantage for much longer.  Deep learning has surged ahead of well-established image \n",
            "recognition techniques [27] and has begun to dominate the benchmarks in handwriting \n",
            "recognition [28], video recognition [29], small-image identification [30], detection in \n",
            "biomedical imaging [31-33], and many others.  It has even achieved super-human \n",
            "accuracy in several image recognition contests [27, 34, 35].  Perhaps agility or dexterity \n",
            "will be a forthcoming achievement where machines will begin to demonstrate human-\n",
            "like proficiency.  If so, it appears that deep neural networks may be the learning model \n",
            "that enables it.\n",
            "2.2 Common DNN structures\n",
            "The idea of using machine learning in controlling robots requires humans to be willing \n",
            "to relinquish a degree of control.  This can seem counterintuitive at first, but the benefit \n",
            "for doing so is that the system can then begin to learn on its own.  This makes the \n",
            "system capable of adapting, and therefore has potential to ultimately make better use of \n",
            "the direction that comes from humans.\n",
            "DNNs are well suited for use with robots because they are flexible, and can be \n",
            "used in structures that other machine learning models cannot support.  Figure 1 \n",
            "diagrams four common structures for using DNNs with robots.  \n",
            "Structure A (in Figure 1) shows a DNN for regressing arbitrary functions.  It is \n",
            "typically trained by presenting a large collection of example training pairs:\n",
            "{ <x1, y1>,<x2, y2>, … , <xn,yn> }.  An optimization method is applied to minimize the\n",
            "5prediction loss.  For regression problems, loss is typically measured with sum-squared \n",
            "error,  \n",
            " , and for classification problems it is often measured with cross-\n",
            "entropy,  \n",
            " , particularly when a softmax layer is used for the output layer \n",
            "of the neural network [36].  Traditionally, the most popular optimization method for \n",
            "neural networks is stochastic gradient descent [37], but improved methods such as \n",
            "RMSProp [38] and Adam [39] have recently garnered widespread usage.  Some other \n",
            "considerations for training them effectively are given in Section 2.4.  After training is \n",
            "finished, novel vectors may be fed in as x to compute corresponding predictions for y.\n",
            "Structure B is called an autoencoder [40].  It is one common model for \n",
            "facilitating “unsupervised learning.”  It requires two DNNs, called an “encoder” and a \n",
            "“decoder.”  In this configuration, only x needs to be supplied by the user.  s is a \n",
            "“latent” or internal encoding that the DNN generates.  For example, x might represent \n",
            "images observed by a robot’s camera, containing thousands or even millions of values.  \n",
            "The encoder might use convolutional layers, which are known to be effective for \n",
            "digesting images [35, 41, 42].  s might be a small vector, perhaps only tens of values.  \n",
            "By learning to reduce x to s, the autoencoder essentially creates its own internal \n",
            "encoding of “state.”  It will not necessarily use an encoding that has meaning for \n",
            "humans, but it will be sufficient for the DNN to approximately reconstruct x.  How are \n",
            "autoencoders useful in robotics?  Sometimes, the robot designer may not know exactly \n",
            "what values are needed by the robot.  Autoencoders enable the system to figure that out \n",
            "autonomously.  This becomes especially useful when a hybrid of supervised and \n",
            "unsupervised learning is used.  For example, the user can impose certain values in s \n",
            "6(perhaps, positional coordinates or joint angles) and the DNNs will learn to work with \n",
            "those values, using the other free elements in s for its own encoding purposes.  \n",
            "Autoencoders may also be used to initialize some parts of Structure C [22].  Generative \n",
            "models are closely related to autoencoders.  They utilize just the decoder portion of the \n",
            "model to predict observations from an internal representation of state.\n",
            "Structure C is a type of “recurrent neural network,” which is designed to model \n",
            "dynamic systems, including robots.  It is often trained with an approach called \n",
            "“backpropagation through time” [43, 44].  Many advances, such as “long short-term \n",
            "memory units,” have made recurrent neural networks much stronger [27, 45].  In this \n",
            "configuration, u represents a control signal.  u may also contain recent observations.  s\n",
            "is an internal representation of future state, and x is a vector of anticipated future \n",
            "observations.  The transition function approximates how the control signal will affect \n",
            "state over time.  Just as with autoencoders, the representation of state can be entirely \n",
            "latent, or partially imposed by the user.  (If it were entirely imposed, the model would \n",
            "be prevented from learning.)  If x includes an estimate of the utility of state s, then this \n",
            "configuration is used in “model-based reinforcement learning” [46].\n",
            "Structure D learns a control policy.  It can facilitate “model-free” reinforcement \n",
            "learning.  It uses a DNN to evaluate the utility or quality, q, of potential control vectors.\n",
            "s is a representation of state, and u is a control vector.  (Gradient methods can find the \n",
            "values for u that maximize q.  In cases with discrete control vectors, u may be omitted \n",
            "from the input-end and q augmented to contain an evaluation of each control vector.)  \n",
            "Configurations like this are used when an objective task is known for the robot to \n",
            "perform, but the user does not know exactly how to achieve it.  By rewarding the robot \n",
            "for accomplishing the task, it can be trained to learn how to prioritize its own choices \n",
            "7for actions.  As one prominent example, reinforcement learning was used to teach a \n",
            "machine to play a wide range of Atari video games [47].\n",
            "Figure 1.  Diagram of some common structures for using neural networks with robots. \n",
            "A: Function approximating models are trained to approximate the mappings represented\n",
            "in a training set of pair-wise examples. B: Autoencoders can reduce complex or high-\n",
            "dimensional observations to a simple feature representation, often extracting intrinsic \n",
            "information from images. C: Recurrent models specialize in dynamics and temporal \n",
            "predictions. D: Policy models trained with reinforcement learning seek to plan the best \n",
            "decisions to make under possible future conditions.\n",
            "2.3 Convolutional layers\n",
            "Each of the various types of deep learning models are made by stacking multiple layers \n",
            "of regression models.  Within these models, different types of layers have evolved for \n",
            "various purposes.  One type of layer that warrants particular mention is convolutional \n",
            "layers [48].  Unlike traditional fully connected layers, convolutional layers use the same\n",
            "weights to operate all across the input space.  This significantly reduces the total \n",
            "number of weights in the neural network, which is especially important with images that\n",
            "typically have hundreds of thousands to millions of pixels that must be processed.  \n",
            "8Processing such images with fully connected layers would require more than (100K)2 to \n",
            "(1M)2 weights connecting each layer, which would be completely impractical.  \n",
            "Convolutional layers were inspired by cortical neurons in the visual cortex, which \n",
            "respond only to stimuli with a receptive field.  Since convolution approximates this \n",
            "behavior, convolutional layers may be expected to excel at image processing tasks.\n",
            "The pioneering works in neural networks with convolutional layers (CNNs) \n",
            "applied them to the task of image recognition [48, 49].  Many subsequent efforts built \n",
            "on these, but widespread interest in convolutional layers surged around 2012, when \n",
            "Krizhevsky used them to dominate in the ImageNet image recognition competition [41],\n",
            "and they were able to achieve super-human recognition on other notable image \n",
            "recognition benchmarks and competitions [27, 34, 35].  A flurry of research quickly \n",
            "followed seeking to establish deeper models with improved image processing \n",
            "capabilities [50, 51].\n",
            "Now, CNNs have become well established as a highly effective deep learning \n",
            "model for a diversity of image-based applications.  These applications include semantic \n",
            "image segmentation [52], object localization within images [53], scaling up images with\n",
            "super resolution [54], facial recognition [55, 56], scene recognition [57], and human \n",
            "gesture recognition [58].  Images are not the only type of signal for which CNNs excel.  \n",
            "Their capabilities are also effective with any type of signal that exhibits spatio-temporal \n",
            "proximity, such as speech recognition [59], and speech and audio synthesis [60].  \n",
            "Naturally, they have also started to dominate in signal processing domains used heavily \n",
            "in robotics, such as pedestrian detection using LIDAR [61] and micro-Doppler \n",
            "signatures [62], and depth-map estimation [63].  Recent works are even starting to \n",
            "combine signals from multiple modalities and combine them together for unified \n",
            "recognition and understanding [64].\n",
            "92.4 High level trajectory of deep learning with robotics\n",
            "Ultimately, the underlying philosophy that prevails in the deep learning community is \n",
            "that every part of a complex system can be made to “learn.”  Thus, the real power of \n",
            "deep learning does not come from using just one of the structures described in the \n",
            "previous section as a component in a robotics system, but in connecting parts of all of \n",
            "these structures together to form a full system that learns throughout.  This is where the \n",
            "“deep” in deep learning begins to make its impact – when each part of a system is \n",
            "capable of learning, the system as a whole can adapt in sophisticated ways.\n",
            "Neuroscientists are even starting to recognize that many of the patterns evolving \n",
            "within the deep learning community and throughout artificial intelligence are starting to \n",
            "mirror some of those that have previously evolved in the brain [65, 66].  Doya identified\n",
            "that supervised learning methods (Structures A and C) mirror the function of the \n",
            "cerebellum, unsupervised methods (Structure B) learn in a manner comparable to that of\n",
            "the cerebral cortex, and reinforcement learning is analogous with the basal ganglia [67]. \n",
            "Thus, the current trajectory of advancement strongly suggests that control of robots is \n",
            "leading toward full cognitive architectures that divide coordination tasks in a manner \n",
            "increasingly analogous with the brain [68-70].\n",
            "3.  Deep learning in robotics\n",
            "The robotics community has identified numerous goal for robotics in the next 5 to 20 \n",
            "years.  These include, but certainly are not limited to, human-like walking and running, \n",
            "teaching by demonstration, mobile navigation in pedestrian environments, collaborative \n",
            "automation, automated bin/shelf picking, automated combat recovery, and automated \n",
            "aircraft inspection and maintenance, and robotic disaster mitigation and recovery [71-\n",
            "75].  This paper identifies seven general challenges for robotics that are critical for \n",
            "10reaching these goals and for which DNN technology has high potential for impact:  \n",
            " \n",
            "Challenge 1:  Learning complex, high-dimensional, and novel dynamics.   Analytic \n",
            "derivation of complex dynamics requires human experts, is time consuming, and poses a\n",
            "trade-off between state dimensionality and tractability.  Making such models robust to \n",
            "uncertainty is difficult, and full state information is often unknown.  Systems that can \n",
            "quickly and autonomously adapt to novel dynamics are needed to solve problems such \n",
            "as grasping new objects, traveling over surfaces with unknown or uncertain properties, \n",
            "managing interactions between a new tool and/or environment, or adapting to \n",
            "degradation and/or failure of robot subsystems.  Also needed are methods to accomplish\n",
            "this for systems that possess hundreds (or even thousands) of degrees of freedom, \n",
            "exhibit high levels of uncertainty, and for which only partial state information is \n",
            "available.\n",
            "Challenge 2:  Learning control policies in dynamic environments.   As with dynamics, \n",
            "control systems that accommodate high degrees of freedom for applications such as \n",
            "multi-arm mobile manipulators, anthropomorphic hands, and swarm robotics are \n",
            "needed.  Such systems will be called upon to function reliably and safely in \n",
            "environments with high uncertainty and limited state information.\n",
            "Challenge 3:  Advanced manipulation.   Despite advances achieved over 3 decades of \n",
            "active research, robust and general solutions for tasks such as grasping deformable \n",
            "and/or complex geometries, using tools, and actuating systems in the environment (turn \n",
            "a valve handle, open a door, and so forth) remain elusive – especially in novel \n",
            "situations.  This challenge includes kinematic, kinetic, and grasp planning inherent in \n",
            "tasks such as these.\n",
            "11Challenge 4:  Advanced object recognition.   DNNs have already proven to be highly \n",
            "adept at recognizing and classifying objects [27, 34, 35].  Advanced application \n",
            "examples include recognizing deformable objects and estimating their state and pose for\n",
            "grasping, semantic task and path specification (e.g., go around the table, to the car, and \n",
            "open the trunk), and recognizing the properties of objects and surfaces such as sharp \n",
            "objects that could pose a danger to human collaborators or wet/slippery floors.\n",
            "Challenge 5:  Interpreting and anticipating human actions.   This challenge is critical if \n",
            "robots are to work with or amongst people in applications such as collaborative robotics\n",
            "for manufacturing, eldercare, autonomous vehicles operating on public thoroughfares, \n",
            "or navigating pedestrian environments.  It will enable teaching by demonstration, which \n",
            "will in turn facilitate task specification by individuals without expertise in robotics or \n",
            "programming.  This challenge may also be extended to perceiving human needs and \n",
            "anticipating when robotic intervention is appropriate.\n",
            "Challenge 6:  Sensor fusion & dimensionality reduction.   The proliferation of low-cost \n",
            "sensing technologies has been a boon for robotics, providing a plethora of potentially \n",
            "rich, high-dimensional, and multimodal data.  This challenge refers to methods for \n",
            "constructing meaningful and useful representations of state from such data.\n",
            "Challenge 7:  High-level task planning.   Robots will need to reliably execute high-level \n",
            "commands that fuse the previous six challenges to achieve a new level of utility, \n",
            "especially if they are to benefit the general public.  For example, the command “get the \n",
            "milk” must autonomously generate the lower-level tasks of navigating to/from the \n",
            "refrigerator, opening/closing the door, identifying the proper container (milk containers \n",
            "may take many forms), and securely grasping the container.\n",
            "Loosely speaking, these challenges form a sort of “basis set” for the goals \n",
            "mentioned above.  For example, human-like walking and running will rely heavily on \n",
            "12Challenges 1 (learning dynamics) and 2 (learning control policies), while teaching by \n",
            "demonstration will require advances in Challenges 4 (object recognition), 5 (interpreting\n",
            "human action), and 6 (sensor fusion).\n",
            "Table 1 categorizes recent robotics research that utilizes DNN technology \n",
            "according to these challenges, as well as the DNN structures discussed in the previous \n",
            "section.  From this several observations are made: First is that Structure A is clearly the \n",
            "most popular DNN architecture in the recent robotics literature.  This is likely explained\n",
            "by its intuitive nature, essentially learning to approximate the same function presented \n",
            "to it in the form of training samples .  It also requires the least amount of domain \n",
            "knowledge in DNNs to implement.  Robotics challenges, however, are not limited to the\n",
            "sort of classification and/or regression problems to which this structure is best suited.  \n",
            "Additional focus on applying Structures B, C, and D to robotics problems may very \n",
            "well catalyse significant advancement in many of the identified challenges.  One of the \n",
            "purposes of this paper is to emphasize the potential of the other structures to the robotics\n",
            "community.  \n",
            "Somewhat related is the fact that some cells in Table 1 are empty.  In the \n",
            "authors’ opinion, this is due to a lack of research focus rather than any inherent \n",
            "incompatibilities between challenges and structures.  In particular, the ability of \n",
            "Structure B to learn compact representations of state would be particularly useful for \n",
            "estimating the pose, state, and properties of objects (Challenge 4) and the state of human\n",
            "collaborators (Challenge 5).\n",
            "Table 1.  An overview of how DNN structures are used in the recent literature to address\n",
            "the seven challenges.\n",
            "DNN Structure\n",
            "A B C D\n",
            "Challenge 1\n",
            "  (Dynamics)[76, 78, 81, 85,\n",
            "115, 127] [87, 112, 113,\n",
            "115[115, 122] [125, 126]\n",
            "Challenge 2 [85,115,] [112] [122] [125,128]\n",
            "13  (Control)\n",
            "Challenge 3\n",
            "  (Manipulation)[79, 82-85, 123] [112] [123] [128]\n",
            "Challenge 4\n",
            "  (Object rec.)[79-81, 88, 123] [123] [128]\n",
            "Challenge 5\n",
            "  (Human actions)[77, 79, 123, 127] [123]\n",
            "Challenge 6\n",
            "  (Sensor fusion)[77, 81, 83, 84,\n",
            "86, 88][87, 116, 117] [114] [116, 117]\n",
            "Challenge 7\n",
            "  (high-level planning)[128]\n",
            "Table 1 also indicates limited application of DNNs to high-level task planning \n",
            "(Challenge 7).  One of the barriers to the application of DNNs is quantifying the quality \n",
            "of such decisions.  Standard benchmarks for decision quality are needed.  Once this is \n",
            "addressed, DNNs may very well be able to be the tool that allows roboticists to make \n",
            "progress on this very significant challenge.\n",
            "The balance of this section is categorized by DNN structure and is organized as \n",
            "follows: 1) a discussion of the structure’s role in robotics, 2) examples from the recent \n",
            "literature of how the structure is being applied in robotics, and 3) practical \n",
            "recommendations for applying the structure in robotics.\n",
            "3.1 Classifiers and discriminative models (Structure A) in robotics\n",
            "3.1.1 The role of Structure A in robotics\n",
            "Structure A involves using a deep learning model to approximate a function from \n",
            "sample input-output pairs.  This may be the most general-purpose deep learning \n",
            "structure, since there are many different functions in robotics that researchers and \n",
            "practitioners may want to approximate from sample observations.  Some examples \n",
            "include mapping from actions to corresponding changes in state, mapping from changes\n",
            "in state to the actions that would cause it, or mapping from forces to motions.  Whereas \n",
            "in some cases physical equations for these functions may already be known, there are \n",
            "many other cases where the environment is just too complex for these equations to yield\n",
            "14acceptable accuracy.  In such situations, learning to approximate the function from \n",
            "sample observations may yield significantly better accuracy.\n",
            "The functions that are approximated need not be continuous.  Function \n",
            "approximating models also excel at classification tasks, such as determining what type \n",
            "of object lies before the robot, which grasping approach or general planning strategy is \n",
            "best suited for current conditions, or what is the state of a certain complex object with \n",
            "which the robot is interacting.\n",
            "The next section reviews some of the many applications for classifiers, \n",
            "regression models, and discriminative models that have appeared in the recent literature \n",
            "with robotics.\n",
            "3.1.2 Examples in recent research\n",
            "Punjani and Abbeel [76] used a function approximating deep learning architecture with \n",
            "rectifiers to model the highly coupled dynamics of a radio-controlled helicopter, which \n",
            "is a challenging analytic derivation and difficult system identification problem.  \n",
            "Training data was obtained as a human expert flew the helicopter through various \n",
            "aerobatic maneuvers, and the DNN outperformed three state-of-the-art methods for \n",
            "obtaining helicopter dynamics by about 60%.\n",
            "Neverova et al. [77] modeled how the time between a driver’s head movement \n",
            "and the occurrence of a maneuver varies with vehicle speed.  The resulting system made\n",
            "predictions every 0.8 seconds based on the preceding 5 seconds of data and anticipated \n",
            "maneuvers about 3.5 seconds before they occurred, with 90.5% accuracy.\n",
            "A great many works have used function approximating models in the domains of\n",
            "(1) detection and perception, (2) grasping and object manipulation, and (3) scene \n",
            "15understanding and sensor fusion.  The following three subsections describe recent works\n",
            "in each of these domains.\n",
            "Detection and Perception .  DNNs have surged ahead of other models in the \n",
            "domains of detection and perception.  They are especially attractive models because \n",
            "they are capable of operating directly on high-dimensional input data instead of \n",
            "requiring feature vectors that are hand-engineered at design time by experts in machine \n",
            "learning and the particular application [1].  This reduces dependence on human experts, \n",
            "and the additional training time may be partially offset by reducing initial engineering \n",
            "effort.\n",
            "Mariolis, Peleka, and Kargakos [78] studied object and pose recognition for \n",
            "garments hanging from a single point, as if picked by a robotic gripper.  Training \n",
            "occurred on pants, shirts, and towels with various size, shape, and material properties, \n",
            "both flat and hanging from various grasp points.  On a test set of six objects different \n",
            "from those used in training, the authors achieved 100% recognition and were able to \n",
            "predict grasp point on the garment with a mean error of 5.3 cm.  These results were \n",
            "more accurate and faster than support vector machines.  Yang, Li, and Fermüller [79] \n",
            "trained a DNN to recognize 48 common kitchen objects and classify human grasps on \n",
            "them from 88 YouTube cooking videos.  Notably, the videos were not created with \n",
            "training robots in mind, exhibiting significant variation in background and scenery.  \n",
            "Power and precision grasps, and subclasses of each were classified.  The system \n",
            "achieved 79% object recognition accuracy and 91% grasp classification accuracy.  Chen\n",
            "et al. [80] identified the existence and pose of doors with a convolutional neural \n",
            "network and passed this information to a navigation algorithm for a mobile robot.  They \n",
            "suggest that navigating by such visual information can be superior to map-building \n",
            "methods in dynamic environments.  Gao, Hendricks, Kuchenbecker, and Darrell [81] \n",
            "16integrated both vison- and contact-based perception to classify objects with haptic \n",
            "adjectives (smooth, compliant, etc.).  If a robot could predict such information before or \n",
            "quickly after making contact with the objects, it can take appropriate actions such as \n",
            "adjusting its grip on fragile objects or avoiding slippery surfaces.  As with the other \n",
            "research studies in this paper, their method did not require manual design of feature \n",
            "vectors from domain-specific knowledge.\n",
            "Grasping and object manipulation.  Yu, Weng, Liang, and Xie [82] used a deep \n",
            "convolutional neural network to recognize five known objects resting on a flat surface \n",
            "and categorize their orientation into discretized categories.  The study focused on \n",
            "recognition and pose estimation, so grasp planning was limited to positioning a parallel \n",
            "gripper at the object’s center and aligned with the estimated angle.  Grasping success \n",
            "rates exceeded 90%.  Lenz, Lee, and Saxena [83] used deep learning to detect optimal \n",
            "grasps for objects from RGB-D (color + depth) images.  The network evaluates \n",
            "numerous potential grasps for the object and identifies the one with the highest potential\n",
            "for success without any prior knowledge of object geometry.  Trials on Baxter and PR2 \n",
            "robots resulted in successful grasps 84% and 89% of the time, respectively, compared to\n",
            "31% for a state-of-the art reference algorithm.  Rather than evaluating a set of potential \n",
            "grasps, Redmon and Angelova [84] trained a convolutional neural network to detect an \n",
            "acceptable grasp directly from RGB-D data in one pass.  They achieved 88% accuracy \n",
            "and claim real-time performance, arriving at a solution in under 100 milliseconds.\n",
            "Levine, Pastor, Krizhevsky, and Quillen [85] trained a convolutional neural \n",
            "network to evaluate the potential of a particular robot motion for successfully grasping \n",
            "common office objects from image data, and used a second network to provide \n",
            "continuous feedback through the grasping process.  Inspired by hand-eye coordination \n",
            "in humans, the system was robust to object movement and uncertainty in gripper \n",
            "17mechanics.  While this research may seem similar to visual servoing at first glance, it \n",
            "differs in that no hand-designed feature vectors were required for perception, and \n",
            "transfer functions for closed-loop control were not modeled analytically.\n",
            "Scene understanding and sensor fusion .  Extracting meaning from video or still \n",
            "scenes is another application where deep learning has made impressive progress.  \n",
            "Neverova, Wolf, Taylor, and Nebout [77] report on their first-place winning entry in the \n",
            "2014 ChaLearn Looking at People Challenge (http://gesture.chalearn.org), which \n",
            "challenges entrants to recognize 20 different Italian conversational gestures from 13,858\n",
            "separate RGB-D videos of different people performing those gestures.  Ouyang and \n",
            "Wang [86] simultaneously addressed four independent aspects of pedestrian detection \n",
            "with a single DNN: feature extraction, articulation and motion handling, occlusion \n",
            "handling, and classification.  They argue that their unified system avoids suboptimal \n",
            "interactions between these usually separate systems.  The integrated network was \n",
            "trained on over 60,000 samples from two publically available datasets.  Compared to 18 \n",
            "other approaches in the published literature, the authors’ system outperforms on both \n",
            "data sets by as much as 9%.  Wu, Yildirim, Lim, Freeman, and Tenenbaum [87] \n",
            "attempted to predict the physical outcome of dynamic scenes by vision alone, based on \n",
            "the premise that humans can often predict the outcome of a dynamic scene from visual \n",
            "information – for example, a block sliding down a ramp and impacting another block.  \n",
            "They use both simulations from a physics engine and physical trials for training.\n",
            "Deep learning has also been found to be effective at handling multimodal data \n",
            "generated in robotic sensing applications.  Previously mentioned examples include \n",
            "integrating vision and haptic sensor data [81] and incorporating both depth data and \n",
            "image information from RGB-D camera data [77, 83].  Additionally, Schmitz et al. [88] \n",
            "studied tactile object recognition with a TWENDY-ONE multi-finger hand, which \n",
            "18provides a multimodal set of 312 values from distributed skin sensors, fingertip forces, \n",
            "joint torques, actuator currents, and joint angles.  The system was trained on a set of \n",
            "twenty objects – some deliberately similar and some vastly different – handed to the \n",
            "robot in various poses.  The investigators achieved an 88% recognition rate, as \n",
            "compared to the 68% using other methods in the literature.\n",
            "3.1.3 Practical recommendations for working with Structure A\n",
            "Due to their large numbers of meta-parameters, DNNs have developed somewhat of a \n",
            "reputation for being difficult for non-experts to use effectively.  However, these \n",
            "parameters also provide significant flexibility, which is a major factor in their overall \n",
            "success.  Therefore, training DNNs requires the user to develop at least a basic level of \n",
            "familiarity with several concepts.  This section summarizes some of the most important \n",
            "concepts involved in training function approximating DNNs.  In particular, applying \n",
            "these techniques will help to address Challenge 4 (advanced object recognition), and to \n",
            "a lesser extent all of the other challenges as well.\n",
            "Although recent trends lean toward deeper and bigger models, a simple neural \n",
            "network with just one hidden layer and a standard sigmoid-shaped activation function \n",
            "will train much faster, and will provide a useful baseline to give meaning to any \n",
            "improvements from the use of deeper models.  When deeper models are used, Leaky \n",
            "Rectifiers tend to promote faster training by diminishing the effects of the vanishing \n",
            "gradient problem [41, 89], and improve accuracy through having a simpler monotonic \n",
            "derivative [91, 91].\n",
            "Since models with more weights have more flexibility to overfit to the training \n",
            "data, regularization is important role for training the best model.  Elastic net combines \n",
            "the well-established L1 and L2 regularization methods to promote robustness against \n",
            "19weight saturation and also promote sparsity in the weights [92].  Newer regularization \n",
            "methods, including drop-out [93] and drop-connect [94] have achieved even better \n",
            "empirical results.  Several regularization methods also exist specifically to improve \n",
            "robustness with autoencoders [95, 96].\n",
            "Special-purpose layers can also make a significant difference with DNNs.  It is a\n",
            "common practice to alternate between convolutional and max pooling layers.  The \n",
            "pooling layers reduce the overall number of weights in the network and also enable the \n",
            "model to learn to recognize objects independent of where they occur in the visual field \n",
            "[97].  Batch normalization layers can yield significant improvements in the rate of \n",
            "convergence by keeping the gradient in a range where it will affect the weights of all \n",
            "neurons [98].  And, residual layers can enable much deeper, and consequently more \n",
            "flexible, models to be trained [99].  \n",
            "To make effective use of deep learning models, it is important to train on one or \n",
            "more General Purpose Graphical Processing Units (GPGPUs) [17].  Many other ways of\n",
            "parallelizing deep neural networks have been attempted, but none of them yet yield the \n",
            "performance gains of GPGPUs [27].  Since DNNs require the use of so many \n",
            "specialized techniques, leveraging an existing toolkit that provides ready-made \n",
            "implementations is an imperative.  Fortunately, the deep learning community has been \n",
            "very helpful in releasing open source implementations of new developments, so many \n",
            "well-refined open source deep learning toolkits are now available:\n",
            "Tensorflow has recently surged in popularity [100].  Theano is a Python-based \n",
            "platform that provides General Purpose Graphical Processing Unit (GPGPU)-\n",
            "parallelization for deep learning [101].  Several popular toolkits build on top of Theano, \n",
            "including as Lasagne and Pylearn2 [102].  Keras is a wrapper around Tensorflow and \n",
            "Theano that seeks to simplify the interfaces for deep learning [103].  Torch offers a \n",
            "20Matlab-like environment written in Lua for deep learning, with particular emphasis on \n",
            "convolutional neural networks [104].  In C++, Caffe is one of the more popular toolkits \n",
            "for high-performance convolutional neural networks [105].  It also provides Python \n",
            "bindings.  Other C++ toolkits with GPU support are available [106, 107].  Some other \n",
            "toolkits with deep learning support include GroundHog, Theanets [100], Kaldi [109], \n",
            "and CURRENNT [110].  Kustikova gives a survey of many deep learning toolkits for \n",
            "image recognition [111].  Toolkits that employ other parallelization methods, besides \n",
            "GPGPUs, include Hadoop, Mahout, Spark, DeepLearning4j, and Scala.\n",
            "3.2 Generative and Unsupervised models (Structure B) in robotics\n",
            "3.2.1 The role of Structure B in robotics\n",
            "One of the characteristic capabilities that make humans so proficient at operating in the \n",
            "real world is their ability to understand what they perceive.  A similar capability is \n",
            "offered in autoencoders, a type of deep learning model that both encodes observations \n",
            "into an internal representation, then decodes it back to the original observation.  These \n",
            "models digest high-dimensional data and produce compact, low-dimensional internal \n",
            "representations that succinctly describe the meaning in the original observations [3].  \n",
            "Thus, auto-encoders are used primarily in cases where high-dimensional observations \n",
            "are available, but the user wants a low-dimensional representation of state.\n",
            "Generative models are closely related.  They utilize only the decoding portion of\n",
            "an autoencoder, and are useful for predicting observations.  Inference methods may be \n",
            "used with generative models to estimate internal representations of state without \n",
            "requiring an encoder to be trained at all.  In many ways, generative models may be \n",
            "considered to be the opposite of classifiers, or discriminative models, because they map \n",
            "21from a succinct representation to a full high-dimensional set of values similar to those \n",
            "that might typically be observed.\n",
            "3.2.2 Examples in recent research\n",
            "Finn et al. [112] trained a deep spatial auto-encoder on visual features to extract \n",
            "meaning from the observations and ultimately to achieve visuomotor control.  The \n",
            "autoencoder learned how robot actions affected the configuration of objects in the work \n",
            "envelope, and this model was used in a closed-loop controller.  Tasks included pushing a\n",
            "block, spooning material into a bowl, scooping with a spatula, and hanging a loop of \n",
            "rope on a hook.\n",
            "Wu et al. [87] used a generative model to anticipate outcomes from physics \n",
            "simulations.  Watter, Springenberg, Bodecker, and Reidmiller [113] also applied a \n",
            "generative model to model the nonlinear dynamics of simple physical systems and \n",
            "control them.  Noda, Arie, Suga, and Ogata [114] developed a novel deep learning \n",
            "solution involving both unsupervised methods and recurrent models for integrating \n",
            "multi-modal sensorimotor data, including RGB images, sound data, and joint angles.\n",
            "Another example of Structure B was demonstrated by Polydoros, Nalpantidis, \n",
            "and Kruger in modeling the inverse dynamics of a manipulator [115].  The network was \n",
            "trained using state variables recorded while operating under standard closed-loop \n",
            "control.  A “fading memory” feature allowed the DNN to adapt as dynamics changed \n",
            "with payload and mechanical wear.  Analytic dynamic models have difficulty coping \n",
            "with such changes, and are difficult to derive for highly compliant serial-elastic \n",
            "manipulators such as the new class of collaborative robots.  The authors report that their\n",
            "system outperforms the state-of-the-art in real-time learning evaluations and converges \n",
            "quickly, even with noisy sensor data.\n",
            "22Günther, Pilarski, Helfrich, Shen, and Diepold [116, 117] designed a DNN to \n",
            "automatically create meaningful feature vectors.  The network was able to extract low-\n",
            "dimensional features from high-dimensional camera images of welds in a laser welding \n",
            "process.  These features were subsequently used with other machine learning and \n",
            "control strategies to close the loop on the welding process.\n",
            "3.2.3 Practical recommendations for working with Structure B\n",
            "Autoencoders and other unsupervised DNN techniques are particularly well suited for \n",
            "addressing challenges pertaining to high-dimensional observations (1 and 6).  They both\n",
            "reduce dimensionality and extract meaningful representations of state, which is the first \n",
            "step in effective sensor fusion.\n",
            "Convolutional layers are well known to be effective for digesting images.  Since \n",
            "images are common with robots, autoencoders that use convolutional layers in their \n",
            "encoding portion tend to be especially effective for estimating state from images [118].  \n",
            "For the decoding portion of the autoencoder, convolutional layers offer little advantage. \n",
            "A somewhat less-known technique involves training the decoder to predict only a single\n",
            "pixel and parameterizing the decoder to enable the user to specify which pixel it should \n",
            "predict [119].  This approach has many analogies with convolution, and experimentally \n",
            "seems to lead to much faster training times.\n",
            "Regularization is particularly important for achieving good results with \n",
            "autoencoders, and specialized regularization methods have been designed particularly \n",
            "for autoencoders [120].  However, some experiments have shown that instead of heavily\n",
            "regularizing the encoder, it may even work better to entirely omit the encoder, and just \n",
            "use a standalone decoder [119].  In this configuration, the internal representation of state\n",
            "is inferred in a latent manner by using gradient descent until the internal representation \n",
            "23converges with the decoder.  Nonlinear dimensionality reduction methods have also \n",
            "been shown to be effective for pretraining such latent representations [119].\n",
            "3.3 Recurrent models (Structure C) in robotics\n",
            "3.3.1 The role of Structure C in robotics\n",
            "Recurrent models excel at learning to anticipate complex dynamics.  The recurrent \n",
            "connections in such models give them a form of “memory” that they can use to \n",
            "remember the current state.  This knowledge of state enables them to model the effects \n",
            "of time in a changing environment.\n",
            "3.3.2 Examples in recent research\n",
            "Jain et al. [121] trained a recurrent architecture to predict traffic maneuvers in a human-\n",
            "driven automobile in an effort to improve current collision avoidance systems which \n",
            "often do not intervene in time to avoid an accident.  Multimodal data inputs included \n",
            "video of the driver, video of the road in front of the car, dynamic state of the vehicle, \n",
            "GPS coordinates, and street maps of the area around the car.  \n",
            "Several researchers have used recurrent networks to deduce system dynamics \n",
            "directly from full observations.  Lenz, Knepper, and Saxena [122] modeled robotic food\n",
            "cutting with a knife.  This includes difficult-to-model effects such as friction, \n",
            "deformation, and hysteresis.  Food-knife surface contact changes through the cut, and so\n",
            "do the material properties of the food, as when passing between the peel and the center \n",
            "of a fruit.  Data obtained while operating under fixed-trajectory stiffness control was \n",
            "used to train the DNN on the system dynamics, and the resulting model was used to \n",
            "implement a model predictive control algorithm (a variation of Structure D).  Their \n",
            "system outperformed fixed-trajectory stiffness control, increasing mean cutting rate \n",
            "from 1.5 cm/s to 5.1 cm/s.\n",
            "24Hwang et al. [123] demonstrated gesture recognition with a recurrent model, and\n",
            "coordinated it with attention switching, object perception, and grasping.  The robot \n",
            "focused on a human collaborator, who gestured to one of two objects.  The robot then \n",
            "switched its focus to the indicated object, recognized the object, and found an \n",
            "acceptable grasp.  Their system achieved a successful grasp 85% of the time when \n",
            "simulated on an iCub humanoid robot.\n",
            "3.3.3 Practical recommendations for working with Structure C\n",
            "Recurrent models are well suited for addressing challenges pertaining to the \n",
            "complexities of temporal effects (Challenges 1, 5, and 7).  This section describes some \n",
            "recommendations for working effectively with recurrent models.\n",
            "Unfortunately, recurrent models have a somewhat negative reputation for being \n",
            "difficult to train.  One of the biggest problems is that training them with gradient \n",
            "methods requires unfolding through time, which effectively makes them behave as \n",
            "networks that are much deeper than they already are.  Given so much depth, the training\n",
            "gradients tend to become vanishingly small [10, 11].  This problem was largely solved \n",
            "by the error-carousel idea in LSTM networks [124], so it would be helpful to become \n",
            "familiar with that solution before attempting to work with recurrent models.\n",
            "When observations are very high dimensional, such as occurs when digital \n",
            "images are used with robots, a much simpler solution becomes possible.  This solution \n",
            "is to simply infer the intrinsic state from the images.  If the state can be inferred \n",
            "accurately, then the recurrent essentially goes away, making it possible to train the \n",
            "structure with example pairs presented in arbitrary order, just like Structure A [22].  \n",
            "Even if the internal state can only be inferred with a small degree of accuracy, this still \n",
            "25provides useful pre-training, which may significantly reduce the necessary training time \n",
            "with a recurrent model [119].\n",
            "3.4 Policy learning models (Structure D) in robotics\n",
            "3.4.1 The role of Structure D in robotics\n",
            "Learning a near optimal (or at least a reasonably acceptable) control policy is often the \n",
            "primary objective in combining machine learning with robotics.  The canonical model \n",
            "for using deep neural networks for learning a control policy is deep Q-learning [47].  It \n",
            "uses a DNN to model a table of Q-values, which are trained to converge to a \n",
            "representation of the values for performing each possible action in any state.  Although \n",
            "Structure D is quite similar to Structure A in terms of the model itself, they are trained \n",
            "in significantly different ways.  Instead of minimizing prediction error against a training\n",
            "set of samples, deep Q-networks seek to maximize long-term reward.  This is done \n",
            "through seeking a balance between exploration and exploitation that ultimately leads to \n",
            "an effective policy model.\n",
            "Ultimately, reinforcement learning models are useful for learning to operate \n",
            "dynamic systems from partial state information, and controllers based on deep \n",
            "reinforcement learning can be very computationally efficient at runtime [125].  They \n",
            "automatically infer priorities based on rewards that are obtained during training.  In \n",
            "theory, they provide a complete control policy learning system, but they do suffer from \n",
            "extremely slow training times.  Consequently, many of the works in the next section \n",
            "combine them with other approaches in order to seek greater levels of control accuracy \n",
            "and training speed.\n",
            "263.4.2 Examples in recent research\n",
            "Zhang, Kahn, and Levine [125] learned a control policy to implement a model \n",
            "predictive control guided search for autonomous aerial vehicles.  Reducing \n",
            "computational load in mobile robotics translates into power savings that increase range \n",
            "and/or improve performance.  Without the need for full state information, fewer onboard\n",
            "sensors are required, further reducing power consumption, cost, and weight.\n",
            "Deep reinforcement learning has also been used to control dynamic systems \n",
            "from video, without direct access to state information.  Lillicrap, Hunt, and Pritzel [126]\n",
            "trained a deep reinforcement learner based on pixel data over 20 simulated dynamic \n",
            "systems and developed a motion planning system that performed as well or better than \n",
            "algorithms that take advantage of the full state of the dynamic system.\n",
            "Finn, Levine, and Abbeel [127] used video of human experts performing various\n",
            "tasks to train a DNN to learn nonlinear cost functions (with Structure A).  Once these \n",
            "cost functions were learned, they could be used to train a reinforcement learner \n",
            "(Structure D) for motion planning.  They demonstrated the ability to complete tasks that\n",
            "involved complex 2nd-order dynamics and hard-to-model interactions between a \n",
            "manipulator and various objects, including 2D navigation, reaching, peg insertion, \n",
            "placing a dish, and pouring.  \n",
            "Visuomotor control requires an even closer integration between object \n",
            "perception and grasping, mapping image data directly to actuator control signals.  \n",
            "Levine, Finn, Darrell, and Abbeel [128] used reinforcement learning (Structure D) to \n",
            "show that this can be superior to separate systems for perception and control.  Test \n",
            "applications included shape sorting, screwing a cap onto a bottle, fitting a hammer claw \n",
            "to a nail, and placing a coat hanger on a rack.  The resulting system could perform the \n",
            "tasks reliably, even with moderate visual distractors.\n",
            "27As mentioned earlier, Günther, Pilarski, Helfrich, Shen, and Diepold [116, 117] \n",
            "combined autoencoders with reinforcement learning models to control a laser welding \n",
            "system from camera images.\n",
            "3.4.3 Practical recommendations for working with Structure D\n",
            "Policy learning models are ultimately the solution to addressing Challenges 2 (learning \n",
            "control policies in dynamic environments) and 7 (high-level task planning).  Perhaps, \n",
            "the biggest difficulty when working with reinforcement learning models, however, is the\n",
            "huge amount of computation time necessary to train them.  Although such models are \n",
            "highly efficient after training, they tend to require significantly more training pattern \n",
            "presentations before they converge to represent reliable control policies.  Taking care to \n",
            "find an efficient GPU-optimized implementation, therefore, can make a big difference.  \n",
            "Another important technique is to train in simulation before attempting to train with an \n",
            "actual robot.  This reduces wear on physical equipment, as well reduces training time.  \n",
            "Even if only a crude simulation is available, a model that has been pre-trained on a \n",
            "similar challenge will converge much more quickly to fit the real challenge than one \n",
            "that was trained from scratch.\n",
            "Since robots often operate in a space with continuous actions, traditional Q-\n",
            "learning is not directly applicable.  Actor-critic models, however, address this problem \n",
            "nicely.  They regress actions in conjunction with the continuous Q-table, and lead to a \n",
            "final model that directly computes the best action given the current observation, which \n",
            "is well suited for robotics applications [129].\n",
            "Another important consideration is the exploration policy.  The traditional \n",
            "epsilon-greedy exploration policy leads to much higher computational training \n",
            "28requirements than modern approaches [130, 131].  It is, therefore, advantageous to train \n",
            "Structure D in an approach that intelligently explores novel states.\n",
            "4.  Current Shortcomings of DNNs for Robotics\n",
            "For all of its benefits, deep learning does pose some drawbacks.  Perhaps most \n",
            "significant is the volume of training data required, which is particularly problematic in \n",
            "robotics because generating training data on physical systems can be expensive and time\n",
            "consuming.  For instance, Levine et al. [85] used 14 robots to collect over 800,000 grasp\n",
            "attempts over a period of 2 months.  Jain et al. [121] trained their traffic maneuver \n",
            "prediction system on 1180 miles of high- and low-speed driving with 10 different \n",
            "drivers.  Punjani and Abbeel [76] required repeated demonstrations of helicopter \n",
            "aerobatic maneuvers by a human expert.  Neverova et al. [77] had access to over 13,000\n",
            "videos of conversations, and Ouyang and Wang [86] had access to 60,000 samples for \n",
            "pedestrian detection.  Pinto and Gupta [132] needed 700 hours of robot time to generate \n",
            "a data set of 50,000 grasps for the training of a convolutional neural network.\n",
            "Despite this, the literature does contain clever approaches to mitigating this \n",
            "disadvantage.  One approach entails using simulation to generate virtual training data.  \n",
            "For example, Mariolis et al. 78] pre-trained their garment pose recognition networks on \n",
            "a large synthetic data set created in simulation using 3D graphics software.  Kappler, \n",
            "Bohg, and Schaal [133] generated a database of over 300,000 grasps on over 700 \n",
            "objects in simulation, generating physics-based grasp quality metrics for each and using \n",
            "this to classify grasp stability automatically.  They validated via human classification of \n",
            "grasps and concluded that the computer- and human-generated labeling had good \n",
            "correlation.  Another strategy is leveraging training data through digital manipulation.  \n",
            "Neverova et al. [77] faced the challenge that speed of conversational gestures varies \n",
            "29significantly among different people.  They varied video playback speed to simulate this\n",
            "temporal variance, expanding their training set without the need to acquire additional \n",
            "samples.  Still other researchers utilizing reinforcement learning, such as Polydoros et \n",
            "al. [115] and Zhang et al. [125], automated training using alternative control systems \n",
            "during the learning phase.\n",
            "Training time is another challenge associated with the sheer size of DNNs.  \n",
            "Typical models involve up to millions of parameters and can take days to train on \n",
            "parallel hardware, which is practical only for frequently repeated tasks that provide \n",
            "adequate payback on training time invested.  One way to reduce training time is \n",
            "distributing a task among multiple, smaller DNNs.  Mariolis et al. [78] trained two \n",
            "DNNs: One performed object classification, and its result was passed to a second \n",
            "network for pose recognition.  This multi-step approach sped both training and \n",
            "classification at runtime.  Lenz et al. [122] employed a two-stage network design for \n",
            "grasp detection.  The first DNN had relatively few parameters.  Sacrificing accuracy for \n",
            "speed, it eliminated highly unlikely grasps.  The second stage had more parameters, \n",
            "making it more accurate, but was relatively quick since it did not need to consider \n",
            "unlikely grasps.  They found the combination to be robust and computationally efficient.\n",
            "It should be noted, however, that this strategy represents a tradeoff with other \n",
            "researchers’ suggestions that integrating multiple functions within a single network \n",
            "results in better performance [86, 128].\n",
            "The work of Zhang et al. [125] highlights two additional challenges.  First, \n",
            "unsupervised learning is not practical for robotic systems where a single failure is \n",
            "catastrophic, as in aerial vehicles.  Second, providing the necessary computational \n",
            "resources for deep learning in a system that is sensitive to weight, power consumption, \n",
            "and cost is often not practical.  The authors trained their aerial systems using a ground-\n",
            "30based control system communicating wirelessly with the vehicle.  This made training \n",
            "safe and automatic, and allowed them to use off-board computing resources for training.\n",
            "5.  Conclusion\n",
            "Deep learning has shown promise in significant sensing, cognition, and action \n",
            "problems, and even the potential to combine these normally separate functions into a \n",
            "single system.  DNNs can operate on raw sensor data and deduce key features in that \n",
            "data without human assistance, potentially greatly reducing up-front engineering time.  \n",
            "They are also adept at fusing high-dimensional, multimodal data.  Improvement with \n",
            "experience has been demonstrated, facilitating adaptation in the dynamic, unstructured \n",
            "environments in which robots operate.\n",
            "Some remaining barriers to the adoption of deep learning in robotics include the \n",
            "necessity for large training data and long training times.  Generating training data on \n",
            "physical systems can be relatively time consuming and expensive.  One promising trend\n",
            "is crowdsourcing training data via cloud robotics [134].  It is not even necessary that \n",
            "this data be from other robots, as shown by Yang’s use of general-purpose cooking \n",
            "videos for object and grasp recognition [79].  Regarding training time, local parallel \n",
            "processing [17] and increases in raw processing speed have led to significant \n",
            "improvements.  Distributed computing offers the potential to direct more computing \n",
            "resources to a given problem [88] but can be limited by communication speeds [2].  \n",
            "There may also be algorithmic ways of making the training process more efficient yet to\n",
            "be discovered.  For example, deep learning researchers are actively working on \n",
            "directing the network’s attention to the most relevant subspaces within the data and \n",
            "applying biologically inspired, sparse DNNs with fewer synaptic connections to train \n",
            "[27].\n",
            "31Ultimately, the trends are moving toward greater levels of cognition, and some \n",
            "researchers even believe that deep learning may achieve human-level abilities in the \n",
            "near future [1, 134].  However, deep learning still has many obstacles to overcome \n",
            "before achieving such an ambitious objective.  Currently, cognitive training datasets do \n",
            "not even exist [134].  Although DNNs excel at 2D image recognition, they are known to\n",
            "be highly susceptible to adversarial samples [135], and they still struggle to model 3D \n",
            "spatial layouts with object invariance [65].  Currently, DNNs appear to be powerful \n",
            "tools for practitioners in robotics, but only time will tell whether they can really deliver \n",
            "the capabilities that are needed for dexterous adaptation in general environments.\n",
            "References\n",
            "[1] LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436-444.\n",
            "[2] Jordan MI, Mitchell TM. Machine learning: arends, perspectives, and prospects. \n",
            "Science. 2015;349(6245):255-260.\n",
            "[3] Böhmer W, Springenberg JT, Boedecker J, et al. Autonomous learning of state \n",
            "representations for control: an emerging field aims to autonomously learn state \n",
            "representations for reinforcement learning agents from their real-world sensor \n",
            "observations. KI-Künstliche Intelligenz. 2015;29(4):353-362.\n",
            "[4] Stigler SM. Gauss and the invention of least squares. Ann of Statistics. \n",
            "1981;9(3):465-474.\n",
            "[5] Haykin S. Neural networks: a comprehensive foundation. 2nd ed. Upper Saddle \n",
            "River, New Jersey: Prentice Hall; 2004.\n",
            "[6] Bryson AE, Denham WF, Dreyfus SE. Optimal programming problems with \n",
            "inequality constraints. AIAA Journal. 1963;1(11):2544-2550.\n",
            "[7] Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-\n",
            "propagating errors. Nature. 1986;323:533-536.\n",
            "[8] Werbos P. Beyond regression: new tools for prediction and analysis in the behavioral\n",
            "sciences. [Ph.D. dissertation]. Dept. Statistics, Harvard Univ.; 1974.\n",
            "[9] Cybenko G. Approximation by superpositions of a sigmoidal function. Math of \n",
            "Control, Signals and Sys. 1989;2(4):303-314.\n",
            "[10] Hochreiter S. Untersuchungen zu dynamischen neuronalen netzen. [Master's \n",
            "thesis]. Institut Fur Informatik, Technische Universitat; 1991.\n",
            "32[11] Hochreiter S. The vanishing gradient problem during learning recurrent neural nets \n",
            "and problem solutions. Int. J. of Uncertainty, Fuzziness and Knowledge-Based \n",
            "Syst. 1998;6(2).\n",
            "[12] Miyamoto H, Kawato M, Setoyama T, & Suzuki R. Feedback-error-learning neural \n",
            "network for trajectory control of a robotic manipulator. Neural Networks \n",
            "1998;1(3):251-265.\n",
            "[13] Lewis FW, Jagannathan S, & Yesildirak A. (1998). Neural network control of robot \n",
            "manipulators and non-linear systems . CRC Press.\n",
            "[14] Miller WT, Werbos PJ, & Sutton RS. (1995). Neural networks for control . MIT \n",
            "Press.\n",
            "[15] Lin CT., & Lee CSG. Neural-network-based fuzzy logic control and decision \n",
            "system. IEEE Transactions on Computers . 1991;40(12):1320-1336.\n",
            "[16] Pomerleau DA. (1989). ALVINN, an autonomous land vehicle in a neural network  \n",
            "(No. AIP-77). Carnegie Mellon University, Computer Science Department.\n",
            "[17] Oh K, Jung K. GPU implementation of neural networks. Pattern Recognition. \n",
            "2004;37(6):1311-1314.\n",
            "[18] Hinton GE, Osindero S, Teh Y. A fast learning algorithm for deep belief nets. \n",
            "Neural Computation. 2006;18(7):1527-1554.\n",
            "[19] Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks. \n",
            "Advances in Neural Information Process. Syst. 25; 2012.\n",
            "[20] Tani J, Ito M, & Sugita Y. Self-organization of distributedly represented multiple \n",
            "behavior schemata in a mirror system: reviews of robot experiments using \n",
            "RNNPB. Neural Networks. 2004;17(8):1273-1289.\n",
            "[21] Ijspeert AJ. Central pattern generators for locomotion control in animals and \n",
            "robots: a review. Neural Networks. 2008;21(4):642-653.\n",
            "[22] Gashler M, Martinez T. Temporal nonlinear dimensionality reduction. Neural \n",
            "Networks (IJCNN), 2011 International Joint Conference on; 2011. p. 1959-1966.\n",
            "[23] Pomerleau DA (2012). Neural network perception for mobile robot guidance  (Vol. \n",
            "239). Springer Science & Business Media.\n",
            " [24] Thrun S. Learning to play the game of chess. Advances in Neural Inform. Process. \n",
            "Syst.: Proc. of the 1994 Conf.\n",
            "[25] Campbell M, Hoane AJ, Hsu F. Deep blue. Artificial Intelligence. 2002;134(1):57-\n",
            "83.\n",
            "[26] Pinto N, Cox DD, DiCarlo JJ. Why is real-world visual object recognition hard? \n",
            "PLoS Computational Biology. 2008;4(1).\n",
            "[27] Schmidhuber J. Deep learning in neural networks: an overview. Neural Networks. \n",
            "2015;6(1)85-117.\n",
            "33[28] Graves A, Liwicki M, Fernández S, et al. A novel connectionist system for \n",
            "unconstrained handwriting recognition. Pattern Anal and Machine Intell., IEEE \n",
            "trans. on. 2009;31(5):855-868.\n",
            "[29] Yang M, Ji S, Xu W, et al. Detecting human actions in surveillance videos. TREC \n",
            "Video Retrieval Evaluation Workshop; 2009.\n",
            "[30] Lin M, Chen Q, Yan S. Network in network. 2013. Available: \n",
            "https://arxiv.org/abs/1312.4400\n",
            "[31] Ciresan D, Giusti A, Gambardella LM, et al. Deep neural networks segment \n",
            "neuronal membranes in electron microscopy images. Advances in Neural \n",
            "Information Processing Sys 25; 2012.\n",
            "[32] Roux L, Racoceanu D, Lomenie N, et al. Mitosis detection in breast cancer \n",
            "histological images an ICPR 2012 contest. J Pathol Inform. 2013;4(8).\n",
            "[33] Cireşan DC, Giusti A, Gambardella LM, et al. Mitosis detection in breast cancer \n",
            "histology images with deep neural networks. In: K. Mori, I. Sakuma, Y. Sato, C. \n",
            "Barillot and N. Navab, editors. Medical Image Computing and Computer-\n",
            "Assisted Intervention–MICCAI 2013. Springer; 2013.\n",
            "[34] Cireşan D, Meier U, Masci J, et al. A committee of neural networks for traffic sign \n",
            "classification. Neural Networks (IJCNN), 2011 Int. Joint Conf. on; 2011. p. \n",
            "1918-1921.\n",
            "[35] Ciresan D, Meier U, Schmidhuber J. Multi-column deep neural networks for image\n",
            "classification. Computer Vision and Pattern Recognition (CVPR), 2012 IEEE \n",
            "Conference on; 2012. p. 3642-3649.\n",
            "[36] Dunne RA, & Campbell NA. On the pairing of the softmax activation and cross-\n",
            "entropy penalty functions and the derivation of the softmax activation function. \n",
            "In Proc. 8th Aust. Conf. on the Neural Networks, Melbourne 1997;181  (Vol. \n",
            "185).\n",
            " [37] Wilson DR, Martinez TR. The general inefficiency of batch training for gradient \n",
            "descent learning. Neural Networks. 2003;16(10):1429-1451.\n",
            "[38] Tieleman T, Hinton G. (2012). Lecture 6.5-rmsprop: divide the gradient by a \n",
            "running average of its recent magnitude. COURSERA: Neural Networks for \n",
            "Machine Learning, 4(2).\n",
            "[39] Kingma D, Ba J. (2014). Adam: a method for stochastic optimization. arXiv \n",
            "preprint arXiv:1412.6980 .\n",
            "[40] Vincent P, Larochelle H, Lajoie I, et al. Stacked denoising dutoencoders: learning \n",
            "useful representations in a deep network with a local denoising criterion. J Mach\n",
            "Learning Research. 2010;11:3371-3408.\n",
            "34[41] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep \n",
            "convolutional neural networks. Advances in Neural Information Processing \n",
            "Systems 25; 2012.\n",
            "[42] LeCun Y, Bengio Y. Convolutional networks for images, speech, and time series. In\n",
            "M. Arbib, editor. The Handbook of Brain Theory and Neural Networks. 2nd \n",
            "edition. Cambridge, MA: MIT Press; 2003.\n",
            "[43] Werbos PJ. Backpropagation through time: what it does and how to do it. Proc. \n",
            "IEEE. 1990;78(10):1550-1560.\n",
            "[44] Sjöberg J, Zhang Q, Ljung L, et al. Nonlinear black-box modeling in system \n",
            "identification: a unified overview. Automatica. 1995;31(12):1691-1724.\n",
            "[45] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation. \n",
            "1997;9(8):1735-1780.\n",
            "[46] Atkeson CG, Santamaria JC. A comparison of direct and model-based \n",
            "reinforcement learning. Robotics and Automation, IEEE Int. Conf. on; \n",
            "Albuquerque, NM. 1997. p. 3557-3564.\n",
            "[47] Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep \n",
            "reinforcement learning. Nature. 2015;518:529-533.\n",
            "[48] LeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD. \n",
            "Backpropagation applied to handwritten zip code recognition. Neural \n",
            "Computation. 1989;1(4), 541-551.\n",
            "[49] LeCun Y, Bottou L, Bengio Y, Haffner P.. Gradient-based learning applied to \n",
            "document recognition. Proceedings of the IEEE . 1998;86(11):2278-2324.\n",
            "[50] Simonyan K, Zisserman A. (2014). Very deep convolutional networks for large-\n",
            "scale image recognition. arXiv preprint arXiv:1409.1556 .\n",
            "[51] Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D., ... Rabinovich A. \n",
            "(2015). Going deeper with convolutions. In Proc of the IEEE Conference on \n",
            "Computer Vision and Pattern Recognition  (pp. 1-9).\n",
            "[52] Chen LC, Papandreou G, Kokkinos I, Murphy K, & Yuille AL. (2014). Semantic \n",
            "image segmentation with deep convolutional nets and fully connected crfs. \n",
            "arXiv preprint arXiv:1412.7062 .\n",
            "[53] Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, LeCun Y. (2013). Overfeat: \n",
            "integrated recognition, localization and detection using convolutional networks. \n",
            "arXiv preprint arXiv:1312.6229 .\n",
            "[54] Dong C, Loy CC, He K, Tang X. (2014, September). Learning a deep \n",
            "convolutional network for image super-resolution. In European Conf on \n",
            "Computer Vision (pp. 184-199). Springer International Publishing.\n",
            "35[55] Sun Y, Wang X, Tang X. (2013). Deep convolutional network cascade for facial \n",
            "point detection. In Proc of the IEEE Conf on Computer Vision and Pattern \n",
            "Recognition (pp. 3476-3483).\n",
            "[56] Taigman Y, Yang M, Ranzato MA, Wolf L. (2014). Deepface: closing the gap to \n",
            "human-level performance in face verification. In Proc of the IEEE Conf on \n",
            "Computer Vision and Pattern Recognition  (pp. 1701-1708).\n",
            "[57] Zhou B, Lapedriza A, Xiao J, Torralba A, Oliva A. (2014). Learning deep features \n",
            "for scene recognition using places database. In Advances in Neural Information \n",
            "Processing Sys (pp. 487-495).\n",
            "[58] Ji S, Xu W, Yang M, Yu K.. 3D convolutional neural networks for human action \n",
            "recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence . \n",
            "2013;35(1):221-231.\n",
            "[59] Graves A, Mohamed AR, Hinton G. (2013, May). Speech recognition with deep \n",
            "recurrent neural networks. In Acoustics, Speech and Signal Proc (ICASSP), \n",
            "2013 IEEE Int Conf on  (pp. 6645-6649). IEEE.\n",
            "[60] Bakshi BR, Stephanopoulos G. Wave ‐net: a multiresolution, hierarchical neural \n",
            "network with localized learning. AIChE Journal. 1993;39(1):57-81.\n",
            "[61] Arel I, Rose DC, Karnowski TP. Deep machine learning - a new frontier in artificial\n",
            "intelligence research [research frontier]. IEEE Computational Intelligence \n",
            "Magazine. 2010;5(4);13-18.\n",
            "[62] Kim Y, Moon T. . Human detection and activity classification based on micro-\n",
            "doppler signatures using deep convolutional neural networks. IEEE Geoscience \n",
            "and Remote Sensing Letters . 2016;13(1):8-12.\n",
            "[63] Liu F, Shen C, Lin G, Reid I.. Learning depth from single monocular images using \n",
            "deep convolutional neural fields. IEEE Transactions on Pattern Analysis and \n",
            "Machine Intelligence  2016;38(10):2024-2039.\n",
            " [64] Eitel A, Springenberg JT, Spinello L, Riedmiller M, Burgard W. (2015, \n",
            "September). Multimodal deep learning for robust rgb-d object recognition. In \n",
            "Intelligent Robots and Sys (IROS), 2015 IEEE/RSJ International Conference on  \n",
            "(pp. 681-687). IEEE.\n",
            "[65] Bohannon J. Helping robots see the big picture. Science. 2014;346(6206):186-187.\n",
            "[66] Sweller J. (2008, September). Evolutionary bases of human cognitive architecture: \n",
            "implications for computing education. In Proc of the Fourth Int Workshop on \n",
            "Computing Education Research  (pp. 1-2). ACM.\n",
            " [67] Doya K. What are the computations of the cerebellum, the basal ganglia and the \n",
            "cerebral cortex? Neural Networks, 1999;12(7):961-974.\n",
            "[68] Langley P, Laird JE, Rogers S. Cognitive architectures: research issues and \n",
            "challenges. Cognitive Sys Research  2009;10(2):141-160.\n",
            "36[69] Sun R. (2006). Cognition and multi-agent interaction: from cognitive modeling to \n",
            "social simulation. Cambridge University Press.\n",
            "[70] Duch W, Oentaryo RJ, Pasquier M. (2008, June). Cognitive Architectures: Where \n",
            "do we go from here? In AGI (Vol. 171, pp. 122-136).\n",
            "[71] A roadmap for US robotics: from internet to robotics, 2016 edition.  \n",
            "[72] World Technology Evaluation Center, Inc. International Assessment of Research \n",
            "and Development in Robotics. Baltimore, MD, USA; 2006.\n",
            "[73] FY2009-2034 Unmanned systems integrated roadmap. Washington, DC: \n",
            "Department of Defence (US); 2009.\n",
            "[74] Material Handling Institute. Material handling and logistics U.S. roadmap 2.0.  \n",
            "2017.\n",
            "[75] DARPA Robotics Challenge [Internet]. [cited 2017 May 20]. Available from:  \n",
            "http://www.darpa.mil/program/darpa-robotics-challenge\n",
            "[76] Punjani AP, Abbeel P. Deep learning helicopter dynamics models. Robotics and \n",
            "Automation (ICRA), 2015 IEEE International Conference on; 2015. p. 3223-\n",
            "3230.\n",
            "[77] Neverova N, Wolf C, Taylor GW, et al. Multi-scale deep learning for gesture \n",
            "detection and localization. Computer Vision-ECCV 2014 Workshops; 2014. p. \n",
            "474-490.\n",
            "[78] Mariolis I, Peleka G, Kargakos A, et al. Pose and category recognition of highly \n",
            "deformable objects using deep learning. Advanced Robotics (ICAR), 2015 \n",
            "International Conference on; Istanbul. 2015. p. 655-662.\n",
            "[79] Yang Y, Li Y, Fermüller C, et al. Robot learning manipulation action plans by \n",
            "watching unconstrained videos from the world wide web. 29th AAAI Conference\n",
            "on Artificial Intelligence (AAAI-15); Austin, TX. 2015.\n",
            "[80] Chen W, Qu T, Zhou Y, et al. Door recognition and deep learning algorithm for \n",
            "visual based robot navigation. Robotics and Biomimetics (ROBIO), 2014 IEEE \n",
            "International Conference on; Bali. 2014. p. 1793-1798.\n",
            "[81] Gao Y, Hendricks LA, Kuchenbecker KJ, et al. Deep learning for tactile \n",
            "understanding from visual and haptic data. 2015. Available: \n",
            "http://arxiv.org/abs/1511.06065\n",
            "[82] Yu J, Weng K, Liang G, et al. A vision-based robotic grasping system using deep \n",
            "learning for 3D object recognition and pose estimation. Robotics and \n",
            "Biomimetics (ROBIO), 2013 IEEE International Conference on; Shenzhen. \n",
            "2013. p. 1175-1180.\n",
            "[83] Lenz I, Lee H, Saxena A. Deep learning for detecting robotic grasps. Int J Robotics \n",
            "res. 2015;34(4-5):705-724.\n",
            "37[84] Redmon J, Angelova A. Real-time grasp detection using convolutional neural \n",
            "networks. 2015 IEEE International Conference on Robotics and Automation \n",
            "(ICRA); Seattle, WA. 2015. p. 1316-1322.\n",
            "[85] Levine S, Pastor P, Krizhevsky A, et al. Learning hand-eye coordination for robotic\n",
            "grasping with deep learning and large-scale data collection. 2016. Available: \n",
            "http://arxiv.org/abs/1603.02199\n",
            "[86] Ouyang W, Wang X. Joint deep learning for pedestrian detection. Computer Vision,\n",
            "2013 IEEE Int. Conf. on; Sidney, VIC. 2013. p. 2056-2063.\n",
            "[87] Wu J, Yildirim I, Lim JJ, et al. Galileo: Perceiving physical object properties by \n",
            "integrating a physics engine with deep learning. Advances in Neural Information\n",
            "Processing Systems 28; 2015.\n",
            "[88] Schmitz A, Bansho Y, Noda K, et al. Tactile object recognition using deep learning \n",
            "and dropout. Humanoid Robots, 2014 14th IEEE-RAS Int. Conf. on; 2014. p. \n",
            "1044-1050.\n",
            "[89] Zheng H, Yang Z, Liu , Liang J, Li Y. (2015, July). Improving deep neural \n",
            "networks using softplus units. In Neural Networks (IJCNN), 2015 Int Joint Conf\n",
            "on (pp. 1-4). IEEE.\n",
            "[90] Xu B, Wang N, Chen T, Li M. (2015). Empirical evaluation of rectified activations \n",
            "in convolutional network. arXiv preprint arXiv:1505.00853 .\n",
            "[91] Gashler MS, Ashmore SC. (2016). Modeling time series data with deep Fourier \n",
            "neural networks. Neurocomputing, 188, 3-11.\n",
            "[92] Zou H, Hastie T. Regularization and variable selection via the elastic net. J Royal \n",
            "Statistical Society: Series B (Statistical Methodology) , 2005;67(2):301-320.\n",
            "[93] Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a \n",
            "simple way to prevent neural networks from overfitting. Journal of Machine \n",
            "Learning Research , 2014;15(1):1929-1958.\n",
            "[94] Wan L, Zeiler M, Zhang S, Cun YL, Fergus R. (2013). Regularization of neural \n",
            "networks using dropconnect. In Proc of the 30th Int Conf on Machine Learning \n",
            "(ICML-13) (pp. 1058-1066).\n",
            "[95] Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. (2011). Contractive auto-\n",
            "encoders: explicit invariance during feature extraction. In Proc of the 28th Int \n",
            "Conf on Machine Learning (ICML-11)  (pp. 833-840).\n",
            "[96] Li Z, Fan Y, Liu W. The effect of whitening transformation on pooling operations \n",
            "in convolutional autoencoders. EURASIP J on Advances in Signal Proc , 2015;\n",
            "(1):37.\n",
            "[97] Jarrett K, Kavukcuoglu K, LeCun Y. (2009, September). What is the best multi-\n",
            "stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th \n",
            "Int Conf on (pp. 2146-2153). IEEE.\n",
            "38[98] Ioffe S, Szegedy C. (2015). Batch normalization: accelerating deep network \n",
            "training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 .\n",
            "[99] He K, Zhang X, Ren S, Sun J. (2016). Deep residual learning for image \n",
            "recognition. In Proc of the IEEE Conf on Computer Vision and Pattern \n",
            "Recognition (pp. 770-778).\n",
            "[100] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, ... Ghemawat S. \n",
            "(2016). Tensorflow: large-scale machine learning on heterogeneous distributed \n",
            "systems. arXiv preprint arXiv:1603.04467 .\n",
            "[101] Bergstra J, Bastien F, Breuleux O, Lamblin P, Pascanu R, Delalleau O, ... Bengio \n",
            "Y. (2011). Theano: deep learning on gpus with python. In NIPS 2011, \n",
            "BigLearning Workshop, Granada, Spain  (Vol. 3).\n",
            "[102] Goodfellow IJ, Warde-Farley D, Lamblin P, Dumoulin V, Mirza M, Pascanu R,... \n",
            "Bengio,Y. (2013). Pylearn2: a machine learning research library. arXiv preprint \n",
            "arXiv:1308.4214.\n",
            "[103] Chollet F. (2015). Keras: deep learning library for theano and tensorflow. URL: \n",
            "https://keras. io/k.\n",
            "[104] Collobert R, Kavukcuoglu K, Farabet C. (2011). Torch7: a matlab-like \n",
            "environment for machine learning. In BigLearn, NIPS Workshop  (No. EPFL-\n",
            "CONF-192376).\n",
            "[105] Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, ... Darrell T. \n",
            "(2014, November). Caffe: convolutional architecture for fast feature embedding. \n",
            "In Proc 22nd ACM Int Conf on Multimedia  (pp. 675-678). ACM.\n",
            "[106] Attardi G. (2015, June). Deepnl: a deep learning nlp pipeline. In Proc of NAACL-\n",
            "HLT (pp. 109-115).\n",
            "[107] Gashler M. Waffles: a machine learning toolkit. J Machine Learning Res , \n",
            "2011;12(Jul):2383-2387.\n",
            "[108] Johnson L. Theanets documentation. Url: \n",
            "http://theanets.readthedocs.org/en/stable/\n",
            "[109] Povey D, Ghoshal A, Boulianne G, Burget L, Glembek O, Goel N,... Silovsky J. \n",
            "(2011). The Kaldi speech recognition toolkit. In IEEE 2011 workshop on \n",
            "automatic speech recognition and understanding  (No. EPFL-CONF-192584). \n",
            "IEEE Signal Processing Society.\n",
            "[110] Weninger F, Bergmann J, Schuller BW. Introducing CURRENNT: the munich \n",
            "open-source CUDA recurrent neural network toolkit. J Machine Learning \n",
            "Research, 2015;16(3):547-551.\n",
            " [111] Kustikova VD, Druzhkov PN. (2014). A survey of deep learning methods and \n",
            "software for image classification and object detection. OGRW2014, 5.\n",
            "39[112] Finn C, Tan XY, Duan Y, et al. Deep spatial autoencoders for visuomotor learning.\n",
            "2015. Available: http://arxiv.org/abs/1509.06113\n",
            "[113] Watter M, Springenberg J, Boedecker J, et al. Embed to control: a locally linear \n",
            "latent dynamics model for control from raw images. Advances in Neural \n",
            "Information Proc Sys 28; 2015.\n",
            "[114] Noda K, Arie H, Suga Y, et al. Multimodal integration learning of robot behavior \n",
            "using deep neural networks. Robotics and Autonomous Sys. 2014;62(6):721-\n",
            "736.\n",
            "[115] Polydoros AS, Nalpantidis L, Kruger V. Real-time deep learning of robotic \n",
            "manipulator inverse dynamics. Intelligent Robots and Systems (IROS), 2015 \n",
            "IEEE/RSJ International Conference on; 2015. p. 3442-3448.\n",
            "[116] Günther J, Pilarski PM, Helfrich G, et al. Intelligent laser welding through \n",
            "representation, prediction, and control learning: an architecture with deep neural \n",
            "networks and reinforcement learning. Mechatronics. 2015;34:1-11.\n",
            "[117] Günther J, Pilarski PM, Helfrich G, et al. First steps towards an intelligent laser \n",
            "welding architecture using deep neural networks and reinforcement learning. \n",
            "Procedia Technology. 2014;15:474-483.\n",
            "[118] Masci J, Meier U, Cireşan D, Schmidhuber J. (2011). Stacked convolutional auto-\n",
            "encoders for hierarchical feature extraction. Artificial Neural Networks and \n",
            "Machine Learning–ICANN 2011 , 52-59.\n",
            "[119] Ashmore SC, Gashler MS. (2016, December). Practical techniques for using \n",
            "neural networks to estimate state from images. In Machine Learning and \n",
            "Applications (ICMLA), 2016 15th IEEE Int Conf on  (pp. 916-919). IEEE.\n",
            "[120] Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. (2011). Contractive auto-\n",
            "encoders: explicit invariance during feature extraction. In Proc of the 28th Int \n",
            "Conf on Machine Learning (ICML-11)  (pp. 833-840).\n",
            "[121] Jain A, Koppula HS, Soh S, et al. Brain4Cars: car that knows before you do via \n",
            "sensory-fusion deep learning architecture. 2016. Available: \n",
            "http://arxiv.org/abs/1601.00740\n",
            "[122] Lenz I, Knepper R, Saxena A. Deepmpc: learning deep latent features for model \n",
            "predictive control. Robotics: Science and Systems XI; Rome, Italy. 2015.\n",
            "[123] Hwang J, Jung M, Madapana N, et al. Achieving \"synergy\" in cognitive behavior \n",
            "of humanoids via deep learning of dynamic visuo-motor-attentional \n",
            "coordination. Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th \n",
            "International Conference on; Seoul. 2015. p. 817-824.\n",
            "[124] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation , \n",
            "1997;9(8):1735-1780.\n",
            "40 [125] Zhang T, Kahn G, Levine S, et al. Learning deep control policies for autonomous \n",
            "aerial vehicles with MPC-guided policy search. 2015. Available: \n",
            "http://arxiv.org/abs/1509.06791\n",
            "[126] Lillicrap TP, Hunt JJ, Pritzel A, et al. Continuous control with deep reinforcement \n",
            "learning. 2015. Available: http://arxiv.org/abs/1509.02971\n",
            "[127] Finn C, Levine S, Abbeel P. Guided cost learning: deep inverse optimal control \n",
            "via policy optimization. 2016. Available: http://arxiv.org/abs/1603.00448\n",
            "[128] Levine S, Finn C, Darrell T, et al. End-to-end training of deep visuomotor \n",
            "policies. J Mach Learning Research. 2016;17:1-40.\n",
            "[129] Rosenstein M, Barto A.(2004). J. 4 supervised actor-critic reinforcement learning. \n",
            "Handbook of Learning and Approximate dynamic programming , 2, 359.\n",
            " [130] Houthooft R, Chen X, Duan Y, Schulman J, De Turck F, Abbeel P. (2016). VIME:\n",
            "variational information maximizing exploration. In Advances in Neural \n",
            "Information Processing Systems  (pp. 1109-1117).\n",
            "[131] Osband I, Blundell C, Pritzel A, Van Roy B. (2016). Deep exploration via \n",
            "bootstrapped DQN. In Advances in Neural Information Processing Systems  (pp. \n",
            "4026-4034).\n",
            "[132] Pinto L, Gupta A. Supersizing self-supervision: learning to grasp from 50k tries \n",
            "and 700 robot hours. 2015. Available: http://arxiv.org/abs/1509.06825\n",
            "[133] Kappler D, Bohg J, Schaal S. Leveraging big data for grasp planning. 2015 IEEE \n",
            "International Conference on Robotics and Automation (ICRA); Seattle, WA. \n",
            "2015. p. 4304-4311.\n",
            "[134] Pratt GA. Is a cambrian explosion coming for robotics? Journal of Economic \n",
            "Perspectives. 2015;29(3):51-60.\n",
            "[135] Szegedy C, Zaremba W, Sutskever I, et al. Intriguing properties of neural \n",
            "networks. 2013. Available: http://arxiv.org/abs/1312.6199\n",
            "411 \n",
            " Deep Learning in Agriculture: A Survey  \n",
            " \n",
            "Andreas Kamilaris1 and Francesc X. Prenafeta -Boldú  \n",
            "Institute for Food and Agricultural Research and Technology (IRTA)  \n",
            " \n",
            " \n",
            "Abstract: Deep learning constitutes a recent , modern  technique for image processing  and \n",
            "data analysis , with promising results and large potential. As deep learning has been \n",
            "successfully applied in various do mains, it has recently entered also the  domain of \n",
            "agriculture. In this paper, we perform a survey of 40 research efforts that employ  deep \n",
            "learning techniques , applied to  various agricultural  and food production challenges . We \n",
            "examine the particular agricultural problems  under study, the  specific  models  and \n",
            "frameworks  employed,  the sources , nature and pre -processing  of data used , and the \n",
            "overall performance  achieved  according to the metrics used at each work under study . \n",
            "Moreover, we study comparisons of deep learning with other existing popular techniques, \n",
            "in respect to differences in classification or regression performance . Our find ings indicate \n",
            "that deep learning provides high accuracy , outperforming  existing commonly used  image \n",
            "processing techniques.  \n",
            " \n",
            "Keywords: Deep learning, Agriculture, Survey , Convolutional Neural Networks, Recurrent \n",
            "Neural Networks, Smart Farming,  Food Systems . \n",
            " \n",
            " \n",
            " \n",
            "                                                 \n",
            "1 Corresponding Author. Email: andreas.kamilaris@irta.cat   \n",
            "2 \n",
            " 1. Introduction  \n",
            "Smart farming (Tyagi, 2016)  is important for tackling the challenges of agricultural \n",
            "production in terms of productivity, environmental impact, food security and sustainability  \n",
            "(Gebbers & Adamchuk, 2010) . As the global population has been continuously increasing  \n",
            "(Kitzes, et al., 2008) , a large  increase on food production  must be achieved  (FAO, 2009) , \n",
            "maintaining at the same time  availability and high nutritional quality  across the globe , \n",
            "protecting the natural ecosystems by using sustainable farming procedures . \n",
            "To address these challenges, the complex, multivariate and unpredictable agricultural \n",
            "ecosystems need to be b etter understood by monitoring, measuring and analyzing \n",
            "continuously various physical aspects and phenomena . This implies analysis of big \n",
            "agricultural data (Kamilaris, Kartakoullis, & Prenafeta -Boldú, A review on the practice of \n",
            "big data analysis in agriculture, 2017) , and the use of new information and communication \n",
            "technologies (ICT)  (Kamilaris, Gao, Prenafeta -Boldú, & Ali, 2016) , both  for short -scale  \n",
            "crop/farm management  as well as for  larger -scale ecosystems’ observation , enhancing  the \n",
            "existing tasks of management and decision /policy  making by context, situation and \n",
            "location awareness.  Larger -scale observation is facilitated by remote sensing \n",
            "(Bastiaanssen, Molden, & Ma kin, 2000) , performed by  means of satellites, airplanes and \n",
            "unmanned aerial vehicles  (UAV) (i.e. drones) , providing  wide-view snapshots o f the \n",
            "agricultural environments.  It has several advantages when applied to agriculture, being a \n",
            "well-known, non-destructive method to collect information about earth features while data \n",
            "may be obtained systematically over  large geographical areas.  \n",
            "A large subset of the volume of data collected through remote sensing involve images . \n",
            "Images constitute, in many cas es, a complete picture of the agricultural environments and \n",
            "could address a variety of challenges (Liaghat & Balasundram, 2010) , (Ozdogan, Yang, \n",
            "Allez, & Cervantes, 2010) . Hence, imaging analysis is an important research area in the \n",
            "agricultural domain and intelligent data analysis techniques are being  used for image 3 \n",
            " identification/ classification, anomaly detection etc. , in various agricultural applications  \n",
            "(Teke, Deveci, Ha liloğlu, Gürbüz, & Sakarya, 2013) , (Saxena & Armstrong, 2014) , (Singh, \n",
            "Ganapathysubramanian, Singh, & Sarkar, 2016) . The most popular techniques and \n",
            "applications are presented in Appendix I , together with the sensing methods  employed  to \n",
            "acquire the images . From  existing sensing methods , the most common one is satellite -\n",
            "based, using multi -spectral and hyperspectral imaging. Synthetic aperture radar  (SAR) , \n",
            "thermal and near infrared (NIR) camer as are being used  in a lesser but increasing extent \n",
            "(Ishimwe, Abutaleb, & Ahmed, 2014) , while optical and X -ray imaging are being  applied in \n",
            "fruit and packaged food grading. The most popular  techniques used for analyzing images \n",
            "include machine learning  (ML) (K-means, support  vector machines  (SVM) , artificial neural \n",
            "networks  (ANN)  amongst others ), linear polarizations, wavelet -based filtering, vegetation \n",
            "indices (NDVI) and regression analysis  (Saxena & Armstrong, 2014) , (Singh, \n",
            "Ganapathysubramanian, Singh, & Sarkar, 2016) . \n",
            "Besides the aforementioned techniques, a new one which is recently gaining momentum is \n",
            "deep learning  (DL) (LeCun, Bengio, & Hinton, 2015) , (LeCun & Bengio, 1995) . DL belongs \n",
            "to the machine learning computational field and is similar to ANN. However, DL is about \n",
            "“deeper” neural networks that provi de a hierarchical representation of the data  by means \n",
            "of various convolutions. This allows larger learning capabilities and thus higher \n",
            "performance and precision. A brief description of DL is attempted  in Section 3.  \n",
            "The motivation for preparing this surve y stems from the fact that DL in agriculture is a \n",
            "recent, modern and promising technique with growing popularity, while advancements and \n",
            "applications of DL in other domains indicate its large potential.  The fact that today there \n",
            "exists at least 4 0 research efforts employing DL to address various agricultural problems \n",
            "with very good results, encouraged the authors to prepare this survey.  To the authors’ \n",
            "knowledge, this is the first such survey in the agricultural domain, while a small number of \n",
            "more  general surveys do exist (Deng & Yu, 2014) , (Wan, et al., 2014) , (Najafabadi, et al., 4 \n",
            " 2015) , covering related work in DL in other domains . \n",
            "2. Methodology  \n",
            "The bibliographic analysis in the domain under study involved two steps : a) collection of \n",
            "related work  and b) detailed review and analysis of this work. In the first step, a keyword -\n",
            "based search for conference papers or journal articles was performed from the scientific \n",
            "databases IEEE Xplore and ScienceDirect, and from the web scientific in dexing services \n",
            "Web of Science and Google Scholar. As search keywords , we used the following query:  \n",
            "[\"deep learning\" ] AND [\"agriculture\" OR ”farming\"]  \n",
            "In this way, we fi ltered out papers referring to DL but not applied to the agri cultural domain . \n",
            "From this effort, 47 papers had been  initially identified. Restricting the search for papers  \n",
            "with appropriate application of the DL technique  and meaningful findings2, the initial \n",
            "number of papers reduced to 40. \n",
            "In the second  step, the 40 papers selected from the previous step were analyzed one -by-\n",
            "one, considering the following research questions:  \n",
            "1. Which  was the agricultural - or food -related problem they addressed?  \n",
            "2. Which was the general a pproach and type of DL -based models  employed ? \n",
            "3. Which s ources  and type s of data had been used ? \n",
            "4. Which were the c lasses  and labels as modeled by the authors?  Were  there any \n",
            "variations among them,  observed by the authors ? \n",
            "5. Any pre -processing of the data or data augmentation techniques used?  \n",
            "6. Which  has been the overall performance depending on the metric adopted ? \n",
            "7. Did the authors test the performance of their models on different datasets?  \n",
            "8. Did the authors compare  their approach with other techniques  and, if yes, w hich \n",
            "was the  difference in performance?  \n",
            "Our main findings are presented in Section 4  and the detailed information per paper is \n",
            "                                                 \n",
            "2 A small number of papers claimed of using DL in some agricultural -related application , but they did not \n",
            "show  any results nor provided perfor mance metrics that  could  indicate the success of the technique used.  5 \n",
            " listed in Appendix II . \n",
            "3. Deep Learning  \n",
            "DL extends classical ML by adding more \"depth\" (complexity) into the model as well as \n",
            "transforming the data using various functions that allow data representation in a \n",
            "hierarchical way, through  several levels of abstraction (Schmidhuber, 2015) , (LeCun & \n",
            "Bengio, 1995) . A strong advantage of DL is feature learning , i.e. the automatic feature \n",
            "extraction from raw data , with features from higher levels of the hierarchy being formed by \n",
            "the composition of lower level features  (LeCun, Bengio, & Hinton, 2015) . DL can solve \n",
            "more complex problems  particularly well and fast,  because of more complex models used,  \n",
            "which allow massive paralleliz ation  (Pan & Yang, 2010) . These complex models employed \n",
            "in DL can increase classification  accuracy or reduce error in regression problems , \n",
            "provided  there are adequately large datasets available describing the problem . DL \n",
            "consist s of various  different components (e.g. convolutions , pooling layers,  fully connected \n",
            "layers , gates, memory cells, activation function s, encode/decode scheme s etc.), \n",
            "depending on the network architecture used (i.e. Unsupervised Pre -trained Networks , \n",
            "Convolutional Neural Networks, Recurrent Neural Networks , Recursive Neural Networks ). \n",
            "The highly hierarchical structure and large learn ing capacity of DL models allow  them to \n",
            "perform classification and predictions particularly well, being flexible and adaptable for a \n",
            "wide variety of highly complex (from a data an alysis perspective) challenges  (Pan & Yang, \n",
            "2010) . Although DL has met popularity in numerous applications dealing with raster -based \n",
            "data (e.g. video, image s), it can be applied to any form of data, such as  audio , speech, \n",
            "and n atural language , or more generally to  continuou s or point data such as weather data \n",
            "(Sehgal, et al., 2017) , soil chemistry  (Song, et al., 2016)  and population data (Demmers T. \n",
            "G., Cao, Parsons, Gauss, & Wathes, 2012) . An example DL architecture is displayed in \n",
            "Figure 1, illustrating CaffeNet  (Jia, et al., 2014) , an example of  a convolutional neural \n",
            "network,  combining  convolutional and fully connected (dense) layers . \n",
            "6 \n",
            "  \n",
            "Figure 1: CaffeNet, a n example CNN architecture . Source:  (Sladojevic, Arsenovic, Anderla, \n",
            "Culibrk, & Stefanovic, 2016)  \n",
            " \n",
            "Convolutional Neural Networks ( CNN ) constitu te a class of deep, feed -forward ANN , and \n",
            "they appear in numerous of the surveyed papers as the technique used (17 papers, 42%) . \n",
            "As the figure shows, various convolutions are performed  at some  layer s of the network, \n",
            "creating different representations of the learning dataset, starting from more general  ones  \n",
            "at the first larger layers , becoming more specific at the deeper layers. The convolution al \n",
            "layers act as feature extractors from the input images whose dimensionality is then \n",
            "reduced by the pooling layer s. The convolutional layers encode multi ple lower -level \n",
            "features into more discriminative feature s, in a way that is spatially c ontext -aware . They \n",
            "may be understood as banks of filters that transform an input image into another , \n",
            "highlighting specific patterns. The fully connected layer s, placed in many cases near the \n",
            "output  of the model , act as classifier s exploiting the  high-level features  learned  to classify \n",
            "input images in  predefined classes  or to make numerical predictions. They take a vector \n",
            "as in put and produce another vector as output.  An example visualization of leaf images \n",
            "after each  processing step of the CaffeNet CNN, at a problem of identifying plant disease s, \n",
            "is depicted in  Figure 2. We can observe that after each processing step, the particular \n",
            "elements of the ima ge that reveal the indication of a disease become more evident, \n",
            "especially at the final step (Pool5).  \n",
            "7 \n",
            "  \n",
            "Figure 2: Visualization of the output layers  images  after each processing step of the CaffeNet CNN \n",
            "(i.e. convolution, pooling, normalization) at a plant disease identification  problem  based on  leaf \n",
            "images . Source:  (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016)  \n",
            " \n",
            "One of the most important advantages of using DL  in image processing is the reduced \n",
            "need of f eature engineering (FE). Previously, traditional approach es for image \n",
            "classification tasks had  been ba sed on hand -engineered features, whose  performance \n",
            "affected  heavily the overall results. FE is a complex , time-consuming  process which needs \n",
            "to be altered whenever the problem or the dataset changes. Thus, FE constitutes an \n",
            "expensive effort that  depend s on expert s’ knowledge and  does not generalize well  (Amara, \n",
            "Bouaziz, & Algergawy, 2017) . On the other hand, DL does not require FE, locating the \n",
            "important features itself through training.  \n",
            "A disadvantage of DL is the generally  longer train ing time. However, testing time is \n",
            "generally faster than other methods ML-based methods (Chen, Lin, Zhao, Wang, & Gu, \n",
            "2014) . Other disadvantages include problems that might occur when using pre -trained \n",
            "models on datasets  that are small or significantly different , optimization issues because of \n",
            "the models’ complexity, as well as hardware restrictions.  \n",
            "In Section 5, we discuss over advantages and disadvantages of DL as they reveal through \n",
            "the surveyed papers.  \n",
            " \n",
            "8 \n",
            " 3.1 Available Architectures, Datasets and Tools  \n",
            "There exist various successful  and popular architect ures, which  researchers may use to \n",
            "start building their models instead of starting from scratch. These include AlexNet \n",
            "(Krizhevsky, Sutskever, & Hinton, 2012) , CaffeNet (Jia, et al., 2014)  (displayed in Figure \n",
            "1), VGG (Simonyan & Zisserman, 2014) , GoogleNet (Szegedy, et al., 2015)  and Inception -\n",
            "ResNet  (Szegedy, Ioffe, Vanhoucke, & Alemi, 2017) , among others . Each architecture has \n",
            "different advantages and scenarios where it is more appropriate to be used  (Canziani, \n",
            "Paszke, & Culurciello, 2016) . It is also worth noting that almost all of the aforementioned \n",
            "models  come  along  with their weights  pre-trained, wh ich means that their network had  \n",
            "been already trained by some dataset and has thus learned to provide accurate \n",
            "classi fication  for some  particular problem domain  (Pan & Yang, 2010) . Common datasets \n",
            "used for pre -training DL architectures include  ImageNet (Deng, et al., 2009)  and PASCAL \n",
            "VOC  (PASCAL VOC Project, 2012)  (see also Appendix III) . \n",
            "Moreover, there exist various tools and platforms allow ing researchers to experiment with  \n",
            "DL (Bahrampour, Ramakrishnan, Schott, & Shah, 2015) . The most popular ones are \n",
            "Theano , TensorFlow , Keras  (which is an application programmer's i nterface on top of  \n",
            "Theano and TensorFlow), Caffe, PyTorch , TFLearn, Pylearn2  and the Deep Learning \n",
            "Matlab Toolbox . Some of these tools (i.e. Theano, Caffe) incorporate popular architectures \n",
            "such as the ones mentioned above  (i.e. AlexNet, VGG, GoogleNet) , either as libraries or \n",
            "classes.  For a more elaborate description of the DL concept and its applications , the \n",
            "reader could refer to existing bibliography  (Schmidhuber, 2015) , (Deng & Yu, 2014) , (Wan, \n",
            "et al., 2014) , (Najafabadi, et al., 2015) , (Canziani, Paszke, & Culurciello, 2016) , \n",
            "(Bahrampour, Ramakrishnan, Schott, & Shah, 2015) . \n",
            " \n",
            "4. Deep Learning Applications in Agriculture  \n",
            "In Appendix II, we list the 40 identified relevant works, indicating the agricultural -related \n",
            "9 \n",
            " research area, the particular problem  they address, DL models and architectures \n",
            "implemented, sources of data used , classes and labels of the data , data pre -processing \n",
            "and/or augmentation employed, overall performance  achieved  according to the metrics \n",
            "adopted , as well as com parisons with other  techniques, wherever available . \n",
            "4.1 Areas of Use  \n",
            "Sixteen  areas have been identified in total, with the popular ones being identification of \n",
            "weeds  (5 paper s), land cover classification (4  papers), plant recognition (4  papers) , fruits \n",
            "counting (4 papers) and crop type classification  (4 papers).  \n",
            "It is remarkable that all papers , except from (Demmers T. G., et al., 2010) , (Demmers T. \n",
            "G., Cao, Parsons, Gauss, & Wathes, 2012)  and (Chen, Lin, Zhao, Wang, & Gu, 2014) , \n",
            "were  published during or after 2015 , indicating how recent and modern this technique is , in \n",
            "the domain of  agriculture. More precisely, from the remaining 37 papers, 15  papers have \n",
            "been publi shed in 2017, 15 in 2016 and  7 in 2015.  \n",
            "The large majority of the papers deal with image classification and identification of areas of \n",
            "interest , including detection of obstacles  (e.g. (Steen, Christiansen, Karstoft, & Jørgensen, \n",
            "2016), (Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016) ) and fruit counting (e.g. \n",
            "(Rahnemoonfar & Sheppard, 2017) , (Sa, et al., 2016) ). Some papers focus on  predicting \n",
            "future parameters, such as  corn yield (Kuwata & Shibasaki, 2015)  soil moisture content at \n",
            "the field (Song, et al., 2016)  and weather conditions (Sehgal, et al., 2017) .  \n",
            "From another perspective, most papers ( 20) target crops, while few works consider issue s \n",
            "such as  weed detection (8 papers),  land cover (4 papers),  research on soil (2 papers),  \n",
            "livestock agriculture ( 3 paper s), obstacle detection (3 papers) and  weather prediction (1 \n",
            "paper) . \n",
            "4.2 Data Sources  \n",
            "Observing the so urces of data used to train the DL model  at every paper, large datasets of \n",
            "images are mainly used, containing thousands of images in some cases , either real ones  10 \n",
            " (e.g. (Mohanty, Hughes, & Salathé, 2016) , (Reyes, Caicedo, & Camargo, 2015) , \n",
            "(Dyrmann, Karstoft, & Midtiby, 2016 ) ), or synthetic produced by the authors \n",
            "(Rahnemoonfar & Sh eppard, 2017) , (Dyrmann, Mortensen, Midtiby, & Jørgensen, 2016) . \n",
            "Some datasets originate from well -known and publicly -available datasets such as \n",
            "PlantVillage , LifeCLEF , MalayaKew , UC Merced  and Flavia (see Appendix III) , while \n",
            "others constitute sets of real images collected  by the authors for their research needs  (e.g. \n",
            "(Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016) , (Bargoti & Underwood, \n",
            "2016) , (Xinshao & Cheng, 2015) , (Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017) ). \n",
            "Papers dealing with land cover , crop type classification and yield estimation , as well as \n",
            "some papers related to weed detect ion employ a smaller number of images (e.g. tens of \n",
            "images), produced by UAV (Lu, et al., 2017) , (Rebetez, J., et al., 2016) , (Milioto, Lottes, & \n",
            "Stachniss, 2017) , airborne  (Chen, Lin, Zhao, Wang, & Gu, 2014) , (Luus, Salmon, van den \n",
            "Bergh, & Maharaj, 2015)  or satellite -based remote sensing (Kussul, Lavreniuk, Skakun, & \n",
            "Shelestov, 2017) , (Minh, et al., 2017) , (Ienco, Gaetano, Dupaquier, & Maurel, 2017) , \n",
            "(Rußwurm & Körner, 2017) . A particular paper investigating segmentation of root and soil \n",
            "uses ima ges from X-ray tomography  (Douarre, Schielein, Frindel, Gerth, & Rousseau, \n",
            "2016) . Moreover, some papers  use text data, collected either from repositories (Kuwata & \n",
            "Shibasaki, 2015) , (Sehgal, et al., 2017)  or field sensors (Song, et al., 2016) , (Demmers T. \n",
            "G., et al., 2010) , (Demmers T. G., Cao, Parsons, Gauss, & Wathes, 2012 ). In general, the \n",
            "more complicated the problem to be solved , the more data is required.  For example, \n",
            "problems involving large number of classes to identify (Mohanty, Hughes, & Salathé, \n",
            "2016) , (Reye s, Caicedo, & Camargo, 2015) , (Xinshao & Cheng, 2015)  and/or small \n",
            "Variation among the classes  (Luus, Salmon, van den Bergh, & Maharaj, 2015) , (Rußwurm \n",
            "& Körner, 2017) , (Yalcin, 2017 ) , (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, 2017) , \n",
            "(Xinshao & Cheng, 2015) , require large number of input images  to train their models.  \n",
            " 11 \n",
            " 4.3 D ata Variation  \n",
            "Variation between classes is necessary for the DL models to be able to differentiate \n",
            "features and characteristics, and perform accurate classifications3. Hence, accuracy is \n",
            "positively correlated with variation among classes. Nineteen papers (47%) revealed some \n",
            "aspects of poor data variation . Luus et al. (Luus, Salmon, van den Bergh, & Maharaj, \n",
            "2015)  observed h igh relevance between some land cover classes (i.e. medium density \n",
            "and dens e residential, buildings and storage tanks ) while Ienko et al. (Ienco, Gaetano, \n",
            "Dupaquier, & Maurel, 2017)  found that tree crops, summer crops and truck f arming were \n",
            "classes highly mixed . A confusion between maize and soybeans  was evident in (Kussul, \n",
            "Lavreniuk, Skakun, & Shelestov, 2017)  and variation was low in botanically related crops, \n",
            "such as meadow, fa llow, triticale, wheat, and rye  (Rußwurm & Körner, 2017) . Moreover , \n",
            "some particular views of the plant s (i.e. flowers and leaf scans) offer different classification \n",
            "accuracy  than branches, stems and photos of the entire plant. A serious issue in plant \n",
            "phenology recognition is the fact that a ppearances change very gradual ly and it is \n",
            "challenging to distinguish images falling into the growing durations that are in the middle of \n",
            "two successive stages  (Yalcin, 2017 ) , (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, \n",
            "2017) . A similar issue appears when assessing the quality of vegetative development  \n",
            "(Minh, et al., 2017) . Furthermore, in the challenging problem of fruit counting, the models \n",
            "suffer from high occlusion, depth variation, and unco ntrolled illuminatio n, including high \n",
            "color similarity between fruit/foliage  (Chen, et al., 2017) , (Bargoti & Underwood, 2016) . \n",
            "Finally, identification of weeds faces issues  with respect to lighting , resolution, and soil \n",
            "type, and small variation between weeds and crops in shape, texture, color  and position \n",
            "(i.e. overlapping) (Dyrmann, Karstoft, & Midtiby, 2016 ) , (Xinshao & Cheng, 2015) , \n",
            "(Dyrmann, Jørgensen, & Midtiby, 2017) . In the large majority of the papers mentioned \n",
            "above (except from (Minh, et al., 2017) ), this low variation has affected classification \n",
            "                                                 \n",
            "3 Classification accuracy is defined in Section 4.7 and Table 1. \n",
            "12 \n",
            " accuracy significantl y, i.e. more than 5%.  \n",
            "4.4 Data Pre -Processing  \n",
            "The large majority of related work (36 papers, 90%) involved  some image pre-processing \n",
            "steps, before the image or particular characteristics/features/statistics of the image were \n",
            "fed as an input to the DL model. The most common pre -processing procedure was image \n",
            "resize (16 papers), in most cases to a smaller size, in order to adapt to the requirements of \n",
            "the DL model. Sizes of 256x256, 128x128, 96x96 and  60x60 pixels  were common.  Image \n",
            "segmentation was also  a popular practice (12 papers), either to increase the size of the \n",
            "dataset (Ienco, Gaetano, Dupaquier, & Maurel, 2017) , (Rebetez, J., et al., 2016) , (Yalcin, \n",
            "2017 )  or to facilitate the learning process by highlight ing region s of interest  (Sladojevic, \n",
            "Arsenovic, Anderla, Culibrk, & Stefanovic, 2016) , (Mohanty, Hughes, & Salathé, 2016) , \n",
            "(Grinblat, Uzal, Larese, & Granitto, 2016) , (Sa, et al., 2016) , (Dyrmann, Karstoft, & Midtiby, \n",
            "2016 ) , (Potena, Nardi, & Pretto, 2016)  or to enable easier data annotation by experts and \n",
            "volunteers (Chen, et al., 2017) , (Bargoti & Underwood, 2016) . Background removal \n",
            "(Mohanty, Hughes, & Salathé, 2016) , (McCool, Perez, & Upcroft, 2017) , (Milioto, Lottes, & \n",
            "Stachniss, 2017) , foreground pixel extraction (Lee, Chan, Wilkin, & Remagnino, 2015)  or \n",
            "non-green pixels remova l based on NDVI  masks  (Dyrmann, Karstoft, & Midtiby, 2016 ) , \n",
            "(Potena, Nardi, & Pretto, 2016)  were also  performed to reduce the dataset s’ overall noise.  \n",
            "Other operations involved the c reation of bound ing boxes  (Chen, et al., 2017) , (Sa, et al., \n",
            "2016) , (McCool, Perez, & Upcroft, 2017) , (Milioto, Lottes, & Stachniss, 2017)  to facilitate \n",
            "detection of  weeds or counting of fruits.  Some datasets were converted to grayscale  \n",
            "(Santoni, Sensuse, Arymurthy, & Fanany, 2015) , (Amara, Bouaziz, & Algergawy, 2017)  or \n",
            "to the HSV color model (Luus, Salmon, van den Bergh, & Maharaj, 2015) , (Lee, Chan, \n",
            "Wilkin, & Remagnino, 2015) . \n",
            "Furthermore, some papers used features extracted from the images as input to their \n",
            "models, such as shape and statist ical feature s (Hall, McCool, Dayoub, Sunderhauf, & 13 \n",
            " Upcroft, 2015) , histograms (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015) , (Xinshao \n",
            "& Cheng, 2015) , (Rebetez, J., et al., 2016) , Principal Component Analysis ( PCA) filters \n",
            "(Xinshao & Cheng, 2015) , Wavelet transformation s (Kuwata & Shibasaki, 2015)  and Gray \n",
            "Level Co -occurrenc e Matrix  (GLCM) features (Santoni, Sensuse, Arymurthy, & Fanany, \n",
            "2015) . Satellite or aerial images  involved a combination of pre -processing steps such as  \n",
            "orthorectification (Lu, et al., 2017) , (Minh, et al., 2017)  calibration and terrain correction  \n",
            "(Kussul, Lavreniuk, Skakun, & Shelestov, 2017) , (Minh, et al., 2017)  and atmospheric \n",
            "correction (Rußwurm & Körner, 2017) .  \n",
            "4.5 Data Augmentation  \n",
            "It is worth -mentioning that some of the related work under study (15 papers, 37%)  \n",
            "employed data augmentation techniques  (Krizhevsky, Sutskever, & Hinton, 2012) , to \n",
            "enlarge artificially their number of training images . This helps to improve the overall \n",
            "learning procedure and performance, and for generalization purposes, by means of \n",
            "feeding the model with varied data.  This augmentation process is important for papers that \n",
            "poss ess only small datasets to train their DL models , such as  (Bargoti & Underwood, \n",
            "2016) , (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016) , (Sørensen, \n",
            "Rasmussen, N ielsen, & Jørgensen, 2017) , (Mortensen, Dyrmann, Karstoft, Jørgensen, & \n",
            "Gislum, 2016) , (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, 2017)  and (Chen, et al., \n",
            "2017) . This process was especially important in papers where the authors trained their \n",
            "models using synthetic images  and tested them on real ones (Rahnemoonfar & Sheppard, \n",
            "2017)  and (Dyrmann, Mortensen, Mi dtiby, & Jørgensen, 2016) . In this case, data \n",
            "augmentation allowed their models to generalize and be able to adapt to the real -world \n",
            "problems more easily.  \n",
            "Transformations are  label -preserving , and included  rotations (12 papers) , dataset \n",
            "partitioning/cropping (3 papers), scaling (3 papers), transposing  (Sørensen, Rasmussen, \n",
            "Nielsen, & Jørgensen, 2017) , mirroring  (Dyrmann, Karstoft, & Midtiby, 2016 ) , translations  14 \n",
            " and perspective transform  (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016) , \n",
            "adaptations of objects’ intensity  in an object detection problem (Steen, Christiansen, \n",
            "Karstoft, & Jørgensen, 2016)  and a PCA augmentati on technique  (Bargoti & Underwood, \n",
            "2016) . \n",
            "Papers involving simulated data performed additional augmentation techniques such as \n",
            "varying the  HSV channels and adding random shadows  (Dyrmann, Mortensen, Midtiby, & \n",
            "Jørgensen, 2016)  or adding s imulated roots to soil images  (Douarre, Schielein, Frindel, \n",
            "Gerth, & Rousseau, 2016) . \n",
            "4.6 Technical Details  \n",
            "From a technical side, almos t half of the research works (17 papers , 42% ) employed \n",
            "popular CNN  architectures such as AlexNet , VGG 16 and Inception -ResNet . From the rest, \n",
            "14 papers developed their own CNN model s, 2 papers adopted a first -order D ifferential  \n",
            "Recurrent Neural N etwork s (DRNN ) model,  5 papers preferred to use a Long Short -Term \n",
            "Memory  (LSTM ) model  (Gers, Schmidhuber, & Cummins, 2000) , one paper used deep \n",
            "belief networks (DBN)  and one paper employed a hybrid of PCA with auto -encoders (AE). \n",
            "Some of the CNN approaches  combined their model  with a classifier at the output layer , \n",
            "such as logistic regression (Chen, Lin, Zhao, Wang, & Gu, 2014) , Scalable Vector \n",
            "Machines (SVM)  (Douarre, Schielein, Frindel, Gerth,  & Rousseau, 2016) , linear regression \n",
            "(Chen, et al., 2017) , Large Margin Classifiers (LCM)  (Xinshao & Cheng, 2015)  and \n",
            "macroscopic cellular automata  (Song, et al., 2016 ). \n",
            "Regarding the frameworks used, all the works that employed some well -known CNN \n",
            "architecture had also used a DL framework, with Caffe being the most popular (13  papers, \n",
            "32%), followed by Tensor Flow  (2 paper s) and deeplea rning4j  (1 paper) . Ten research \n",
            "works developed their own software, while some authors decided to build their own \n",
            "models on top of Caffe (5 papers), Keras/Theano (5 papers), Keras/TensorFlow (4 \n",
            "papers), Pylearn 2 (1 paper), MatCon vNet (1 paper) and Deep Learning Matlab Toolbox  (1 15 \n",
            " paper). A possible reason for the wide use of Caffe is that it incorporates various CNN  \n",
            "frameworks and datasets, which  can be used then easily and automatically by its users.  \n",
            "Most of the studies divided their dataset between training and testing/verifica tion data \n",
            "using a ratio of 80 -20 or 90 -10 respectively.  In addition , various learning rates have been \n",
            "recorded, from 0.001 (Amara, Bouaziz, & Algergawy, 2017)  and 0.005 (Mohanty, Hughes, \n",
            "& Salathé, 2016)  up to 0.01 (Grinblat, Uzal, Larese, & Granitto, 2016) . Learning rate is \n",
            "about how quickly a network  learns. Higher values help avoid the solver being stuck in \n",
            "local minima, which can reduce performance signific antly . A general  approach used by \n",
            "many of the evaluated papers is to  start out with a high learning rate and lower it as the \n",
            "training goes on.  We note that learning rate is very dependent on the network architecture . \n",
            "Moreover , most of the research works that incorp orated popular DL architectures took \n",
            "advantage of transfer learning  (Pan & Yang, 2010) , which concerns l everaging the already \n",
            "existing knowledge  of some related task or domain  in order to increase the learning \n",
            "efficiency of the  problem under study by fine-tuning pre -trained  models.  As sometimes it is \n",
            "not possible to train a network from scratch due to h aving a small training data set  or \n",
            "having a complex multi -task network,  it is required that the network be at least partially \n",
            "initialized with weights fr om another pre -trained model. A common transfer learning \n",
            "technique is the use of pre -trained CNN , which are  CNN model s that have been already \n",
            "trained on some relevant data set with possibly different number of classes. These models \n",
            "are then adapted to the particular challenge and dataset. This method was followed  \n",
            "(among others)  in (Lu, et al., 2017) , (Douarre, Schielein, Frindel, Gerth, & Rousseau, \n",
            "2016) , (Reyes, Caicedo, & Camargo, 2015) , (Bargoti & Underwood, 2016) , (Steen, \n",
            "Christiansen, Karstoft, & Jørgensen, 2016) , (Lee, Chan, Wilkin, & Remagni no, 2015) , (Sa, \n",
            "et al., 2016) , (Mohanty, Hughes, & Salathé, 2016) , (Christiansen, Nielsen, Steen, \n",
            "Jørgensen, & Karstoft, 2016) , (Sørensen, R asmussen, Nielsen, & Jørgensen, 2017) , for \n",
            "the VGG16 , DenseNet , AlexNet and GoogleNet architectures.  16 \n",
            " 4.7 Outputs  \n",
            "Finally, concerning the 3 1 papers that involved classification, the classes as used by the \n",
            "authors ranged from 2 (Lu, et al., 2017) , (Pound, M. P., et al., 2016) , (Douarre, Schielein, \n",
            "Frindel, Gerth, & Rousseau, 2016) , (Milioto, Lottes, & Stachniss, 2017)  up to 1,000 \n",
            "(Reyes, Caicedo, & Camargo, 2015) . A large number of classes was observed in (Luus, \n",
            "Salmon, van den Bergh, & Maharaj, 2015)  (21 land -use classe s), (Rebetez, J., et al., \n",
            "2016)  (22 different crops plus soil ), (Lee, Chan, Wilkin, & Remagnino, 2015)  (44 plant \n",
            "species ) and (Xinshao & Cheng, 2015)  (91 classes of  common weeds found in agricultural \n",
            "fields ). In these papers, the  number of outputs of the model was equal to the number of \n",
            "classes  respectively . Each output was a  different probability  for the input image, segment, \n",
            "blob or pixel to belong  to each class , and then the model picked  the highest probability as \n",
            "its predicted class.  \n",
            "From the rest 9 papers, 2 performed predictions of fruits counted (scalar value  as output ) \n",
            "(Rahnemoonfar & Sheppard, 2017) , (Chen, et al., 2017) , 2 identified regions of fruits in the \n",
            "image ( multiple bounding box es) (Bargoti & Underwood, 2016) , (Sa, et al., 2016) , 2 \n",
            "predicted animal growth  (scalar value)  (Demmers T. G., et al., 2010) , (Demmers T. G., \n",
            "Cao, Parsons, Gauss, & Wathes, 2012) , one  predicted weather conditions (scalar value)  \n",
            "(Sehgal, et al., 2017) , one  crop yield index  (scalar value) (Kuwata & Sh ibasaki, 2015)  and \n",
            "one paper predicted  percentage of soil moisture conten t (scalar value) (Song, et al., 2016) . \n",
            "4.8 Performance Metrics  \n",
            "Regarding methods  used to evaluate performance , various metrics  have been employed \n",
            "by the authors , each being specific  to the model used at each study.  Table 1 lists t hese \n",
            "metrics , together with their definition/ description , and the symbol we use to refer to them  in \n",
            "this survey . In some papers where the authors referred to  accuracy without specifying its \n",
            "definition, we assumed they referred to classification accuracy (CA, first metric listed in \n",
            "Table 1). From this point onwards, we refer to “DL performance” as its score in some \n",
            "17 \n",
            " performance metric from the ones listed in Table 1. \n",
            "Table 1: Performance metrics used in related work  under study.  \n",
            "No. Performance \n",
            "Metric  Symbol \n",
            "Used  Description  \n",
            "1. Classification \n",
            "Accuracy  CA The percentage of correct predictions where the top class (the one \n",
            "having the highest probability), as indicated by the DL model, is the \n",
            "same as the target label as annotated beforehand by the authors. For \n",
            "multi -class classification problems, CA is average d among all the \n",
            "classes. CA is mentioned as Rank -1 identification rate in (Hall, \n",
            "McCool, Dayoub, Sunderhauf, & Upcroft, 2015).  \n",
            "2. Precision  P The fraction of true positives (TP, correct predictions) from the total \n",
            "amount of relevant results, i.e. the sum of TP and false positives (FP). \n",
            "For multi -class classification problems, P is averaged among the \n",
            "classes. P=TP/(TP+FP)  \n",
            "3. Recall  R The fraction of TP from the total amount of TP and false negatives \n",
            "(FN). For multi -class classification problems, R gets averaged among \n",
            "all the classes. R=TP/(TP+FN)  \n",
            "4. F1 score  F1 The harmonic mean of precision and recall. For multi -class \n",
            "classification problems, F1 gets averaged among all the classes. It is \n",
            "mentioned as F -measure in (Minh, et al., 2017).  \n",
            "F1=2 * (TP*FP) / (TP+FP)  \n",
            "5. LifeCLEF metric  LC A score4 related to the rank of the correct species in the list of retrieved \n",
            "species  \n",
            "6. Quality Measure  QM Obtained by multiplying sensitivity (proportion of pixels that were \n",
            "detected correctly) and specificity (which proportion of detected pi xels \n",
            "are truly correct). QM=TP2 / ((TP+FP)(TP+FN))  \n",
            "7. Mean Square Error  MSE  Mean of the square of the errors between predicted and observed \n",
            "values.  \n",
            "8. Root Mean Square \n",
            "Error  RMSE  Standard deviation of the differences between predicted values and \n",
            "observed  value s. A normalized RMSE (N -RMSE) has been  used in \n",
            "(Sehgal, et al., 2017).  \n",
            "9. Mean Relative \n",
            "Error  MRE  The mean error between predicted and observed values, in \n",
            "percentage.  \n",
            "10. Ratio of total fruits \n",
            "counted  RFC Ratio of the predicted count of fruits by the model, with the actual \n",
            "count. The actual count was attained by taking the average count of \n",
            "individuals (i.e. experts or volunteers) observing the images \n",
            "independently.  \n",
            "11. L2 error  L2 Root of the squares of the sums of the differences between predicted \n",
            "counts of fruits by t he model and the actual counts.  \n",
            "12. Intersection over \n",
            "Union  IoU A metric that evaluates predicted bounding boxes, by dividing the area \n",
            "of overlap between the predicted and the ground truth boxes, by the \n",
            "area of their union. An average (Dyrmann, Mortensen, Midtiby, & \n",
            "Jørgensen, 2016) or frequency weighted (Mortensen, Dyrmann, \n",
            "Karstoft, Jørgensen, & Gislum, 2016) IoU can be calculated.  \n",
            "13. CA-IoU, F1-IoU,  CA-IoU These  are the same CA,  F1, P  and R metrics  as defined above, \n",
            "                                                 \n",
            "4 LifeCLEF 2015 Challenge. http://www.imageclef.org/lifeclef/2015/plant   \n",
            "18 \n",
            " P-IoU or R-IoU F1-IoU  \n",
            "P-IoU  \n",
            "R-IoU combined with IoU in order to consider true /false  positives /negatives . \n",
            "Used in problems involving bounding boxes. This is done by putting a \n",
            "minimum threshold on IoU, i.e. any value above this thr eshold would \n",
            "be considered a s positive by the metric (and the model involved) . \n",
            "Thresholds of 20% (Bargoti & Underwood, 2016) , 40% (Sa, et al., \n",
            "2016)  and 50% (Steen, Christiansen, Karstoft, & Jørgensen, 2016) , \n",
            "(Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016) , \n",
            "(Dyrmann, Jørgensen, & Midtiby, 2017)  have  been observed5. \n",
            " \n",
            "CA was  the most popular metric used  (24 papers , 60%), followed by F1 (10  papers, 25%). \n",
            "Some papers  include d RMSE  (4 papers), IoU (3 papers), RFC (Chen, et al., 2017) , \n",
            "(Rahnemoonfar & Sheppard, 2017)  or others . Some works used a combination of metrics \n",
            "to eval uate their efforts.  We note that some papers employing CA, F1, P or R, used IoU in \n",
            "order to consider a model’s prediction (Bargoti & Underwood, 2016) , (Sa, et al., 2016) , \n",
            "(Steen, Christiansen, Karstoft, & Jørgensen, 2016) , (Christiansen, Nielsen, Steen, \n",
            "Jørgensen, & Karstoft, 2016) , (Dyrmann, Jørgensen, & Midtiby, 2017) . In these cases, a  \n",
            "minimum threshold was put on IoU, and  any value above this threshold would be \n",
            "considered as positive  by the model.  \n",
            "We note that in some cases, a trade -off can exist between metrics. For example, in a \n",
            "weed detection problem (Miliot o, Lottes, & Stachniss, 2017) , it might be desirable to have \n",
            "a high R to eliminate most weeds, but not eliminating crops is of a critical importance , \n",
            "hence a lower P might be acceptable.  \n",
            "4.9 Overall Performance  \n",
            "We note that it is difficult if not impo ssible to compare between papers, as different metrics  \n",
            "are employed  for different tasks , considering different models, datasets and parameters . \n",
            "Hence, t he reader should consider our comments in this section with some caution.  \n",
            "In 19 out of the 24 papers that involved CA as a metric , accuracy was  high (i.e. above \n",
            "90%), indicating good performance . The highest CA has been observed in the works of  \n",
            "(Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015) , (Pound , M. P., et al., 2016) , (Chen, \n",
            "                                                 \n",
            "5 In Appendix II, where we list the values of the metric s used at each paper, we denote CA -IoU(x), F1 -IoU(x), \n",
            "P-IoU(x) or R -IoU(x), where x is the threshold (in percentage), over which results are considered as positive \n",
            "by the DL model employed.  19 \n",
            " Lin, Zhao, Wang, & Gu, 2014) , (Lee, Chan, Wilkin, & Remagnino, 2015) , (Minh, et al., \n",
            "2017) , (Potena, Nardi, &  Pretto, 2016)  and (Steen, Christiansen, Karstoft, & Jørgensen, \n",
            "2016) , with values of 98% or more, constituting remarkable  results . From the 10 papers \n",
            "using F1 as metric, 5 had values higher than 0.90 with the highest F1 observed in \n",
            "(Mohanty, Hughes, & Salathé, 2016)  and (Minh, et al., 2017)  with values higher than 0.99.  \n",
            "The wor ks of (Dyrmann, Karstoft, & Midtiby, 2016 ) , (Rußwurm & Körner, 2017) , (Ienco, \n",
            "Gaetano, Dupaquier, & Maurel, 2017) , (Mortensen, Dyrmann, Karst oft, Jørgensen, & \n",
            "Gislum, 2016) , (Rebetez, J., et al., 2016) , (Christiansen, Nielsen, Steen, Jørgensen, & \n",
            "Karstoft, 2016)  and (Yalcin, 2017 )  were among the ones with the lowest  CA (i.e. 7 3-79%) \n",
            "and/or F1 scores (i.e. 0.558  - 0.746) , however state of the art work in these particular \n",
            "problems has shown low er CA (i.e. SVM, RF, Naïve - Bayes classifie r). Particularly in \n",
            "(Rußwurm & Körner, 2017) , the three -unit LSTM model employed provided 16.3% better \n",
            "CA than a CNN, which belongs to the family of DL. Besides, the above can be considered \n",
            "as “hard er” problems, because of  the use of satellite data (Ienco, Gaetano, Dupaqu ier, & \n",
            "Maurel, 2017) , (Rußwurm & Körner, 2017)  large number of classes  (Dyrmann, Karstoft, & \n",
            "Midtiby, 2016 ) , (Rußwurm & Körner, 2017) , (Rebetez, J., et al., 2016) , small training \n",
            "datasets (Mortensen, Dyrmann, Karstoft, Jørgensen, & Gislum, 2016) , (Christiansen, \n",
            "Nielsen, Steen, Jørgensen, & Karstoft, 2016)  or very low variation  among the classes \n",
            "(Yalcin, 2017 ) , (Dyrmann, Karstoft, & Midtiby, 2016 ) , (Rebetez, J., et al., 2016) .  \n",
            "4.10 Generalizations on Different Datasets  \n",
            "It is important to  examine whether the authors had tested their implementations on the \n",
            "same dataset (e.g. by dividing the dataset into training and testing/validation sets) or used \n",
            "different datasets to test their solution. Fro m the 40 papers, only 8 (20 %) used different \n",
            "datasets for testing than th e one for training. From these , 2 approaches trained their \n",
            "models by using simulated data and tested on real data (Dyrmann, Mortensen, Midtiby, & \n",
            "Jørgensen, 2016) , (Rahnemoonfar & Sheppard, 2017)  and 2 papers tested their models 20 \n",
            " on a dataset produced 2 -4 weeks after, with a more advanced growth stage of plants and \n",
            "weeds (Milioto, Lottes, & Stachniss, 2017) , (Potena, Nardi, & Pretto, 2016) . Moreover, 3  \n",
            "paper s used different fields for testing than the ones used for training (McCool, Perez, & \n",
            "Upcroft, 2017) , with a severe degree of occlusion compared to the other training field \n",
            "(Dyrmann, Jørgensen, & Midtiby, 2017) , or containing other obstacles such as people and \n",
            "animals (Steen, Christiansen, Karstoft, & Jørgensen, 2016) . Sa et al. (Sa, et al., 2016)  \n",
            "used a different dataset to evaluate w hether the model can generalize  on different fruits . \n",
            "From the other 32 papers, different tre es were used in training and testing  in (Chen, et al., \n",
            "2017) , while different rooms for pigs (Demmers T. G., Cao, Parsons, Gauss, & Wathes, \n",
            "2012)  and chicken  (Demmers T. G., et al., 2010)  were considered. Moreover, Hall et al. \n",
            "applied condition variations in testing (i.e. translation s, scaling, ro tations, shading and \n",
            "occlusions ) (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015)  while scaling for a \n",
            "certain range translation distance and rotation angle  was performed on the testing dataset \n",
            "in (Xinshao & Cheng, 2015) . The rest 27 papers did not perform any changes between the \n",
            "training/testing dataset s, a fact t hat lowers the overall confidence  for the results presented.  \n",
            "Finally, it is interesting to observe how these generalizations affected the performance of \n",
            "the models, at least in cases where both data from same and different datasets were used \n",
            "in testing. In (Sa, et al., 2016) , F1-IoU(40) was higher for the detection of apples ( 0.938 ), \n",
            "strawberry  (0.948 ), avocado ( 0.932 ) and mango ( 0.942 ), than in the default case of sweet \n",
            "pepper  (0.838 ). In (Rahnemoonfar & Sheppard, 2017) , RFC was 2% less in the real \n",
            "images tha n in the synthetic ones. In (Potena, Nardi, & Pretto, 2016) , CA was 37.6% less \n",
            "at the dataset involving plants of 4-weeks more advanced growth. According to the \n",
            "authors, the model was trained based on plants that  were in their first growth stage, thus \n",
            "without their complete morphological features , which were included in the testing dataset.  \n",
            "Moreover, in (Milioto, Lottes, & Stachniss, 2017)  P was 2% higher at the 2 -weeks more \n",
            "advanced gr owth  dataset, with 9% lower R.  21 \n",
            " Hence, in the first case there was improvement in performance  (Sa, et al., 2016) , and in \n",
            "the last three  cases a  reduction , slight one in (Rahnemoonfar & Sheppard, 2017)  and \n",
            "(Milioto, Lottes, & Stachniss, 2017)  but considerable in (Potena, Nardi, & Pretto, 2016) . \n",
            "From the other papers using different testing datasets , as mentioned above , high \n",
            "percentages of CA (94-97.3%), P-IoU (86.6%) and low values of MRE (1. 8 -10%) have \n",
            "been reported. These show  that the DL models were able to generalize well to different \n",
            "datasets. However, without more comparisons, this is only a speculation that can be \n",
            "figured out of the sma ll number of observations available.  \n",
            "4.11 Performance Comparison with Other Approaches  \n",
            "A critical aspect of this survey is to examine how DL performs in relation to other existing \n",
            "techniques. The 14th column of Appendix II presents whether the authors of related work \n",
            "compared their DL-based approach with other techniques used for solving their problem \n",
            "under study. We focus only on comparisons between techniques used for the same \n",
            "dataset at the same rese arch paper, with the same metric.  \n",
            "In almost all cases, the DL models outperform other approaches implemented for \n",
            "comparison purposes. CNN show  1-8% higher CA  in comparison to SVM (Chen, Lin, \n",
            "Zhao, Wang, & Gu, 2014) , (Lee, Chan, Wilkin, & Remagnino, 2015) , (Grinblat, Uzal, \n",
            "Larese, & Granitto, 2016) , (Pound, M. P., et al., 2016) , 41% improvement of CA when \n",
            "compared to ANN (Lee, Chan, Wilkin, & Remagnino, 2015)  and 3 -8% higher CA when \n",
            "compared to RF (Kussul, Lavreniuk, Skakun, & Shelestov, 2017) , (Minh, et al., 2017) , \n",
            "(McCool, Perez, & Upcroft, 2017) , (Potena, Nardi, & Pretto, 2016) , (Hall, McCool, Dayoub, \n",
            "Sunderhauf, & Upcroft, 2015) . CNN also seem to be superior than unsupervised feature \n",
            "learning  with 3-11% higher CA (Luus, Salmon, van den Bergh, & Maharaj, 2015) , 2-44% \n",
            "improved  CA in relation to l ocal shape and color  features  (Dyrmann, Karstoft, & Midtiby, \n",
            "2016 ) , (Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017) , and 2% better CA (Kussul, \n",
            "Lavreniuk, Skakun, & Shelestov, 2017)  or 18% less RMSE (Song, et al., 2016)  compared 22 \n",
            " to multilayer perceptrons. CNN had also superior per formance than Penalized \n",
            "Discriminant Analysis  (Grinblat, Uzal, Larese, & Granitto, 2016) , SVM  Regression  (Kuwata \n",
            "& Shibasaki, 2015) , area-based technique s (Rahnemoonfar & Sheppard, 2017) , texture -\n",
            "based regression model s (Chen, et al., 2017) , LMC  classifiers (Xinshao & Cheng, 2015) , \n",
            "Gaussian Mixture Models (Santoni, Sensuse, Arymurthy, & F anany, 2015)  and Naïve -\n",
            "Bayes classifiers (Yalcin, 2017 ) . \n",
            "In cases where Recurrent Neural Networks (RNN ) (Mandic & Chambers, 2001)  \n",
            "architectures  were employed , the LSTM model had 1% higher CA than RF and SVM  in \n",
            "(Ienco, Gaetano, Dupaquier, & Maurel, 2017) , 44% improved CA than SVM  in (Rußwurm \n",
            "& Körner, 2017)  and 7 -9% better CA than RF and SVM  in (Minh, et al., 2017) . \n",
            "In only one case, DL showed worse performance against a nother  technique , and this was \n",
            "when a CNN was compared to an approach  involving local descriptors to represent \n",
            "images  together with KNN as the classification strategy (20% worse LC) (Reyes, Caicedo, \n",
            "& Camargo, 2015) . \n",
            " \n",
            "5. Discussion  \n",
            "Our analysis has shown that DL offers superior performance  in the vast majority of related \n",
            "work . When comparing the performance of DL -based approaches with other techniques at \n",
            "each paper, it is of paramount importance to adhere to the same experimental conditions \n",
            "(i.e. datasets and performance metrics ). From the related work under study, 28 out of the  \n",
            "40 papers ( 70%) performed direct, valid and correct comparisons among the DL -based \n",
            "approach employed and other state -of-art techniques used to solve the particular problem \n",
            "tackled at each paper. Due to the f act that each paper involved  different datasets,  pre-\n",
            "processing techniques, metrics, models and parameters, it is difficult if not impossible to \n",
            "generalize and perform comparisons between papers. Thus, our comparisons have been  \n",
            "strictly limited among the techniques used at each paper.  Thus, based on the se 23 \n",
            " constraints, we have observed that DL  has outperformed traditional approaches used such \n",
            "as SVM, RF, ANN, LMC classifiers and others. It seems that the automatic feature \n",
            "extraction performed  by DL models is more effective than the feature extraction proc ess \n",
            "through traditional approaches such as Scale Invariant Feature Transform (SIFT) , GLCM , \n",
            "histograms, a rea-based technique s (ABT), statistics -, texture -, color - and shape -based  \n",
            "algorithms, c onditional random fields to model color and visual texture featur es, local de -\n",
            "correlated channel feature s and other m anual feature extraction techniques . This is \n",
            "reinforced by the combined CNN+LSTM model employed in (Namin, Esmaeilzadeh, Najafi, \n",
            "Brown, & Borevitz, 2017) , which outperformed a  LSTM model which used h and crafted \n",
            "feature descriptors as inputs by 25% higher CA.  Interesting attempt s to combine hand -\n",
            "crafted features and CNN -based  features  were  performed in (Hall, McCool, Dayoub, \n",
            "Sunderhauf, & Upcroft, 2015)  and (Rebetez, J., et al., 2016) . \n",
            "Although DL has been associated with computer vision and image analysis  (which is also \n",
            "the general case in this survey) , we have observed  5 related works where DL-based  \n",
            "models have been  trained based on  field sensory  data (Kuwata & Shibasaki, 2015) , \n",
            "(Sehgal, et al., 2017)  and a combination of static and dynamic environmental variables  \n",
            "(Song, et al., 2016) , (Demmers T. G., et al., 2010) , (Demmers T. G., Cao, Parsons, Gauss, \n",
            "& Wathes, 2012) . These papers indicate the potent ial of DL to be applied in a wide variety \n",
            "of agricultural problems, not onl y those involving images.   \n",
            "Examining agricultural areas where DL techniques have been applied, l eaf classification , \n",
            "leaf and plant disease detection , plant recognition  and f ruit counting  have some papers \n",
            "which present very good performance (i.e. CA  > 95% , F1 > 0.92  or RFC  > 0.9 ). This is \n",
            "probably because of the availability of datasets in these domains, as well as the distinct \n",
            "characteristics of (sick) leaves/plants and fruits in the image. On the other hand, some \n",
            "papers in l and cover classification , crop type classification , plant phenology r ecognition  \n",
            "and weed detectio n showed average performance (i.e. CA < 87% or F1 < 0.8). This could 24 \n",
            " be due to leaf occlusion in weed detection, use of noise -prone satellite imagery in land \n",
            "cover problems, crops with low variation and botanical relationship or the fact that \n",
            "appearances change very gradually  while plants grow in phenology recognition efforts.  \n",
            "Without underestimating the quality of any of the surveyed papers, we highlight some that \n",
            "claim high performance (CA >  91%, F1-IoU(20) > 0.90 or RFC > 0.9 1), considering the \n",
            "complexity of the problem in terms of its definition or the large number of classes involved \n",
            "(more than 21 classes ). These papers are the following: (Mohanty, Hughes, & Salathé , \n",
            "2016) , (Luus, Salmon, van den Bergh, & Maharaj, 2015) , (Lee, Chan, Wilkin, & \n",
            "Remagnino, 2015) , (Rahnemoonfar & Sheppard, 2017) , (Chen, et al., 2017) , (Bargoti & \n",
            "Underwood, 2016) , (Xinshao & Cheng, 2015)  and (Hall, McCool, Dayoub, Sunderhauf, & \n",
            "Upcroft, 2015) . We also highlight papers that trained thei r models on simulated data, and \n",
            "tested them on real data, which are (Dyrmann, Mortensen, Midtiby, & Jørgensen, 2016) , \n",
            "(Rahnemoonfar & Sheppard, 2017) , and (Douarre, Schielei n, Frindel, Gerth, & Rousseau, \n",
            "2016) . These works constitute important efforts in the DL community, as they attempt to \n",
            "solve the problem of inexistent or not large enough datasets in various problems.  \n",
            "Finally, as discussed in Section 4.10, most author s used the same dataset s for training \n",
            "and testing their implementation, a fact that lowers the confidence in the overall findings, \n",
            "although there have been indications that the models seem to generalize well, with only \n",
            "small reductions in performance.  \n",
            "5.1 Advanced Deep Learning Applications  \n",
            "Although the majority of papers used typical CNN architectures to perform classification \n",
            "(23 papers, 57%), some authors experimented with more advanced models in order to \n",
            "solve more complex problems, such as c rop type classification from UAV imagery (CNN + \n",
            "HistNN using RGB histogram s) (Rebetez, J., et al., 2016) , estimating  number of tomato \n",
            "fruits (Modified Inception -ResNet CNN ) (Rahnemoonfar & Sheppard, 2017 ) and estimating \n",
            "number of orange or apple fruits  (CNN adapted for blob detection and counting + Linear 25 \n",
            " Regressi on) (Chen, et al., 2017) . Particularly interesting were the approaches employing \n",
            "the Faster Region -based CNN + VGG16 model  (Bargoti & Underwood, 2016) , (Sa, et al., \n",
            "2016) , in order not only to count fruits and vegetables, but also to locate their placement in \n",
            "the image by means of bounding box es. Similarly,  the work in (Dyrmann, Jørgensen, & \n",
            "Midtiby, 2017)  used the DetectNet CNN to detect bounding box es of weed instan ces in \n",
            "images of cereal fields. These approaches (Faster Region -based CNN , DetectNet CNN ) \n",
            "constitute a very promising research direction, since the task of identifying the bounding \n",
            "box of fruits/vegetables/weeds in an image has numerous real -life applications and could \n",
            "solve various agricultural problems  \n",
            "Moreover, considering not only space bu t also time series, some authors employed RNN -\n",
            "based models in land cover classification (one-unit LSTM model + SVM) (Ienco, Gaetano, \n",
            "Dupaquier, & Maurel, 2017) , crop type classification (three -unit LSTM ) (Rußwurm & \n",
            "Körner, 2017) , classification of different accessions of Arabidopsis thaliana based on \n",
            "successive top -view images (CNN+ LSTM ) (Namin, Esmaeilzadeh, Najafi, Brown, & \n",
            "Borevitz, 2017) , mapping winter vegetation qu ality coverage  (Five -unit LSTM, Gated \n",
            "Recurrent Unit) (Minh, et al., 2017) , estimating  the weight of pigs or chickens ( DRNN ) \n",
            "(Demmers T. G., et al., 2010) , (Demmers T. G., Cao, Parsons, Gauss, & Wathes, 2012)  \n",
            "and for  predict ing weather based on previous year’s conditions  (LSTM) (Sehgal, et al., \n",
            "2017) . RNN -based models offer higher performance, as they can capture the time \n",
            "dimension, which is  impossible to be exploited by simple CNN. RNN architectures tend to \n",
            "exhibit dynamic temporal behavior , being able to record long-short temporal \n",
            "dependencies , remember ing and forgetting  after some time or when needed (i.e. LSTM). \n",
            "Differences in performance  between RNN and CNN are distinct in the related work under \n",
            "study, as shown in Table 2. This 16% improvement in CA could be attributed to the \n",
            "additional information provided by the time series.  For example, in the crop type \n",
            "classification case (Rußwurm &  Körner, 2017) , the authors mention,  “crops  change their \n",
            "26 \n",
            " spectral characteristics due to environmental influences and can thus not be monitored \n",
            "effectively with classical mono -temporal approaches. Performance of temporal models \n",
            "increases at the beginn ing of vegetation period ”. LSTM -based approach es work  well also \n",
            "for low represented and difficult classes , as demonstrated in (Ienco, Gaetano, Dupaquier, \n",
            "& Maurel, 2017) . \n",
            "Table 2: Difference in Performance between CNN and RNN.  \n",
            "No. Application in \n",
            "Agriculture  Performan\n",
            "ce Metric  Difference in Performance  Reference  \n",
            "1. Crop type classification \n",
            "considering time series  CA, F1  Three -unit LSTM : 76.2% (CA), \n",
            "0.558  (F1)  \n",
            "CNN: 59.9% (CA), 0.236 (F1)  (Rußwurm & Körner, 2017)  \n",
            "2. Classify the phenotyping \n",
            "of Arabidopsis in four \n",
            "accessions  CA CNN+ LSTM : 93%  \n",
            "CNN: 76.8%  (Namin, Esmaeilzadeh, \n",
            "Najafi, Brown, & Borevitz, \n",
            "2017)  \n",
            " \n",
            "Finally, the critical aspect of fast processing of DL models in order to be easily  used in \n",
            "robots for real -time decision making (e.g. detection of weeds) was examined in (McCool, \n",
            "Perez, & Upcroft, 2017) , and it is worth -mention ing. The authors have showed that a  \n",
            "lightweight implementation had only a small penalty in CA (3.90%), being much faster  (i.e. \n",
            "processing of 40.6 times more  pixels  per second ). They proposed the  idea of “teacher and \n",
            "student network s”, where the teacher is the more heavy approach that helps the student \n",
            "(light implementation) to learn faster and better.  \n",
            "5.2 Advantages  of Deep Learning  \n",
            "Except from  improvements in performance  of the classification/prediction  problems in the \n",
            "surveyed works  (see Sections 4.9 and 4.11) , the advantage  of DL in terms of reduced \n",
            "effort in feature engineering was demonstrated in many  of the papers . Hand-engineered \n",
            "components  require considerable time, an effort that takes place automatically in DL. \n",
            "Besides, sometimes manual search for good feature extractors  is not an easy and obvious \n",
            "task. For example, in the case of estimating crop yield (Kuwata & Shibasaki, 2015) , \n",
            "extract ing manually  features that significantly affect ed crop growth was not possible.  This 27 \n",
            " was also the case of estimating the soil moisture content  (Song, et al., 2016) . \n",
            "Moreover, DL models seem to generalize well. For example, i n the case  of fruit counting , \n",
            "the model  learned  explicitly to count  (Rahnemoonfar & Sheppard, 2017) . In the banana \n",
            "leaf classification problem (Amara, Bouaziz, & Algergawy, 2017) , the model was robust \n",
            "under challenging conditions such as illumination, complex backg round, different \n",
            "resolution, size  and orien tation of  the images . Also in the fruits counting paper s (Chen, et \n",
            "al., 2017) , (Rahnemoonfar & Sheppard, 2017) , the model s were  robust to occlusion, \n",
            "variation,  illumination and scale . The same detection framework s could be used for a \n",
            "variety of circular fruits such as peaches, citrus, mangoes etc . As another example, a  key \n",
            "feature of the DeepAnomaly model was  the ability to detect unknown objects/anoma lies \n",
            "and not ju st a set of predefined objects, exploit ing the homogeneous characteristics of an \n",
            "agricultural field to detect distant, heavy occluded and unknown objects  (Christiansen, \n",
            "Nielsen, Steen, Jørgensen, & Karstoft, 2016) . Moreover, in the 8 papers mentioned in \n",
            "Section 4.10 where different datasets were used for testing, the performance of the model \n",
            "was generally high, with only small reductions in performance in comparison with the \n",
            "performance when using the same dataset  for training and testing.  \n",
            "Although DL takes longer time to train than other traditional approaches (e.g. SVM, RF), its \n",
            "testing time efficiency is quite fast. For example, i n detecting obstacles and anomaly \n",
            "(Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016) , the model took  much  longer \n",
            "to train,  but after it did, its testing time was less than the one of SVM and KNN . Besides, if \n",
            "we take into account the time needed to manually design filters and extract features, “ the \n",
            "time used on annotating images and training the CNN becomes almost negligible ” \n",
            "(Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017) .  \n",
            "Another advantage of DL is the possibility to develop simulated datasets to train the \n",
            "model, whic h could be properly designed in order to solve real -world problems. For \n",
            "example, in the issue of detecting weeds and maize in fields (Dyrmann, Mortensen, 28 \n",
            " Midtiby, & Jørgensen, 2016) , the authors overcame  the pl ant foliage overlapping problem \n",
            "by simulating top -down images of overlapping plants on soil background . The trained \n",
            "network was then  capable of distinguish weeds from maize even in  overlapping conditions.  \n",
            "5.3 Disadvantages  and Limitations of Deep Learning  \n",
            "A considerab le drawback and barrier in the use of DL is the need of  large dataset s, which \n",
            "would serve as the input during the training procedure. In spite of data augmentation \n",
            "techniques which augment some dataset with label -preserving transformations, in reality at \n",
            "least some h undreds of images are required, depending on the comple xity of the problem \n",
            "under study  (i.e. number of classes, precision required etc.). For example, the authors in \n",
            "(Mohanty, Hughes, & Salathé, 2016)  and (Sa, et al., 2016)  comment ed that a more  diverse \n",
            "set of training data was needed  to improve CA.  A big problem with many datasets is the \n",
            "low variation among the different classes  (Yalcin, 2017 ) , as discussed in Section 4.3 , or \n",
            "the existence of noise, in the form of low resolution , inaccuracy of sensory equipment \n",
            "(Song, et al., 2016) , crops’ occlusions , plants overlapping  and clustering , and others . \n",
            "As data annotation is a necessary operation in the large majority of cases, some tasks are \n",
            "more complex and there is a need for experts (who might be difficult to involve) in order to \n",
            "annotate input images. As mentioned in (Amara, Bouaziz, & Algergawy, 2017) , there is a \n",
            "limited availability of resources and expertise on banana pathology worldwide . In some \n",
            "cases, experts or volunteers are susceptible to errors during data labeling, especially when \n",
            "this is a challenging task e.g. fruit count (Chen, et al., 2017) , (Bargoti & Underwood, 2016)  \n",
            "or to determine if images  contain weeds  or not  (Sørensen, Rasmussen, Nielsen, & \n",
            "Jørgensen, 2017) , (Dyrmann, Jørgensen, & Midtiby, 2017) . \n",
            "Another limitation is the fact that the DL models can learn some problem particularly well, \n",
            "even generalize in some aspects as mentioned in Section 5.2, but they cannot generalize \n",
            "beyond the “boundaries of the data set’s expressiveness”. For example, classification of \n",
            "single leaves, facing up, on a homogeneous background  is performed in (Mohanty, 29 \n",
            " Hughes, & Salathé, 2016) . A real world application should be able to classify images of a \n",
            "disease as it presents itself directly on the plant. Many  diseases do not  present \n",
            "themselves on the upper side of leaves only.  As another example, plant recognition in \n",
            "(Lee, Chan, Wilkin, & Remagnino, 2015)  was noticeably  affected by environmental factors \n",
            "such as wrinkled surface and insect damages . The model for counting tomatoes in \n",
            "(Rahnemoonfar & Sheppard, 2017)  could count ripe and half -ripe fruits, however, “ it failed \n",
            "to count green fruits  because it was not trained for this purpose ”. If an object size in a \n",
            "testing image was significantly less than that of a training set, the model missed  the \n",
            "detection  in (Sa, et al., 2016) . Difficulty in detecting heavily occlu ded and distant objects  \n",
            "was observed in (Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016) . Occlusion \n",
            "was a serious issue also in (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015) . \n",
            "A general issue in computer vision, not only in DL, is tha t data pre -processing is \n",
            "sometimes a necessary and time -consuming task, espe cially when satellite or aerial \n",
            "photos are involved, as we saw in Section 4.4. A problem with hyperspectral data is their \n",
            "high dimensionality and limited training samples (Chen, Lin, Zhao, Wang, & Gu, 2014) . \n",
            "Moreover, sometimes the existing datasets do not describe completely the problem they \n",
            "target  (Song, et al., 2016) . As an example, for estimating corn yield (Kuwata & Shibasaki, \n",
            "2015) , it was necessary to consider also external factors other than the weather by \n",
            "inputting cultivation information such as fertilization and irrigation .  \n",
            "Finall y, in the domain of agriculture, there do not exist many publicly available  datasets for \n",
            "researchers to work with, and in many cases,  researchers need to develop their own sets \n",
            "of images . This could require many hours or days of work.  \n",
            "5.4 Future of Deep Learning in Agriculture  \n",
            "Observing Appendix I, which lists various existing applications of computer vision  in \n",
            "agriculture , we can see that only the problems of l and cover classification , crop type \n",
            "estimation, crop phenology , weed detection  and fruit gradin g have been approximated 30 \n",
            " using DL. It is interesting to see how DL would behave also in other agricultural -related \n",
            "problems listed in Appendix I, such as seeds identification, s oil and leaf nitrogen content , \n",
            "irrigation, p lants ’ water stress detection , water erosion assessment , pest detection , \n",
            "herbicide use, identification of contaminants, diseases or defect s on food, c rop hail \n",
            "damage  and greenhouse monitoring. I ntuitively , since many of the aforementioned \n",
            "research areas employ data analysis techniques (see Appendix I) with similar concepts  \n",
            "and comparable performance to DL (i.e. linear and logistic regression, SVM, KNN, K -\n",
            "means clustering, Wavelet -based filtering, Fourier transform ) (Singh, \n",
            "Ganapathysubramanian, Singh, & Sarkar, 2 016), then it could be worth to examine the \n",
            "applicability of DL on these problems too.  \n",
            "Other possible  application areas could be the use of aerial imagery (i.e. by means of \n",
            "drones) to monitor the effectiveness  of the seeding  process , to increase the quality of wine \n",
            "production by harvest ing grapes at the right moment  for best maturity levels , to monitor \n",
            "animals and their movements to  consider their overall welfare  and identify possible \n",
            "diseases , and many other scenarios where computer visi on is involved.  \n",
            "In spite of the limited availability of open datasets in agriculture, In Appendix III, we list \n",
            "some of the most popular, free to download datasets available on the web, which could be \n",
            "used by researchers to start testing their DL architect ures. These datasets could be used \n",
            "to pre -train DL models and then adapt them to more specific future agricultural challenges. \n",
            "In addition to these  datasets , remote sensing data containing multi -temporal , multi -spectral \n",
            "and multi -source images that could b e used in problems related to land and crop cover \n",
            "classification are available from satellites such as MERIS, MODIS, AVHRR , RapidEye , \n",
            "Sentinel , Landsat  etc. \n",
            "More approaches adopting LSTM or other RNN models are expected in the future, \n",
            "exploiting the time dimension to perform higher performance classification or prediction.  \n",
            "An example application could be to estimate the growth of plants, trees or even animals 31 \n",
            " based on previous consecutive observations, to predict their yield, assess their water \n",
            "needs or av oid diseases from occurring. These models could find applicability in \n",
            "environmental informatics too, for understanding climatic change, predicting weather \n",
            "conditions and phenomena, estimating the environmental impact of various physical or \n",
            "artificial proce sses (Kamilaris, Assumpcio, Blasi, Torrellas, & Prenafeta -Boldú, 2017)  etc. \n",
            "Related work under study involved up to a five-unit LSTM model (Minh, et al., 2017) . We \n",
            "expect in the future to see more layers stacked together  in order  to build more complex  \n",
            "LSTM a rchitectures  (Ienco, Gaetano, Dupaquier, & Maurel, 2017) . We also believe that \n",
            "datasets with increasing temporal sequence length  will appear, which could improve the \n",
            "performance of LSTM (Rußwurm & Körner, 2017) . \n",
            "Moreover, more complex architectures would appear, combining various DL models and \n",
            "classifiers together, or combining hand -crafted features with automatic features extracted \n",
            "by using various techniques, fused together to improve the overall outcome, similar to \n",
            "what performed in (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015)  and (Rebetez , J., \n",
            "et al., 2016) . Researchers are expected to test their models using more general and \n",
            "realistic dataset, demonstrating the ability of the models to generalize to various real -world \n",
            "situations. A combination of popular performance metrics, such as the ones mentioned in \n",
            "Table 1, are essential to be adopted  by the authors  for comparison purposes. It would be \n",
            "desirable if researchers made their data sets publicly available, for use also by the general \n",
            "research community.  \n",
            "Finally, some of the solutions discussed in the surveyed papers could have a commercial \n",
            "use in the near future. The approaches incorporating Faster Region -based CNN  and \n",
            "DetectNet CNN  (Bargoti & Underwood, 2016) , (Chen, et al., 2017) , (Rahnemoonfar & \n",
            "Sheppard, 2017)  would be extremely useful for automatic robots that collect crops, remove \n",
            "weeds or for  estimating the expected yields of various crops. A f uture application of this \n",
            "technique could be also in microbiology for human or animal cell counting  (Chen, et al., \n",
            "32 \n",
            " 2017) . The DRNN model controlling  the daily feed intake of pigs or chicken , predicting \n",
            "quite  accurately the required feed intake for the whole of the growing period , would be \n",
            "useful to farmers when deciding on a growth curve suitable for various scenarios . \n",
            "Following some growth patterns would have  potential advant ages for animal  welfare in \n",
            "terms of leg health, without compromising the idea animals’ final weight and total feed \n",
            "intake requirement  (Demmers T. G., et al., 2010) , (Demmers T. G., Cao, Parsons, Gauss, \n",
            "& Wathes, 2012) .  \n",
            " \n",
            "6. Conclusion  \n",
            "In this paper, we have perform ed a survey of deep learning -based  research efforts  applied \n",
            "in the agricultural domain . We have identified 40 relevant papers, examining the particular \n",
            "area and problem they focus on, technical details of the models  employed, sources of data \n",
            "used , pre-processing tasks and data augmentation techniques adopted,  and overall \n",
            "performance according to the performance m etrics employed by each paper . We have \n",
            "then compared deep learning with other existing techniques, in terms of performance . Our \n",
            "findings indicate that deep learning offers  better performance  and outperforms other \n",
            "popular image processing techniques.  For fu ture work, we plan to apply the general \n",
            "concepts and best practices of deep learning, as described through this survey, to other \n",
            "areas of agriculture where this modern technique has not yet been adequately  used. Some \n",
            "of these areas have been identified  in the discussion  section . \n",
            "Our aim is that  this survey would  motivate more researchers t o experiment with deep \n",
            "learning, apply ing it for  solving various  agricultural problems  involving  classification or \n",
            "prediction , related to com puter vision and image analysis, or more generally to data \n",
            "analysis . The overall benefits of deep learning are encouraging for its further use towards \n",
            "smarter , more sustainable farming and more secure food production.  \n",
            " 33 \n",
            " Acknowledgments  \n",
            "We would like to than k the reviewers, whose valuable feedback, suggestions and \n",
            "comments increased significantly the overall quality of this survey. This research has been \n",
            "supported by the P -SPHERE project, which has received funding from the European \n",
            "Union’s Horizon 2020 resea rch and innovation programme under the Marie Skodowska -\n",
            "Curie grant agreement No 665919.  \n",
            "References  \n",
            "Amara, J., Bouaziz, B., & Algergawy, A. (2017). A Deep Learning -based Approach for Banana \n",
            "Leaf Diseases Classification. (págs. 79 -88). Stuttgart: BTW workshop.  \n",
            "Bahrampour, S., Ramakrishnan, N., Schott, L., & Shah, M. (2015). Comparative study of deep \n",
            "learning software frameworks. arXiv preprint arXiv, 1511 (06435).  \n",
            "Bargoti, S., & Underwood, J. (2016). Deep Fruit Detection in Orchards. arXiv preprint arXiv, \n",
            "1610 (03677).  \n",
            "Bastiaanssen, W., Molden, D., & Makin, I. (2000). Remote sensing for irrigated agriculture: \n",
            "examples from research and possible applications. Agricultural water management, 46 (2), \n",
            "137-155. \n",
            "Canziani, A., Paszke, A., & Culurcie llo, E. (2016). An Analysis of Deep Neural Network Models \n",
            "for Practical Applications. arXiv preprint arXiv, 1605 (07678).  \n",
            "Chen, S. W., Shivakumar, S. S., Dcunha, S., Das, J., Okon, E., Qu, C., & Kumar, V. (2017). \n",
            "Counting Apples and Oranges With Deep Learni ng: A Data -Driven Approach. IEEE \n",
            "Robotics and Automation Letters, 2 (2), 781 -788. \n",
            "Chen, Y., Lin, Z., Zhao, X., Wang, G., & Gu, Y. (2014). Deep Learning -Based Classification of \n",
            "Hyperspectral Data. IEEE Journal of Selected topics in applied earth observations  and \n",
            "remote sensing, 7 (6), 2094 -2107.  \n",
            "Chi, M., Plaza, A., Benediktsson, J. A., Sun, Z., Shen, J., & Zhu, Y. (2016). Big data for remote \n",
            "sensing: challenges and opportunities. Proceedings of the IEEE, 104 (11), 2207 -2219.  \n",
            "Christiansen, P., Nielsen, L. N., St een, K. A., Jørgensen, R. N., & Karstoft, H. (2016). \n",
            "DeepAnomaly: Combining Background Subtraction and Deep Learning for Detecting \n",
            "Obstacles and Anomalies in an Agricultural Field. Sensors , 16 (11), 1904.  \n",
            "Demmers, T. G., Cao, Y., Gauss, S., Lowe, J. C., Pa rsons, D. J., & Wathes, C. M. (2010). Neural \n",
            "Predictive Control of Broiler Chicken Growth. IFAC Proceedings Volumes, 43 (6), 311 -316. \n",
            "Demmers, T. G., Cao, Y., Parsons, D. J., Gauss, S., & Wathes, C. M. (2012). Simultaneous \n",
            "Monitoring and Control of Pig Grow th and Ammonia Emissions. IX International Livestock \n",
            "Environment Symposium (ILES IX).  Valencia, Spain: American Society of Agricultural and \n",
            "Biological Engineers.  \n",
            "Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei -Fei, L. (2009). Imagenet: A large -scale \n",
            "hierarchical image database. (págs. 248 -255). Miami, FL, USA: IEEE Conference on \n",
            "Computer Vision and Pattern Recognition (CVPR).  \n",
            "Deng, L., & Yu, D. (2014). Deep learning: methods and applications. Foundations and Trends in \n",
            "Signal Processing, 7 (3-4), 197 -387. \n",
            "Douarre, C., Schielein, R., Frindel, C., Gerth, S., & Rousseau, D. (2016). Deep learning based root -\n",
            "soil segmentation from X -ray tomography. bioRxiv, 071662 . \n",
            "Dyrmann, M., Jørgensen, R. N., & Midtiby, H. S. (2017). RoboWeedSupport - Detection of weed \n",
            "locations in leaf occluded cereal crops using a fully convolutional neural network. 11th 34 \n",
            " European Conference on Precision Agriculture (ECPA).  Edinburgh, Scotland.  \n",
            "Dyrmann, M., Karstoft, H., & Midtiby, H. S. (2016 ). Plant species classification using deep \n",
            "convolutional neural network. Biosystems Engineering, 151 , 72-80. \n",
            "Dyrmann, M., Mortensen, A. K., Midtiby, H. S., & Jørgensen, R. N. (2016). Pixel -wise \n",
            "classification of weeds and crops in images by using a fully convolutional neural network. \n",
            "International Conference on Agricultural Engineering.  Aarhus, Denmark.  \n",
            "FAO. (2009). How to Feed the World in 2050.  Rome: Food and Agriculture Organization of the \n",
            "United Nations.  \n",
            "Gebbers, R., & Adamchuk, V. I. (2010). Precision agriculture and food security. Science, \n",
            "327(5967), 828 -831. \n",
            "Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual prediction with \n",
            "LSTM. Neural Computation, 12 (10), 2451 -2471.  \n",
            "Grinblat, G. L., Uzal, L. C., Larese, M. G., & Granitto, P. M. (2016). Deep learning for plant \n",
            "identification using vein morphological patterns. Computers and Electronics in Agriculture, \n",
            "127, 418 -424. \n",
            "Hall, D., McCool, C., Dayoub, F., Sunderhauf, N., & Upcroft, B. (2015). Evaluation of features for \n",
            "leaf classification in challenging conditions. Winter Conference on Applications of \n",
            "Computer Vision (WACV)  (págs. 797 -804). Waikoloa Beach, Hawaii: IEEE.  \n",
            "Hashem, I., Yaqoob, I., Anuar, N., Mokhtar, S., Gani, A., & Khan, S. (2015). The rise of “big data” \n",
            "on cloud computing: Review and open research issu es. Information Systems, 47 , 98-115. \n",
            "Ienco, D., Gaetano, R., Dupaquier, C., & Maurel, P. (2017). Land Cover Classification via Multi -\n",
            "temporal Spatial Data by Recurrent Neural Networks.  arXiv preprint arXiv:1704.04055.  \n",
            "Ishimwe, R., Abutaleb, K., &  Ahmed, F. (2014). Applications of thermal imaging in agriculture —A \n",
            "review. Advances in Remote Sensing, 3 (3), 128.  \n",
            "Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., . . . Darrell, T. (2014). \n",
            "Caffe: Convolutional architecture for fas t feature embedding. Proceedings of the 22nd \n",
            "International Conference on Multimedia  (págs. 675 -678). Orlando, FL, USA: ACM.  \n",
            "Kamilaris, A., Assumpcio, A., Blasi, A. B., Torrellas, M., & Prenafeta -Boldú, F. X. (2017). \n",
            "Estimating the Environmental Impact of A griculture by Means of Geospatial and Big Data \n",
            "Analysis: The Case of Catalonia. From Science to Society  (págs. 39 -48). Luxembourg: \n",
            "Springer.  \n",
            "Kamilaris, A., Gao, F., Prenafeta -Boldú, F. X., & Ali, M. I. (2016). Agri -IoT: A semantic \n",
            "framework for Internet of  Things -enabled smart farming applications. 3rd World Forum on \n",
            "Internet of Things (WF -IoT) (págs. 442 -447). Reston, VA, USA: IEEE.  \n",
            "Kamilaris, A., Kartakoullis, A., & Prenafeta -Boldú, F. X. (2017). A review on the practice of big \n",
            "data analysis in agricultur e. Computers and Electronics in Agriculture, 143 (1), 23 -37. \n",
            "Kitzes, J., Wackernagel, M., Loh, J., Peller, A., Goldfinger, S., Cheng, D., & Tea, K. (2008). Shrink \n",
            "and share: humanity's present and future Ecological Footprint. Philosophical Transactions \n",
            "of the Royal Society of London B: Biological Sciences, 363 (1491), 467 -475. \n",
            "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep \n",
            "convolutional neural networks. Advances in neural information processing systems , 1097 -\n",
            "1105.  \n",
            "Kussul, N., Lavreniuk, M., Skakun, S., & Shelestov, A. (2017). Deep Learning Classification of \n",
            "Land Cover and Crop Types Using Remote Sensing Data. IEEE Geoscience and Remote \n",
            "Sensing Letters, 14 (5), 778 -782. \n",
            "Kuwata, K., & Shibasaki, R. (2015). Estimating crop  yields with deep learning and remotely sensed \n",
            "data. (págs. 858 -861). Milan, Italy: IEEE International Geoscience and Remote Sensing \n",
            "Symposium (IGARSS).  \n",
            "LeCun, Y., & Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The \n",
            "handboo k of brain theory and neural networks, 3361 (10).  \n",
            "LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521 (7553), 436 -444. \n",
            "Lee, S. H., Chan, C. S., Wilkin, P., & Remagnino, P. (2015). Deep -plant: Plant identification with 35 \n",
            " convolutional neural networks. (págs. 452 -456). Quebec city, Canada: IEEE International \n",
            "Conference on Image Processing (ICIP).  \n",
            "Liaghat, S., & Balasundram, S. K. (2010). A review: The role of remote sensing in precision \n",
            "agriculture. American journal of agricultural and biologic al sciences, 5 (1), 50 -55. \n",
            "Lu, H., Fu, X., Liu, C., Li, L. G., He, Y. X., & Li, N. W. (2017). Cultivated land information \n",
            "extraction in UAV imagery based on deep convolutional neural network and transfer \n",
            "learning. Journal of Mountain Science, 14 (4), 731 -741. \n",
            "Luus, F. P., Salmon, B. P., van den Bergh, F., & Maharaj, B. T. (2015). Multiview deep learning for \n",
            "land-use classification. IEEE Geoscience and Remote Sensing Letters, 12 (12), 2448 -2452.  \n",
            "Mandic, D. P., & Chambers, J. A. (2001). Recurrent neural networks  for prediction: learning \n",
            "algorithms, architectures and stability.  New York: John Wiley.  \n",
            "McCool, C., Perez, T., & Upcroft, B. (2017). Mixtures of Lightweight Deep Convolutional Neural \n",
            "Networks: Applied to Agricultural Robotics. IEEE Robotics and Automation  Letters, 2 (3), \n",
            "1344 -1351.  \n",
            "Milioto, A., Lottes, P., & Stachniss, C. (2017). Real -time blob -wise sugar beets vs weeds \n",
            "classification for monitoring fields using convolutional neural networks. Proceedings of the \n",
            "International Conference on Unmanned Aerial Ve hicles in Geomatics.  Bonn, Germany.  \n",
            "Minh, D. H., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., & Maurel, P. (2017). \n",
            "Deep Recurrent Neural Networks for mapping winter vegetation quality coverage via multi -\n",
            "temporal SAR Sentinel -1. arXiv prep rint arXiv:1708.03694.  \n",
            "Mohanty, S. P., Hughes, D. P., & Salathé, M. (2016). Using deep learning for image -based plant \n",
            "disease detection. Frontiers in plant science, 7 . \n",
            "Mortensen, A. K., Dyrmann, M., Karstoft, H., Jørgensen, R. N., & Gislum, R. (2016). Sema ntic \n",
            "segmentation of mixed crops using deep convolutional neural network. International \n",
            "Conference on Agricultural Engineering.  Aarhus, Denmark.  \n",
            "Najafabadi, M. M., Villanustre, F., Khoshgoftaar, T. M., Seliya, N., Wald, R., & Muharemagic, E. \n",
            "(2015). Deep l earning applications and challenges in big data analytics. Journal of Big Data, \n",
            "2(1), 1.  \n",
            "Namin, S. T., Esmaeilzadeh, M., Najafi, M., Brown, T. B., & Borevitz, J. O. (2017). Deep \n",
            "Phenotyping: Deep Learning For Temporal Phenotype/Genotype Classification.  bioRxiv, \n",
            "134205.  \n",
            "Ozdogan, M., Yang, Y., Allez, G., & Cervantes, C. (2010). Remote sensing of irrigated agriculture: \n",
            "Opportunities and challenges. Remote sensing, 2 (9), 2274 -2304.  \n",
            "Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transacti ons on knowledge and \n",
            "data engineering , 22 (10), 1345 -1359.  \n",
            "PASCAL VOC Project. (2012). The PASCAL Visual Object Classes . Obtenido de \n",
            "http://host.robots.ox.ac.uk/pascal/VOC/  \n",
            "Potena, C., Nardi, D., &  Pretto, A. (2016). Fast and accurate crop and weed identification with \n",
            "summarized train sets for precision agriculture. International Conference on Intelligent \n",
            "Autonomous Systems  (págs. 105 -121). Shanghai, China: Springer, Cham.  \n",
            "Pound, M. P., et al. (2016 ). Deep Machine Learning provides state -of-the-art performance in \n",
            "image -based plant phenotyping.  bioRxiv, 053033.  \n",
            "Rahnemoonfar, M., & Sheppard, C. (2017). Deep Count: Fruit Counting Based on Deep Simulated \n",
            "Learning. Sensors, 17 (4), 905.  \n",
            "Rebetez, J., et al.  (2016). Augmenting a convolutional neural network with local histograms —a \n",
            "case study in crop classification from high -resolution UAV imagery. European Symposium \n",
            "on Artificial Neural Networks, Computational Intelligence and Machine Learning.  Bruges, \n",
            "Belgiu m. \n",
            "Reyes, A. K., Caicedo, J. C., & Camargo, J. E. (2015). Fine -tuning Deep Convolutional Networks \n",
            "for Plant Recognition. Toulouse: CLEF (Working Notes).  \n",
            "Rußwurm, M., & Körner, M. (2017). Multi -Temporal Land Cover Classification with Long Short -\n",
            "Term Memory Neural Networks. International Archives of the Photogrammetry, Remote 36 \n",
            " Sensing & Spatial Information Sciences, 42 . \n",
            "Sa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., & McCool, C. (2016). Deepfruits: A fruit \n",
            "detection system using deep neural networks. Sensors, 16 (8), 1222.  \n",
            "Santoni, M. M., Sensuse, D. I., Arymurthy, A. M., & Fanany, M. I. (2015). Cattle Race \n",
            "Classification Using Gray Level Co -occurrence Matrix Convolutional Neural Networks. \n",
            "Procedia Computer Science, 59 , 493 -502. \n",
            "Saxena, L., & Armstrong, L. (2014). A survey of image processing techniques for agriculture. \n",
            "Perth, Australia: Proceedings of Asian Federation for Information Technology in \n",
            "Agriculture, Australian Society of Information and Communication Technologies in \n",
            "Agriculture.  \n",
            "Schmidhuber, J. ( 2015). Deep learning in neural networks: An overview. Neural networks, 61 , 85-\n",
            "117. \n",
            "Sehgal, G., Gupta, B., Paneri, K., Singh, K., Sharma, G., & Shroff, G. (2017). Crop Planning using \n",
            "Stochastic Visual Optimization.  arXiv preprint arXiv:1710.09077.  \n",
            "Simonyan,  K., & Zisserman, A. (2014). Very deep convolutional networks for large -scale image \n",
            "recognition. arXiv preprint arXiv, 1409 (1556).  \n",
            "Singh, A., Ganapathysubramanian, B., Singh, A. K., & Sarkar, S. (2016). Machine Learning for \n",
            "High -Throughput Stress Phenotypi ng in Plants. Trends in Plant Science, 21 (2), 110 -124. \n",
            "Sladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D., & Stefanovic, D. (2016). Deep neural \n",
            "networks based recognition of plant diseases by leaf image classification. Computational \n",
            "intelligence and n euroscience, 2016 . \n",
            "Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., & Yang, J. (2016). Modeling spatio -temporal \n",
            "distribution of soil moisture by deep learning -based cellular automata model. Journal of \n",
            "Arid Land, 8 (5), 734 -748. \n",
            "Sørensen, R. A., Rasmussen, J. , Nielsen, J., & Jørgensen, R. (2017). Thistle detection using \n",
            "convolutional neural networks. Montpellier, France: EFITA Congress.  \n",
            "Steen, K. A., Christiansen, P., Karstoft, H., & Jørgensen, R. N. (2016). Using Deep Learning to \n",
            "Challenge Safety Standard for  Highly Autonomous Machines in Agriculture. Journal of \n",
            "Imaging, 2 (1), 6.  \n",
            "Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception -v4, Inception -ResNet and \n",
            "the Impact of Residual Connections on Learning. (págs. 4278 -4284). AAAI.  \n",
            "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., & Rabinovich, A. (2015). \n",
            "Going deeper with convolutions. (págs. 1 -9). Boston, MA, USA: IEEE conference on \n",
            "computer vision and pattern recognition.  \n",
            "Teke, M., Deveci, H. S., Haliloğlu, O., Gürbüz, S. Z. , & Sakarya, U. (2013). A short survey of \n",
            "hyperspectral remote sensing applications in agriculture. Istanbul, Turkey: 6th International \n",
            "Conference on Recent Advances in Space Technologies (RAST), IEEE.  \n",
            "Tyagi, A. C. (2016). Towards a Second Green Revolution . Irrigation and Drainage, 65 (4), 388 -\n",
            "389. \n",
            "Waga, D., & Rabah, K. (2014). Environmental conditions’ big data management and cloud \n",
            "computing analytics for sustainable agriculture. World Journal of Computer Application and \n",
            "Technology, 2 (3), 73 -81. \n",
            "Wan, J., Wang, D., Hoi, S. C., Wu, P., Zhu, J., Zhang, Y., & Li, J. (2014). Deep learning for \n",
            "content -based image retrieval: A comprehensive study. (págs. 157 -166). Orlando, FL: \n",
            "Proceedings of the 22nd ACM international conference on Multimedia, ACM.  \n",
            "Weber, R. H., & Weber, R. (2010). Internet of Things  (Vol. 12). New York, NY, USA: Springer.  \n",
            "Xinshao, W., & Cheng, C. (2015). Weed seeds classification based on PCANet deep learning \n",
            "baseline. (págs. 408 -415). IEEE Signal and Information Processing Association Annual \n",
            "Summit and Conference (APSIPA).  \n",
            "Yalcin, H. (2017 ). Plant phenology recognition using deep learning: Deep -Pheno. 6th International \n",
            "Conference on Agro -Geoinformatics.  Fairfax VA, USA.  \n",
            "  37 \n",
            " Appendix  I: Applications of computer vision  in agriculture and popular techniques used.  \n",
            "No. Application in \n",
            "Agriculture  Remote sensing  Techniques for data analysis  \n",
            "1. Soil and \n",
            "vegetation/crop \n",
            "mapping  Hyperspectral imaging \n",
            "(satellite and airborne), \n",
            "multi‐spectral imaging \n",
            "(satellite), synthetic \n",
            "aperture radar (SAR)  Image fusion, SVM , end -member extraction algorithm, \n",
            "co-polarized phase differences (PPD), linear \n",
            "polarizations (HH, VV, HV), distance -based \n",
            "classification, decision trees, linear mixing models, \n",
            "logistic regression, ANN, NDVI  \n",
            "2. Leaf a rea index \n",
            "and crop canopy  Hyperspectral imaging \n",
            "(airborne), multi ‐spectral \n",
            "imaging (airborne)  Linear regression analysis, NDVI  \n",
            "3. Crop phenology  Satellite remote sensing \n",
            "(general)  Wavelet -based filtering, Fourier transforms, NDVI  \n",
            "4. Crop height, \n",
            "estimation of \n",
            "yields, fertilizers' \n",
            "effect and \n",
            "biomass  Light Detection and \n",
            "Ranging (LIDAR), \n",
            "hyperspectral and multi -\n",
            "spectral imaging, SAR, \n",
            "red-edge camera, \n",
            "thermal infrared  Linear and exponential regression analysis, linear  \n",
            "polarizations (VV), wavelet -based  filtering, vegetation \n",
            "indices (NDVI, ICWSI), ANN  \n",
            "5. Crop monitoring  Satellite remote sensing, \n",
            "(hyperspectral and multi -\n",
            "spectral imaging), NIR \n",
            "camera, SAR  Stepwise discriminate analysis (DISCRIM) feature \n",
            "extraction, linear regression analysis, co -polarize d phase \n",
            "differences (PPD), linear polarizations (HH, VV, HV, RR \n",
            "and RL), classification and regression tree analysis  \n",
            "6. Identification of \n",
            "seeds and \n",
            "reorganization of \n",
            "species  Remote sensing in \n",
            "general, cameras and \n",
            "photo -detectors, \n",
            "hyperspectral imaging  Principal component analysis, feature extraction,  linear \n",
            "regression analysis  \n",
            "7. Soil and leaf \n",
            "nitrogen content \n",
            "and treatment, \n",
            "salinity detection  Hyperspectral and multi -\n",
            "spectral imaging, thermal \n",
            "imaging  Linear and exponential regression analysis  \n",
            " \n",
            "8. Irrigation  \n",
            " Satellite remote sensing \n",
            "(hyperspectral and multi -\n",
            "spectral imaging), red -\n",
            "edge camera, thermal \n",
            "infrared  Image classification techniques (unsupervised \n",
            "clustering, density slicing with thresholds), decision \n",
            "trees, linear regression analysis, NDVI  \n",
            "9. Plants water \n",
            "stress detection \n",
            "and drought \n",
            "conditions  Satellite remote sensing \n",
            "(hyperspectral and multi -\n",
            "spectral imaging, radar \n",
            "images), thermal \n",
            "imaging, NIR camera, \n",
            "red-edge camera  Fraunhofer Line Depth (FLD) principle,  linear regression \n",
            "analysis, NDVI  \n",
            "10. Water erosion \n",
            "assessment  Satellite remote sensing \n",
            "(optical and radar \n",
            "images), SAR, NIR \n",
            "camera  Interferometric SAR image processing,  linear and \n",
            "exponential regression analysis, contour tracing, linear \n",
            "polarizations (HH, VV)  38 \n",
            " 11. Pest detection \n",
            "and \n",
            "management  Hyperspectral and multi -\n",
            "spectral imaging, \n",
            "microwave remote \n",
            "sensing, thermal camera  Image processing using sample imagery, linear and \n",
            "exponential regression analysis, statistical analysis, \n",
            "CEM nonlinear signal processing, NDVI  \n",
            "12. Weed detection  Remote sensing in \n",
            "general, optical cameras \n",
            "and photo -detectors, \n",
            "hyperspectral and multi -\n",
            "spectral imaging  Pixel classification based on k -means clustering and \n",
            "Bayes classifier, feature extraction techniques with FFT \n",
            "and GLCM, wavelet -based classification a nd Gabor \n",
            "filtering, genetic algorithms, fuzzy techniques, artificial \n",
            "neural networks, erosion and dilation segmentation, \n",
            "logistic regression, edge detection, color detection, \n",
            "principal component analysis  \n",
            "13. Herbicide  Remote sensing in \n",
            "general, optical cameras \n",
            "and photo -detectors  \n",
            " Fuzzy techniques, discriminant analysis  \n",
            "14. Fruit grading  Optical cameras and \n",
            "photo -detectors, \n",
            "monochrome images \n",
            "with different \n",
            "illuminations  K-means clustering, image fusion, color histogram \n",
            "techniques, machine learning (esp.  SVM), Bayesian \n",
            "discriminant analysis, Bayes filtering, linear discriminant \n",
            "analysis  \n",
            "15. Packaged food \n",
            "and food \n",
            "products – \n",
            "identification of \n",
            "contaminants, \n",
            "diseases or \n",
            "defects, bruise \n",
            "detection  X-ray imaging (or \n",
            "transmitted light), CCD \n",
            "cameras, monochrome \n",
            "images with different \n",
            "illuminations, thermal \n",
            "cameras, multi -spectral \n",
            "and hyperspectral NIR -\n",
            "based imaging  3D vision, invariance, pattern recognition and image \n",
            "modality,  multivariate image analysis with principal \n",
            "component analysis, K -mean clustering, SVM, linear \n",
            "discriminant analysis, classification trees, K -nearest \n",
            "neighbors, decision trees, fusion, feature extraction \n",
            "techniques with FFT, standard Bayesian discriminant \n",
            "analysis, feature analysis, color, shape and geometric \n",
            "features using discrimination ana lysis, pulsed -phase \n",
            "thermography  \n",
            "16. Crop hail \n",
            "damage  Multi -spectral imaging, \n",
            "polarimetric radar \n",
            "imagery  Linear and exponential regression analysis, \n",
            "unsupervised image classification  \n",
            "17. Agricultural \n",
            "expansion and \n",
            "intensification  Satellite remote sensing  \n",
            "in general  Wavelet -based filtering  \n",
            "18. Greenhouse \n",
            "monitoring  Optical and thermal \n",
            "cameras  Linear and exponential regression analysis, \n",
            "unsupervised classification, NDVI, IR thermography  39 \n",
            " Appendix  II: Applications of deep learning in agriculture . \n",
            "No. Agri \n",
            "Area Problem \n",
            "Description  Data Used  Classes and \n",
            "Labels  Variation \n",
            "among \n",
            "Classes  DL \n",
            "Model \n",
            "Used  FW \n",
            "Used  Data Pre -\n",
            "Processing  Data \n",
            "augmenta\n",
            "tion Data for \n",
            "Training \n",
            "vs. \n",
            "Testing  Perfor\n",
            "mance \n",
            "Metric \n",
            "Used  Value of \n",
            "Metric \n",
            "Used  Comparison \n",
            "with other \n",
            "technique  Ref. \n",
            "1. \n",
            "Leaf classification  Classify \n",
            "leaves of \n",
            "different \n",
            "plant species  Flavia dataset, \n",
            "consisting of 1,907 \n",
            "leaf images of 32 \n",
            "species with at \n",
            "least 50 images \n",
            "per species and at \n",
            "most 77 images . 32 classes: 32 \n",
            "Different plant \n",
            "species  N/A Author -\n",
            "defined \n",
            "CNN + \n",
            "RF \n",
            "classifier  Caffe  Feature \n",
            "extraction \n",
            "based on \n",
            "Histograms of \n",
            "Curvature \n",
            "over Scale \n",
            "(HoCS), \n",
            "shape and \n",
            "statistical \n",
            "features , use \n",
            "of normalized \n",
            "excessive \n",
            "green (NExG) \n",
            "vegetative \n",
            "index, white \n",
            "border \n",
            "doubling \n",
            "image size, \n",
            "segmentation  N/A Same.  \n",
            "(condition \n",
            "variations \n",
            "applied in \n",
            "testing: \n",
            "translations, \n",
            "scaling, \n",
            "rotations, \n",
            "shading and \n",
            "occlusions ) CA 97.3%  \n",
            "±0.6%  Feature \n",
            "extraction \n",
            "(shape and \n",
            "statistical \n",
            "features) and \n",
            "RF classifier \n",
            "(91.2% ± \n",
            "1.6%)  (Hall, \n",
            "McCool, \n",
            "Dayoub, \n",
            "Sunderha\n",
            "uf, & \n",
            "Upcroft, \n",
            "2015)  \n",
            "2. \n",
            "Leaf disease detection  \n",
            "13 different \n",
            "types of \n",
            "plant \n",
            "diseases out \n",
            "of healthy \n",
            "leaves  Authors -created \n",
            "database \n",
            "containing 4,483 \n",
            "images . 15 classes: \n",
            "Plant diseases \n",
            "(13), healthy \n",
            "leaves (1) and \n",
            "background \n",
            "images (1)  N/A  CaffeNet \n",
            "CNN  Caffe  Cropping, \n",
            "square around \n",
            "the leaves to  \n",
            "highlight \n",
            "region of \n",
            "interest , \n",
            "resized to \n",
            "256× 256 pix, \n",
            "dupl. image \n",
            "removal  \n",
            " Affine \n",
            "transform  \n",
            "(translation,  \n",
            "rotation ), \n",
            "perspective \n",
            "transform , \n",
            "and image \n",
            "rotations.  Same  CA 96.30%  Better results \n",
            "than SVM (no \n",
            "more details)  (Sladojevi\n",
            "c, \n",
            "Arsenovic\n",
            ", Anderla, \n",
            "Culibrk, & \n",
            "Stefanovi\n",
            "c, 2016)  40 \n",
            " 3. \n",
            "Plant disease detection  \n",
            " Identify 14 \n",
            "crop species \n",
            "and 26 \n",
            "diseases  PlantVillage public \n",
            "dataset of 54,306 \n",
            "images of \n",
            "diseased and \n",
            "healthy plant \n",
            "leaves collected \n",
            "under controlled \n",
            "conditions . 38 class labels \n",
            "as crop - \n",
            "disease pairs  N/A AlexNet, \n",
            "GoogleN\n",
            "et CNNs  Caffe  Resized to \n",
            "256×256 pix., \n",
            "segmentation , \n",
            "background  \n",
            "Information \n",
            "removal , fixed \n",
            "color casts  N/A Same. Also \n",
            "tested on a  \n",
            "dataset of  \n",
            "download ed \n",
            "images \n",
            "from Bing \n",
            "Image \n",
            "Search and \n",
            "IPM Images  F1 0.9935  Substantial  \n",
            "margin in \n",
            "standard \n",
            "benchmarks \n",
            "with \n",
            "approaches \n",
            "using hand -\n",
            "engineered \n",
            "features  (Mohanty, \n",
            "Hughes, \n",
            "& \n",
            "Salathé, \n",
            "2016)  \n",
            "4. Classify \n",
            "banana \n",
            "leaves ’ \n",
            "diseases  Dataset of 3,700 \n",
            "images of banana \n",
            "diseases obtained \n",
            "from the \n",
            "PlantVillage \n",
            "dataset . 3 classes: \n",
            "healthy,  \n",
            "black sigatoka \n",
            "and black \n",
            "speckle  N/A LeNet  \n",
            "CNN  deeplear\n",
            "ning4j  Resized to \n",
            "60x60 pix. , \n",
            "converted to \n",
            "grayscale  N/A Same  CA, F1  96+% \n",
            "(CA), \n",
            "0.968 \n",
            "(F1) Methods \n",
            "using hand -\n",
            "crafted \n",
            "features not \n",
            "generalize \n",
            "well (Amara, \n",
            "Bouaziz, \n",
            "& \n",
            "Algergaw\n",
            "y, 2017)  \n",
            "5. \n",
            "Land cover classification  Identify 13 \n",
            "different \n",
            "land-cover \n",
            "classes in \n",
            "KSC and 9  \n",
            "different  \n",
            "classes  in \n",
            "Pavia  A mixed \n",
            "vegetation site \n",
            "over Kennedy \n",
            "Space Center \n",
            "(KSC), FL, USA \n",
            "(Dataset 1), and \n",
            "an urban site over \n",
            "the city of Pavia, \n",
            "Italy (Dataset 2). \n",
            "Hyperspectral \n",
            "datasets . 13 different  \n",
            "land-cover \n",
            "classes \n",
            "(Dataset 1), 9 \n",
            "land cover \n",
            "classes trees \n",
            "(Dataset 2): \n",
            "Soil, meadow, \n",
            "water, \n",
            "shadows, \n",
            "different \n",
            "materials  N/A Hybrid of \n",
            "PCA, \n",
            "autoenc\n",
            "oder \n",
            "(AE), \n",
            "and \n",
            "logistic \n",
            "regressi\n",
            "on Develop\n",
            "ed by \n",
            "the \n",
            "authors  Some bands \n",
            "removed due \n",
            "to noise  N/A Same  CA 98.70%  1% more \n",
            "precise than \n",
            "RBF-SVM  (Chen, \n",
            "Lin, Zhao, \n",
            "Wang, & \n",
            "Gu, 2014)  \n",
            "6. Identify 21 \n",
            "land-use \n",
            "classes \n",
            "containing a \n",
            "variety of \n",
            "spatial \n",
            "patterns  UC Merced land -\n",
            "use data set. \n",
            "Aerial ortho -\n",
            "imagery with a \n",
            "0.3048 -m pixel \n",
            "resolution. Dataset \n",
            "compiled from a  \n",
            "selection of 100 \n",
            "images/ class . 21 land -use \n",
            "classes: \n",
            "Agricultural, \n",
            "airplane, \n",
            "sports, beach, \n",
            "buildings, \n",
            "residential, \n",
            "forest, freeway, \n",
            "harbor, parking \n",
            "lot, river etc.  High relevance \n",
            "between \n",
            "medium density \n",
            "and dense \n",
            "resident ial, as \n",
            "well as between \n",
            "buildings and \n",
            "storage tanks  Author -\n",
            "defined \n",
            "CNN  + \n",
            "multiview \n",
            "model \n",
            "averaging  Theano  From  RGB to \n",
            "HSV (hu e-\n",
            "saturation -\n",
            "value) color \n",
            "model, \n",
            "resized to \n",
            "96×96 pix., \n",
            "creation of \n",
            "multiscale \n",
            "views  Views \n",
            "flipped \n",
            "horizontally \n",
            "or vertically \n",
            "with a \n",
            "probability \n",
            "of 0.5  Same  CA 93.48%  Unsupervised \n",
            "feature \n",
            "learning \n",
            "(UFL): 82-\n",
            "90%  \n",
            "SIFT : 85%  (Luus, \n",
            "Salmon, \n",
            "van den \n",
            "Bergh, & \n",
            "Maharaj, \n",
            "2015)  41 \n",
            " 7. Extract \n",
            "information \n",
            "about \n",
            "cultivated \n",
            "land Images from UAV \n",
            "at the areas \n",
            "Pengzhou County \n",
            "and Guanghan \n",
            "County, Sichuan \n",
            "Province, China.  2 classes: \n",
            "Cultivated vs. \n",
            "non-cultivated  The cultivated \n",
            "land samples \n",
            "and part of  \n",
            "forest land \n",
            "samples were \n",
            "easily confused  Author -\n",
            "defined \n",
            "CNN  N/A Orthorectificati\n",
            "on, image \n",
            "matching, \n",
            "linear land \n",
            "elimination , \n",
            "correct \n",
            "distortion, \n",
            "zoomed to \n",
            "40×40 pix.  N/A Same  CA 88-91% N/A (Lu, et al., \n",
            "2017)  \n",
            "8. Land cover \n",
            "classification \n",
            "considering \n",
            "time series  First dataset \n",
            "generated using a \n",
            "time series of \n",
            "Pléiades VHSR \n",
            "images at THAU \n",
            "Basin. Second \n",
            "dataset generated \n",
            "from an annual \n",
            "time series of 23 \n",
            "Landsat 8 images \n",
            "acquired in 2014 \n",
            "above Reunion \n",
            "Island . 11 classes \n",
            "(dataset 1),  \n",
            "9 classes \n",
            "(dataset 2) . \n",
            "Land cover \n",
            "classes such \n",
            "as trees, crops , \n",
            "forests, water , \n",
            "soils, urban \n",
            "areas , \n",
            "grasslands, \n",
            "etc. (Image \n",
            "object or pixel)  Tree Crops, \n",
            "Summer crops \n",
            "and Truck \n",
            "Farming  were \n",
            "classes highly \n",
            "mixed  One-unit \n",
            "LSTM + \n",
            "RFF,  \n",
            "One-unit \n",
            "LSTM + \n",
            "SVM  Keras/  \n",
            "Theano  Multiresolution \n",
            "segmentation \n",
            "technique, \n",
            "feature \n",
            "extraction, \n",
            "pixel-wise \n",
            "multi -temporal \n",
            "linea r \n",
            "interpolation, \n",
            "various \n",
            "radiometric \n",
            "indices \n",
            "calculated  N/A Same  CA, F1 First \n",
            "Dataset: \n",
            "75.34% \n",
            "(CA), \n",
            "0.7463  \n",
            "(F1) \n",
            "Second \n",
            "Dataset: \n",
            "84.61% \n",
            "(CA), \n",
            "0.8441  \n",
            "(F1) RF and  SVM  \n",
            "(best of both ): \n",
            "First Dataset: \n",
            "74.20% ( CA), \n",
            "0.7158 (F1)  \n",
            "Second \n",
            "Dataset: \n",
            "83.82% (CA), \n",
            "0.8274 (F1)  (Ienco, \n",
            "Gaetano, \n",
            "Dupaquie\n",
            "r, & \n",
            "Maurel, \n",
            "2017)  \n",
            "9. \n",
            "Crop type classification  Classificatio\n",
            "n of crops \n",
            "wheat, \n",
            "maize, \n",
            "soybeans \n",
            "sunflower  \n",
            "and sugar \n",
            "beet 19 multi -temporal \n",
            "scenes acquired \n",
            "by Landsat -8 and \n",
            "Sentinel -1A RS \n",
            "satellites  from a \n",
            "test site in \n",
            "Ukraine . 11 classes: \n",
            "water, forest, \n",
            "grassland,  \n",
            "bare land, \n",
            "wheat, maize, \n",
            "rapeseed, \n",
            "cereals, sugar \n",
            "beet, \n",
            "sunflowers  and \n",
            "soybeans.  General \n",
            "confusion \n",
            "between maize \n",
            "and soybeans  Author -\n",
            "defined \n",
            "CNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  Calibration, \n",
            "multi -looking , \n",
            "speckle \n",
            "filtering (3× 3 \n",
            "window with \n",
            "Refined Lee \n",
            "algorithm), \n",
            "terrain \n",
            "correction, \n",
            "segmentation , \n",
            "restoration of \n",
            "missing data  N/A Same  CA 94.60%  Multilayer \n",
            "perceptron : \n",
            "92.7%,  \n",
            "RF: 88% (Kussul, \n",
            "Lavreniuk\n",
            ", Skakun, \n",
            "& \n",
            "Shelestov\n",
            ", 2017)  42 \n",
            " 10. Classificatio\n",
            "n of crops oil \n",
            "radish, \n",
            "barley, \n",
            "seeded \n",
            "grass, weed \n",
            "and stump  36 plots at Foulum \n",
            "Research Center, \n",
            "Denmark \n",
            "containing oil \n",
            "radish as a catch \n",
            "crop and amounts \n",
            "of barley, grass, \n",
            "weed and stump. \n",
            "352 patches in \n",
            "total.  7 classes: oil \n",
            "radish, barley, \n",
            "weed,  stump, \n",
            "soil, equipment \n",
            "and unknown  \n",
            "(pixel of the \n",
            "image)  Coarse features \n",
            "(radish leafs and \n",
            "soil) were \n",
            "predicted quite \n",
            "well. F iner \n",
            "features (barley, \n",
            "grass or stump ) \n",
            "not so much.  Adapted \n",
            "version \n",
            "of \n",
            "VGG16 \n",
            "CNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  Resized to \n",
            "1600x1600 \n",
            "pix. centered \n",
            "on the sample \n",
            "areas, division \n",
            "into 400x400  \n",
            "pix. patches  Rotations  0, \n",
            "90, 180 and \n",
            "270 \n",
            "degrees , \n",
            "flipped \n",
            "diagonally \n",
            "and same \n",
            "set of \n",
            "rotations  Same  CA, IoU  79% \n",
            "(CA), \n",
            "0.66 (IoU)  N/A (Mortens\n",
            "en, \n",
            "Dyrmann, \n",
            "Karstoft, \n",
            "Jørgense\n",
            "n, & \n",
            "Gislum, \n",
            "2016)  \n",
            "11. Crop type \n",
            "classification \n",
            "considering \n",
            "time series  A raster dataset of \n",
            "26 SENTINEL 2A \n",
            "images, acquired \n",
            "between 2015  \n",
            "2016 at Munich \n",
            "Germany. \n",
            "Shortwave infrared \n",
            "1 and 2 bands \n",
            "were selected.  19 classes : \n",
            "corn, meadow, \n",
            "asparagus, \n",
            "rape, hop, \n",
            "summer oats, \n",
            "winter spelt, \n",
            "fallow, wheat, \n",
            "barley, winter \n",
            "rye, beans and \n",
            "others  Some classes \n",
            "represent \n",
            "distinct \n",
            "cultivated crop s, \n",
            "other s (such as \n",
            "meadow, fallow , \n",
            "triticale, wheat, \n",
            "and rye ) are \n",
            "botanically \n",
            "related . Three -\n",
            "unit \n",
            "LSTM TensorFl\n",
            "ow Atmospherical\n",
            "ly corrected  N/A Same  CA, F1  76.2% \n",
            "(CA), \n",
            "0.558  \n",
            "(F1) CNN : 59.9% \n",
            "(CA), 0.236 \n",
            "(F1) \n",
            "SVM : 31.7 \n",
            "(CA), 84.8% \n",
            "0.317 (F1) (Rußwur\n",
            "m & \n",
            "Körner, \n",
            "2017)  \n",
            "12. Crop type \n",
            "classification \n",
            "from UAV \n",
            "imagery  Aerial images  of \n",
            "experimental farm \n",
            "fields issued from \n",
            "a series of \n",
            "experiments \n",
            "conducted by the \n",
            "Swiss \n",
            "Confederation’s \n",
            "Agroscope \n",
            "research center . 23 classes: 22 \n",
            "different crops \n",
            "plus soil  (pixel \n",
            "of the image)  Lin and Simplex \n",
            "have very \n",
            "similar \n",
            "histograms  CNN + \n",
            "HistNN \n",
            "(usin g \n",
            "RGB \n",
            "histogram\n",
            "s) Keras  Image \n",
            "segmentation  N/A Same  F1 0.90 \n",
            "(experime\n",
            "nt 0), \n",
            "0.73 \n",
            "(experime\n",
            "nt 1) CNN: 0.83 \n",
            "(experiment \n",
            "0), 0.70 \n",
            "(experiment 1)  \n",
            "HistNN : 0.86 \n",
            "(experiment \n",
            "0), 0.71 \n",
            "(experiment 1)  (Rebetez, \n",
            "J., et al., \n",
            "2016)  \n",
            "13.. \n",
            "Plant r ecognition  \n",
            " Recognize 7 \n",
            "views of \n",
            "different \n",
            "plants: entire \n",
            "plant, \n",
            "branch, \n",
            "flower, fruit, LifeCLEF 2015 \n",
            "plant dataset, \n",
            "which has 91,759 \n",
            "images distributed \n",
            "in 13,887 plant \n",
            "observations. \n",
            "Each observation \n",
            "captures the 1,000 classes: \n",
            "Species that \n",
            "include trees, \n",
            "herbs, and \n",
            "ferns, among \n",
            "others.  Images of \n",
            "flowers and leaf \n",
            "scans offer \n",
            "higher accuracy \n",
            "than the rest of \n",
            "the views  AlexNet \n",
            "CNN  Caffe  N/A N/A Same  LC 48.60%  20% worse \n",
            "than local \n",
            "descriptors to \n",
            "represent \n",
            "images and \n",
            "KNN, dense \n",
            "SIFT and a \n",
            "Gaussian (Reyes, \n",
            "Caicedo, \n",
            "& \n",
            "Camargo, \n",
            "2015)  43 \n",
            " leaf, stem \n",
            "and scans  appearance of t he \n",
            "plant from various \n",
            "points of view: \n",
            "entire plant, leaf \n",
            "branch, fruit, stem \n",
            "scan, flower.  Mixture Model  \n",
            "14. Root and \n",
            "shoot feature \n",
            "identification \n",
            "and \n",
            "localisation  The first dataset \n",
            "contains  2,500 \n",
            "annotated images \n",
            "of whole root \n",
            "systems. The \n",
            "second  hand -\n",
            "annotated 1 ,664 \n",
            "images of wheat \n",
            "plants, labeling \n",
            "leaf tips, leaf \n",
            "bases, ear tips, \n",
            "and ear bases . 2 classes:  \n",
            "Prediction if a \n",
            "root tip is \n",
            "present or not \n",
            "(first dataset)  \n",
            "5 classes:  Leaf \n",
            "tips and bases, \n",
            "ear tips and \n",
            "bases, and \n",
            "negative  \n",
            "(second \n",
            "dataset)  N/A Author -\n",
            "defined \n",
            "CNN  Caffe  Image \n",
            "cropping at \n",
            "annotated \n",
            "locations \n",
            "128x128 pix., \n",
            "resized to \n",
            "64x64 for use \n",
            "in the network  N/A Same  CA 98.4% \n",
            "(first \n",
            "dataset)\n",
            "97.3%  \n",
            "(second \n",
            "dataset)  Sparse coding \n",
            "approach \n",
            "using SIFT  + \n",
            "SVM: 80 -90% (Pound, \n",
            "M. P., et \n",
            "al., 2016)  \n",
            "15. Recognize \n",
            "44 different \n",
            "plant species  MalayaKew (MK) \n",
            "Leaf Dataset \n",
            "which consists of \n",
            "44 classes, \n",
            "collected at the \n",
            "Royal Botanic \n",
            "Gardens, Kew, \n",
            "England . 44 classes: \n",
            "Species such \n",
            "as acutissima, \n",
            "macranthera,  \n",
            "rubra,  robur f. \n",
            "purpurascens \n",
            "etc. N/A AlexNet \n",
            "CNN  Caffe  Foreground \n",
            "pixels \n",
            "extracted \n",
            "using HSV \n",
            "color space ,  \n",
            "image \n",
            "cropping \n",
            "within leaf  \n",
            "area Rotation  in \n",
            "7 different \n",
            "orientations  Same  CA 99.60%  SVM : 95.1%, \n",
            "ANN : 58% (Lee, \n",
            "Chan, \n",
            "Wilkin, & \n",
            "Remagni\n",
            "no, 2015)  \n",
            "16. Identify \n",
            "plants  from \n",
            "leaf vein \n",
            "patterns of \n",
            "white , soya \n",
            "and red \n",
            "beans  866 leaf images \n",
            "provided by INTA \n",
            "Argentina. Dataset \n",
            "divided into three \n",
            "classes: 422 \n",
            "images \n",
            "correspond to \n",
            "soybean leaves, \n",
            "272 to red bean \n",
            "leaves and 172 to \n",
            "white bean leaves . 3 classes: \n",
            "Legume \n",
            "species white \n",
            "bean,  \n",
            "red bean and \n",
            "soybean  At soybean, \n",
            "informative \n",
            "regions are in \n",
            "the central vein. \n",
            "For white and \n",
            "red bean, outer \n",
            "and smaller \n",
            "veins are also \n",
            "relevant.  Author -\n",
            "defined \n",
            "CNN  Pylearn2  Vein \n",
            "segmentation, \n",
            "central patch \n",
            "extraction  N/A Same  CA 96.90%  Penali zed \n",
            "Discriminant \n",
            "Analysis \n",
            "(PDA): 95.1%   \n",
            "SVM and RF \n",
            "slightly worse  (Grinblat, \n",
            "Uzal, \n",
            "Larese, & \n",
            "Granitto, \n",
            "2016)  44 \n",
            " 17. \n",
            "Plant phenology r ecognition  Classify \n",
            "phenological  \n",
            "stages of \n",
            "several \n",
            "types of \n",
            "plants purely \n",
            "based on the \n",
            "visual data  Datase t collected \n",
            "through TARBIL \n",
            "Agro -informatics \n",
            "Research Center \n",
            "of ITU, for which \n",
            "over a thousand \n",
            "agrostations are \n",
            "placed throughout \n",
            "Turkey. Different \n",
            "images of  various \n",
            "plants , at different \n",
            "phenological \n",
            "stages . 9 classes: \n",
            "Different \n",
            "growth stages \n",
            "of plants, \n",
            "starting from \n",
            "plowing to \n",
            "cropping,  for \n",
            "the plants \n",
            "wheat, barley, \n",
            "lentil, cotton, \n",
            "pepper and \n",
            "corn. (image \n",
            "segment)  Appearances \n",
            "change very \n",
            "gradually and it \n",
            "is challenging to \n",
            "distinguish \n",
            "images fall ing \n",
            "into the growing \n",
            "durations that \n",
            "are in the mi ddle \n",
            "of two \n",
            "successive \n",
            "stages. Some \n",
            "plants from \n",
            "different classes \n",
            "have  \n",
            "similar color and \n",
            "texture \n",
            "distributions  AlexNet \n",
            "CNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  Image \n",
            "segmentation  Images  are \n",
            "divided into \n",
            "large \n",
            "patches and \n",
            "features ar e \n",
            "extracted \n",
            "for each \n",
            "patch. \n",
            "227x 227 \n",
            "pix. patches \n",
            "are carved \n",
            "from the \n",
            "original \n",
            "images  Same  CA, F1  73.76 – \n",
            "87.14 \n",
            "(CA), \n",
            "0.7417 – \n",
            "0.8728 \n",
            "(F1) Hand crafted \n",
            "feature \n",
            "descriptors \n",
            "(GLCM and \n",
            "HOG) through \n",
            "a Naïve -\n",
            "Bayes \n",
            "classifier: \n",
            "68.97 – 82.41 \n",
            "(CA), 0.6931 \n",
            "– 0.8226 (F1 ) (Yalcin, \n",
            "2017 )  \n",
            "18. Classify the \n",
            "phenotyping \n",
            "of \n",
            "Arabidopsis \n",
            "in four \n",
            "accessions  Dataset composed \n",
            "of sequences of \n",
            "images captured \n",
            "from the plants in \n",
            "different days \n",
            "while they grow, \n",
            "successive top -\n",
            "view images of \n",
            "different \n",
            "accessions of \n",
            "Arabidopsis \n",
            "thaliana . 4 classes: 4 \n",
            "different \n",
            "accessions of \n",
            "Arabidopsis : \n",
            "Genotype \n",
            "states SF -2, \n",
            "CVI, Landsberg \n",
            "(Ler) and \n",
            "Columbia (Col)  Plants change in \n",
            "size rapidly  \n",
            "during their \n",
            "growth, the \n",
            "decomposed \n",
            "images from the \n",
            "plant sequences \n",
            "are not \n",
            "sufficiently \n",
            "consistent  CNN+  \n",
            "LSTM  Keras/  \n",
            "Theano  Camera \n",
            "distortion \n",
            "removal, color \n",
            "correction, \n",
            "temporal \n",
            "matching, \n",
            "plant \n",
            "segmentation \n",
            "through the \n",
            "GrabCut \n",
            "algorithm  Image \n",
            "rotations  by \n",
            "90, 180 and \n",
            "270 \n",
            "degrees \n",
            "around its \n",
            "center  Same  CA 93% Hand crafted \n",
            "feature \n",
            "descriptors + \n",
            "LSTM : 68% \n",
            "CNN : 76.8%  (Namin, \n",
            "Esmaeilz\n",
            "adeh, \n",
            "Najafi, \n",
            "Brown, & \n",
            "Borevitz, \n",
            "2017)  \n",
            "19. \n",
            "Segmentation  \n",
            "of root and soil  \n",
            "Identify roots \n",
            "from soils  Soil images \n",
            "coming from X -ray \n",
            "tomography . 2 classes: Root \n",
            "or soil (pixel of \n",
            "the image)  Soil/root \n",
            "contrast is \n",
            "sometimes very \n",
            "low Author -\n",
            "defined \n",
            "CNN \n",
            "with \n",
            "SVM for \n",
            "classific\n",
            "ation  MatCon\n",
            "vNet Image \n",
            "segmentation  Simulat ed \n",
            "roots added \n",
            "to soil \n",
            "images  Same  QM 0.23 \n",
            "(simulati\n",
            "on)  \n",
            "0.57 \n",
            "(real \n",
            "roots)  N/A (Douarre, \n",
            "Schielein, \n",
            "Frindel, \n",
            "Gerth, & \n",
            "Roussea\n",
            "u, 2016)  45 \n",
            " 20. \n",
            "Crop yield estimation  Estimate \n",
            "corn yield of \n",
            "county level \n",
            "in U.S.  Corn yields from \n",
            "2001 to 2010 in \n",
            "Illinois U.S., \n",
            "downloaded from \n",
            "Climate Research \n",
            "Unit (CRU), plus \n",
            "MODIS Enhanced \n",
            "Vegetation Index . Crop yield \n",
            "index (scalar \n",
            "value)  N/A Author -\n",
            "defined \n",
            "CNN  Caffe  Enhanced \n",
            "Vegetation \n",
            "Index (EVI), \n",
            "hard threshold \n",
            "algorithm, \n",
            "Wavelet  \n",
            "transformation \n",
            "for detecting  \n",
            "crop \n",
            "phenology  N/A Same  RMSE   6.298  Support \n",
            "Vector \n",
            "Regression \n",
            "(SVR) : 8.204  (Kuwata \n",
            "& \n",
            "Shibasaki\n",
            ", 2015)  \n",
            "21. Mapping \n",
            "winter \n",
            "vegetation \n",
            "quality \n",
            "coverage  \n",
            "considering \n",
            "time series  Sentinel -1 dataset \n",
            "including 13 \n",
            "acquisitions in \n",
            "TOPS mode from \n",
            "October 2016 to  \n",
            "February 2017, \n",
            "with a temporal \n",
            "baseline of 12 \n",
            "days. Dual-\n",
            "polarization \n",
            "(VV+VH) data  in \n",
            "26 images . 5 classes: \n",
            "Estimations  of \n",
            "the qu ality of \n",
            "vegetative \n",
            "development \n",
            "as bare soi l, \n",
            "very low, low, \n",
            "average, high  “Low” class \n",
            "intersects the \n",
            "temporal profiles \n",
            "of all the other \n",
            "classes multiple \n",
            "times. A \n",
            "misclassifi cation \n",
            "rate exists \n",
            "between the \n",
            "“low” and “b are \n",
            "soil” classes  Five-unit \n",
            "LSTM, \n",
            "Gated \n",
            "Recurre\n",
            "nt Unit \n",
            "(GRU)  Keras/  \n",
            "Theano  Intensity \n",
            "image gen. , \n",
            "radiometrical \n",
            "calibration, \n",
            "temporal \n",
            "filtering for \n",
            "noise \n",
            "reduction, \n",
            "orthorectificati\n",
            "on into map \n",
            "coordinates, \n",
            "transformed to \n",
            "logarithm \n",
            "scale, \n",
            "normalized  N/A Same  CA, F1  99.05% \n",
            "(CA), \n",
            "0.99 (F1)  RF and SVM  \n",
            "(best of both) : \n",
            "91.77% ( CA), \n",
            "0.9179 (F1)  (Minh, et \n",
            "al., 2017)  \n",
            "22. \n",
            "Fruit counting  Predict \n",
            "number of \n",
            "tomatoes  in \n",
            "the images  24,000 synthetic \n",
            "images produced \n",
            "by the authors . Estimated \n",
            "number of \n",
            "tomato fruits \n",
            "(scalar value)  N/A Modified \n",
            "Inceptio\n",
            "n-\n",
            "ResNet \n",
            "CNN  TensorFl\n",
            "ow Blurred \n",
            "synthetic \n",
            "images by a \n",
            "Gaussian filter  Generated \n",
            "synthetic \n",
            "128x128 \n",
            "pix. images \n",
            "to train the \n",
            "network, \n",
            "colored \n",
            "circles to \n",
            "simulate \n",
            "background \n",
            "and tomato \n",
            "plant /crops . \n",
            " Trained \n",
            "entirely on \n",
            "synthetic \n",
            "data and \n",
            "tested on \n",
            "real data RFC, \n",
            "RMSE  91% \n",
            "(RFC)  \n",
            "1.16 \n",
            "(RMSE) \n",
            "on real \n",
            "images , \n",
            "93% \n",
            "(RFC)   \n",
            "2.52 \n",
            "(RMSE ) \n",
            "on \n",
            "synthetic \n",
            "images   ABT: 66.16%  \n",
            "(RFC),  13.56  \n",
            "(RMSE)  (Rahnem\n",
            "oonfar & \n",
            "Sheppard\n",
            ", 2017)  46 \n",
            " 23. Map from \n",
            "input images \n",
            "of apples \n",
            "and oranges \n",
            "to total fruit \n",
            "counts  71 1280×960 \n",
            "orange  images \n",
            "(day time) and 21 \n",
            "1920×1200 apple \n",
            "images (night \n",
            "time) . Number of \n",
            "orange or \n",
            "apple fruits \n",
            "(scalar value)  High variation in \n",
            "CA. For orange, \n",
            "dataset has high \n",
            "occlusion, depth \n",
            "variation, and \n",
            "uncontrolled \n",
            "illumination. For \n",
            "apples, data set \n",
            "has high color  \n",
            "similarity \n",
            "between \n",
            "fruit/foliage  CNN \n",
            "(blob \n",
            "detection \n",
            "and \n",
            "counting) \n",
            "+ Linear \n",
            "Regre ssi\n",
            "on Caffe  Image \n",
            "segmentation \n",
            "for easier data \n",
            "annotation by \n",
            "users, \n",
            "creation of \n",
            "bounding \n",
            "boxes around \n",
            "image blobs  Training set \n",
            "partitioned \n",
            "into 100 \n",
            "randomly \n",
            "cropped \n",
            "and flipped \n",
            "320× 240 \n",
            "pix. sub-\n",
            "images  Same ( but \n",
            "different \n",
            "trees  used \n",
            "in training \n",
            "and testing)  RFC, \n",
            "L2  \n",
            "0.968 \n",
            "(RFC), \n",
            "13.8 (L2) \n",
            "for \n",
            "oranges \n",
            "0.913  \n",
            "(RFC), \n",
            "10.5 (L2)  \n",
            "for apples  Best texture -\n",
            "based \n",
            "regression \n",
            "model : 0.682  \n",
            "(RFC) (Chen, et \n",
            "al., 2017)  \n",
            "24. Fruit \n",
            "detection  \n",
            "in orchards, \n",
            "including \n",
            "mangoes, \n",
            "almonds and \n",
            "apples  Images of three \n",
            "fruit varieties: \n",
            "apples (726), \n",
            "almonds (385) and \n",
            "mangoes (1,154), \n",
            "captured at \n",
            "orchards in \n",
            "Victoria and \n",
            "Queensland, \n",
            "Australia.  Sections of \n",
            "apples, \n",
            "almonds and \n",
            "mangoes at the \n",
            "image \n",
            "(bounding box)  Within class \n",
            "variations due to \n",
            "distance to fruit \n",
            "illumination, fruit \n",
            "clustering, and \n",
            "camera view -\n",
            "point. Almonds  \n",
            "similar in color \n",
            "and texture to \n",
            "the foliage  Faster \n",
            "Region -\n",
            "based \n",
            "CNN \n",
            "with \n",
            "VGG16 \n",
            "model  Caffe  Image \n",
            "segmentation \n",
            "for easier data \n",
            "annotation  Flip, scale, \n",
            "flip-scale \n",
            "and the \n",
            "PCA \n",
            "augmentati\n",
            "on \n",
            "technique \n",
            "presented \n",
            "in AlexNet  Same  F1-IoU \n",
            "(20) 0.904  \n",
            "(apples ) \n",
            "0.908  \n",
            "(mango)  \n",
            "0.775  \n",
            "(almonds )  ZF network : \n",
            "0.892  \n",
            "(apples)  \n",
            "0.876  \n",
            "(mango)  \n",
            "0.726  \n",
            "(almonds)  (Bargoti & \n",
            "Underwo\n",
            "od, 2016)  \n",
            "25. Detection of \n",
            "sweet \n",
            "pepper and \n",
            "rock melon \n",
            "fruits  122 images \n",
            "obtained from two \n",
            "modalities: color \n",
            "(RGB) and Near -\n",
            "Infrared (NIR) . Sections of \n",
            "sweet red \n",
            "peppers and \n",
            "rock melons on \n",
            "the image \n",
            "(bounding box)  Variations to \n",
            "camera setup, \n",
            "time and \n",
            "locations of data \n",
            "acquisition. \n",
            "Time for data \n",
            "collection is day \n",
            "and night, sites \n",
            "are different . \n",
            "Varied fruit \n",
            "ripeness.  Faster \n",
            "Region -\n",
            "based \n",
            "CNN \n",
            "with \n",
            "VGG16 \n",
            "model  Caffe  Early/ late \n",
            "fusion \n",
            "techniques for \n",
            "combining the \n",
            "classification \n",
            "info from color \n",
            "and NIR \n",
            "imagery, \n",
            "bounding box \n",
            "segmentation, \n",
            "pairwise IoU N/A Same \n",
            "(authors \n",
            "demonstrat\n",
            "e by using a \n",
            "small \n",
            "dataset that \n",
            "the model \n",
            "can \n",
            "generalize)  F1-IoU \n",
            "(40) 0.838  Conditional \n",
            "Random Field \n",
            "to model col or \n",
            "and visual  \n",
            "texture  \n",
            "featur es: \n",
            "0.807  (Sa, et \n",
            "al., 2016)  \n",
            "26. \n",
            "Obstacle \n",
            "detection  Identify ISO \n",
            "barrel -\n",
            "shaped \n",
            "obstacles in 437 images from \n",
            "authors' \n",
            "experiments and \n",
            "recordings , 1,925 \n",
            "positive and Identify if a \n",
            "barrel -shaped \n",
            "object is \n",
            "present in the \n",
            "image N/A AlexNet \n",
            "CNN  Caffe  Resized to \n",
            "114×114 pix. , \n",
            "bounding \n",
            "boxes of the \n",
            "object created  Various \n",
            "rotations at  \n",
            "13 scales, \n",
            "intensity of \n",
            "the object Testing in \n",
            "different \n",
            "fields (row \n",
            "crops,  grass \n",
            "mowing ), CA-IoU \n",
            "(50) 99.9% in \n",
            "row \n",
            "crops \n",
            "and \n",
            "90.8% in N/A (Steen, \n",
            "Christians\n",
            "en, \n",
            "Karstoft, \n",
            "& 47 \n",
            " row crops \n",
            "and grass \n",
            "mowing  11,550 negative \n",
            "samples . (bounding box)  adapted  containing \n",
            "other \n",
            "obstacles \n",
            "(people and \n",
            "animals ) grass \n",
            "mowing  Jørgense\n",
            "n, 2016)  \n",
            "27. Detect \n",
            "obstacles \n",
            "that are \n",
            "distant, \n",
            "heavily \n",
            "occluded \n",
            "and \n",
            "unknown  Background data \n",
            "of 48 images and \n",
            "test data of 48 \n",
            "images from \n",
            "annotations of \n",
            "humans, houses, \n",
            "barrels, wells and \n",
            "mannequins . Classify each \n",
            "pixel as either \n",
            "foreground \n",
            "(contains a \n",
            "human) or \n",
            "background \n",
            "(anomaly \n",
            "detection)  N/A AlexNet \n",
            "and \n",
            "VGG \n",
            "CNNs  Caffe  Image \n",
            "cropping, \n",
            "resized by a \n",
            "factor of 0.75  N/A Same  F1-IoU \n",
            "(50) 0.72 Local de -\n",
            "correlated \n",
            "channel \n",
            "features : \n",
            "0.113  (Christian\n",
            "sen, \n",
            "Nielsen, \n",
            "Steen, \n",
            "Jørgense\n",
            "n, & \n",
            "Karstoft, \n",
            "2016)  \n",
            "28. \n",
            "Identification of weeds  Classify 91 \n",
            "weed seed \n",
            "types  Dataset of 3,980 \n",
            "images containing \n",
            "91 types of weed \n",
            "seeds . 91 classes: \n",
            "Different \n",
            "common \n",
            "weeds found in \n",
            "agricultural \n",
            "fields  Similarity \n",
            "between some  \n",
            "classes is very \n",
            "high (only slight \n",
            "differences in \n",
            "shape, texture, \n",
            "and color)  PCANet \n",
            "+ LMC \n",
            "classifier\n",
            "s Develop\n",
            "ed by \n",
            "the \n",
            "authors  Image filter \n",
            "extraction \n",
            "through PCA \n",
            "filters bank, \n",
            "binarization \n",
            "and \n",
            "histograms ’ \n",
            "counting  N/A Same ( also \n",
            "scaling for a \n",
            "certain \n",
            "range \n",
            "translation \n",
            "distance \n",
            "and rotation \n",
            "angle  CA 90.96%  Manual \n",
            "feature \n",
            "extraction \n",
            "techniques + \n",
            "LMC \n",
            "classifiers: \n",
            "64.80%  (Xinshao \n",
            "& Cheng, \n",
            "2015)  \n",
            "29. Classify \n",
            "weed from \n",
            "crop species \n",
            "based on 22 \n",
            "different \n",
            "species in \n",
            "total.  Dataset of 10,413 \n",
            "images, taken \n",
            "mainly from BBCH \n",
            "12-16 containing \n",
            "22 weed and crop \n",
            "species at early \n",
            "growth stages . 22 classes: \n",
            "Different \n",
            "species of \n",
            "weeds and \n",
            "crops at early \n",
            "growth stages \n",
            "e.g. \n",
            "chamomile, \n",
            "knotweed, \n",
            "cranesbill, \n",
            "chickweed and \n",
            "veronica  Variations with \n",
            "respect to \n",
            "lighting, \n",
            "resolution, and \n",
            "soil type.  Some \n",
            "species \n",
            "(Veronica, Field \n",
            "Pancy) were \n",
            "very similar and \n",
            "difficult to \n",
            "classify  Variation \n",
            "of \n",
            "VGG16  Theano -\n",
            "based \n",
            "Lasagne  \n",
            "library \n",
            "for \n",
            "Python  Green \n",
            "segmentation \n",
            "to detect \n",
            "green pixels, \n",
            "non-green \n",
            "pixels \n",
            "removal,  \n",
            "padding \n",
            "added  to \n",
            "make images \n",
            "square , \n",
            "resized to \n",
            "128x128 pix.  Image  \n",
            "mirroring \n",
            "and rotation \n",
            "in 90 \n",
            "degree \n",
            "increments  Same  CA 86.2% Local shape \n",
            "and color  \n",
            "features : \n",
            "42.5% and \n",
            "12.2% \n",
            "respectively  (Dyrmann\n",
            ", Karstoft, \n",
            "& Midtiby, \n",
            "2016 )  \n",
            "30. Identify \n",
            "thistle in 4,500 images from \n",
            "10, 20, 30, and \n",
            "50m of altitude 2 classes: \n",
            "Whether the \n",
            "image contains Small variations \n",
            "in some images \n",
            "depending on DenseN\n",
            "et CNN  Caffe  Image \n",
            "cropping  Random flip  \n",
            "both \n",
            "horizontally Same (extra \n",
            "tests for the \n",
            "case of CA 97% Color feature -\n",
            "based  Thistle -\n",
            "Tool: 95% (Sørense\n",
            "n, \n",
            "Rasmuss48 \n",
            " winter wheat \n",
            "and spring \n",
            "barley \n",
            "images  captured by a \n",
            "Canon PowerShot \n",
            "G15 camera . thistle in winter \n",
            "wheat or not   \n",
            "(Heatmap of \n",
            "classes is \n",
            "generated at \n",
            "the output)  the percentage \n",
            "of thistles they \n",
            "contain  and \n",
            "vertically, \n",
            "random  \n",
            "transposin g winter \n",
            "barley ) en, \n",
            "Nielsen, \n",
            "& \n",
            "Jørgense\n",
            "n, 2017)  \n",
            "31. Weed \n",
            "segment atio\n",
            "n for robotic \n",
            "platforms  Crop/Weed Field \n",
            "Image Dataset \n",
            "(CW-FID), consists \n",
            "of 20 training and \n",
            "40 testing images. \n",
            "A dataset of  60 \n",
            "top-down field \n",
            "images of a \n",
            "common culture \n",
            "(organic carrots) \n",
            "with the presence \n",
            "of intra -row and \n",
            "close -to-crop \n",
            "weeds . 2 classes: \n",
            "carrot plants \n",
            "and weeds  \n",
            "(image region)  N/A Adapted \n",
            "version of \n",
            "Inception -\n",
            "v3 + \n",
            "lightweigh\n",
            "t DCNN + \n",
            "set of K \n",
            "lightweigh\n",
            "t models \n",
            "as a \n",
            "mixture \n",
            "model \n",
            "(MixDCN\n",
            "N) TensorFl\n",
            "ow Image up -\n",
            "sampling to \n",
            "299x299  pix., \n",
            "NDVI -based \n",
            "vegetation \n",
            "masks, \n",
            "extract ing \n",
            "regions based \n",
            "on a sliding \n",
            "window on the \n",
            "color image  N/A Same \n",
            "(different \n",
            "carrot fields \n",
            "used for \n",
            "testing)  CA 93.90% Feature \n",
            "extraction \n",
            "(shape and \n",
            "statistical \n",
            "features) and \n",
            "RF classifier : \n",
            "85.9%  (McCool, \n",
            "Perez, & \n",
            "Upcroft, \n",
            "2017)  \n",
            "32. Automating \n",
            "weed \n",
            "detection in \n",
            "color images \n",
            "despite \n",
            "heavy leaf \n",
            "occlusion  1,427 images from \n",
            "winter wheat \n",
            "fields, of which \n",
            "18,541 weeds \n",
            "have been \n",
            "annotated, \n",
            "collected using a \n",
            "camera mounted \n",
            "on an all -terrain \n",
            "vehicle.  Detect single \n",
            "weed instances \n",
            "in images of \n",
            "cereal fields  \n",
            "(bounding box) . \n",
            "A coverage \n",
            "map is \n",
            "produced . \n",
            " Large parts of \n",
            "the weeds \n",
            "overlap with \n",
            "wheat plants  Based on \n",
            "DetectNet \n",
            "CNN \n",
            "(which is \n",
            "based on \n",
            "GoogLeN\n",
            "et CNN ) Develop\n",
            "ed by \n",
            "the \n",
            "authors  Resized to \n",
            "1224×1024 \n",
            "pix. N/A Different \n",
            "field used \n",
            "for testing. \n",
            "This field \n",
            "has a \n",
            "severe \n",
            "degree of \n",
            "occlusion \n",
            "compared \n",
            "to the  \n",
            "others  IoU \n",
            "P- IoU \n",
            "(50)  \n",
            "R-IoU \n",
            "(50) 0.64  \n",
            "(IoU), \n",
            "86.6%  \n",
            "(P- IoU), \n",
            "46.3% \n",
            "(R-IoU)  N/A (Dyrmann\n",
            ", \n",
            "Jørgense\n",
            "n, & \n",
            "Midtiby, \n",
            "2017)  49 \n",
            " 33. \n",
            "Crop/weed detection and classification  Detecting \n",
            "sugar beet \n",
            "plants and  \n",
            "weeds in the \n",
            "field based \n",
            "on image \n",
            "data 1,969 RGB+NIR \n",
            "images captured \n",
            "using a JAI \n",
            "camera in nadir \n",
            "view placed on a \n",
            "UAV. Identify  if an \n",
            "image patch \n",
            "belongs to \n",
            "weed or sugar \n",
            "beet (i mage \n",
            "region)  N/A Author -\n",
            "defined \n",
            "CNN  TensorFl\n",
            "ow Separate d \n",
            "vegetation / \n",
            "background \n",
            "based on \n",
            "NDVI, binary \n",
            "mask to \n",
            "describe \n",
            "vegetation , \n",
            "blob \n",
            "segmentation, \n",
            "resized to \n",
            "64x64 pix.,  \n",
            "normalized \n",
            "and centered  64 even \n",
            "rotations  Same (also \n",
            "generalized \n",
            "to a second \n",
            "dataset \n",
            "produced  2-\n",
            "weeks after, \n",
            "at a more  \n",
            "advanced \n",
            "growth \n",
            "stage)  P, R Dataset \n",
            "A: 97% \n",
            "(P), 98% \n",
            "(R) \n",
            "Dataset \n",
            "B: 99% \n",
            "(P), 89% \n",
            "(R) N/A (Milioto, \n",
            "Lottes, & \n",
            "Stachniss\n",
            ", 2017)  \n",
            "34. Detecting \n",
            "and \n",
            "classifying \n",
            "sugar beet \n",
            "plants and \n",
            "weeds  1,600 4 -channels \n",
            "RGB+NIR images \n",
            "captured before \n",
            "(700 images ) and \n",
            "after (900  images ) \n",
            "a 4-week period, \n",
            "provided by a \n",
            "multispectral JAI \n",
            "camera mounted \n",
            "on a BOSCH \n",
            "Bonirob farm \n",
            "robot . Identifies if a \n",
            "blob belongs to \n",
            "sugar beet \n",
            "crop, weeds or \n",
            "soil (image \n",
            "blob)  N/A Author -\n",
            "defined \n",
            "CNN  TensorFl\n",
            "ow Pixel-wise \n",
            "segmentation \n",
            "between \n",
            "green \n",
            "vegetation \n",
            "and soi l based \n",
            "on NDVI and \n",
            "light CNN, \n",
            "unsupervised \n",
            "dataset \n",
            "summariz.  N/A Same (also \n",
            "generalized \n",
            "to a second \n",
            "dataset \n",
            "produced  4-\n",
            "weeks after, \n",
            "at a more \n",
            "advanced \n",
            "growth \n",
            "stage)  CA 98% \n",
            "(Dataset \n",
            "A), \n",
            "59.4%  \n",
            "(Dataset \n",
            "B) Feature \n",
            "extraction \n",
            "(shape and \n",
            "statistical \n",
            "features) and \n",
            "RF classifier : \n",
            "95% (Potena, \n",
            "Nardi, & \n",
            "Pretto, \n",
            "2016)  50 \n",
            " 35. Detecting \n",
            "and \n",
            "classifying \n",
            "weeds and \n",
            "maize in \n",
            "fields  Simulated top -\n",
            "down images of \n",
            "overlapping plants \n",
            "on soil background \n",
            "A total of 301 \n",
            "images of soil and \n",
            "8,430 images of \n",
            "segmented plant s. \n",
            "The plants cover \n",
            "23 different weed \n",
            "species and \n",
            "maize .  Identifies if an \n",
            "image patch \n",
            "belongs to \n",
            "weed, soil or \n",
            "maize crop \n",
            "(image pixel)  N/A Adapted \n",
            "version \n",
            "of \n",
            "VGG16 \n",
            "CNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  Image \n",
            "cropping  in \n",
            "800x800 pix.  Random \n",
            "scaling  from \n",
            "80 to 100% \n",
            "of original \n",
            "size, \n",
            "random \n",
            "rotations  in \n",
            "one degree \n",
            "increments, \n",
            "varied hue, \n",
            "saturation \n",
            "and \n",
            "intensity , \n",
            "random \n",
            "shadows  Tested on \n",
            "real images \n",
            "while \n",
            "trained on \n",
            "simulated \n",
            "ones  CA, \n",
            "IoU 94% CA, \n",
            "0.71 IoU \n",
            "(crops), \n",
            "0.70 IoU \n",
            "(weeds) \n",
            "0.93 IoU \n",
            "(soil)  N/A (Dyrmann\n",
            ", \n",
            "Mortense\n",
            "n, \n",
            "Midtiby, & \n",
            "Jørgense\n",
            "n, 2016)  \n",
            "36. \n",
            "Prediction of soil moisture content  \n",
            "Predict the \n",
            "soil moisture \n",
            "content  over \n",
            "an irrigated \n",
            "corn field  Soil data collected \n",
            "from an irrigated \n",
            "corn field (an area \n",
            "of 22 sq. km) in \n",
            "the Zhangye oasis, \n",
            "Northwest China.  Percentage of \n",
            "soil moisture \n",
            "content (SMC) \n",
            "(scalar value)  N/A Deep  \n",
            "belief \n",
            "network -\n",
            "based \n",
            "macrosc\n",
            "opic \n",
            "cellular \n",
            "automat\n",
            "a (DBN -\n",
            "MCA)  Develop\n",
            "ed by \n",
            "the \n",
            "authors  Geospatial \n",
            "interpolation \n",
            "for creation o f \n",
            "soil moisture \n",
            "content maps, \n",
            "multivariate \n",
            "geostatistical \n",
            "approach for \n",
            "estimating \n",
            "themati c soil \n",
            "maps, maps \n",
            "converted to  \n",
            "TIFF, \n",
            "resampled to \n",
            "10-m res.  N/A Same  RMSE  6.77 Multi -layer \n",
            "perceptron \n",
            "MCA (MLP -\n",
            "MCA) : 18% \n",
            "reduction  in \n",
            "RMSE  (Song, et \n",
            "al., 2016)  51 \n",
            " 37. \n",
            "Animal research  Practical and \n",
            "accurate \n",
            "cattle \n",
            "identification \n",
            "from 5 \n",
            "different \n",
            "races  1,300 images \n",
            "collected  by the \n",
            "authors . 5 classes: \n",
            "Cattle races, \n",
            "Bali \n",
            "Onggole or \n",
            "Pasuruan, \n",
            "Aceh  \n",
            "Madura and \n",
            "Pesisir  N/A  GLCM – \n",
            "CNN  Deep \n",
            "Learning \n",
            "Matlab \n",
            "Toolbox  GLCM  \n",
            "features  \n",
            "extraction  \n",
            "(contrast, \n",
            "energy and \n",
            "homogeneity), \n",
            "saliency maps \n",
            "to accelerate \n",
            "feature \n",
            "extraction  N/A Same  CA 93.76%  CNN  without \n",
            "extra inputs: \n",
            "89.68% \n",
            "Gaussian \n",
            "Mixture \n",
            "Model  \n",
            "(GMM): 90%  (Santoni, \n",
            "Sensuse, \n",
            "Arymurth\n",
            "y, & \n",
            "Fanany, \n",
            "2015)  \n",
            "38. Predict \n",
            "growth  of \n",
            "pigs 160 pigs, housed \n",
            "in two clim ate \n",
            "controlled rooms, \n",
            "four pens/room, 10 \n",
            "pigs/ pen. \n",
            "Ammonia, ambient \n",
            "and indoor air \n",
            "temperature and \n",
            "humidity, feed \n",
            "dosage and \n",
            "ventilation \n",
            "measured at 6 -\n",
            "minute intervals . Estimation of \n",
            "the weight of \n",
            "pigs (scalar \n",
            "value)  N/A First-\n",
            "order \n",
            "DRNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  N/A N/A Tested on \n",
            "different \n",
            "rooms of \n",
            "pigs than \n",
            "the ones \n",
            "which were \n",
            "used for \n",
            "training  MSE, \n",
            "MRE  0.002  \n",
            "(MSE ) on \n",
            "same \n",
            "dataset), \n",
            "10% \n",
            "(MRE ) in \n",
            "relation to \n",
            "a \n",
            "controller  N/A (Demmer\n",
            "s T. G., \n",
            "Cao, \n",
            "Parsons, \n",
            "Gauss, & \n",
            "Wathes, \n",
            "2012)  \n",
            "39. Control of \n",
            "the growth of \n",
            "broiler \n",
            "chickens  Collecting dat a \n",
            "from 8 rooms, \n",
            "each room \n",
            "housing 262 \n",
            "broilers, \n",
            "measuring  bird \n",
            "weight, feed \n",
            "amount, light \n",
            "intensity and \n",
            "relative humidity.  Estimation of  \n",
            "the weight of \n",
            "chicken  (scalar \n",
            "value)  N/A First-\n",
            "order \n",
            "DRNN  Develop\n",
            "ed by \n",
            "the \n",
            "authors  N/A N/A Tested on \n",
            "different \n",
            "rooms of \n",
            "chicken \n",
            "than the \n",
            "ones which \n",
            "were used \n",
            "for training  MSE, \n",
            "MRE  0.02 \n",
            "(MSE ), \n",
            "1.8%  \n",
            "(MRE )  in \n",
            "relation to \n",
            "a \n",
            "controller  N/A (Demmer\n",
            "s T. G., et \n",
            "al., 2010)  52 \n",
            " 40. \n",
            "Weather prediction  Predict \n",
            "weather  \n",
            "based on \n",
            "previous \n",
            "year’s \n",
            "conditions  Syngenta Crop \n",
            "Challenge 2016 \n",
            "dataset, containing \n",
            "6,490 sub -regions \n",
            "with three weather \n",
            "condition attributes \n",
            "from the years  \n",
            "2000 to 2015 . Predicted \n",
            "values of \n",
            "temperature, \n",
            "precipitation \n",
            "and solar \n",
            "radiation  \n",
            "(scalar value)  N/A LSTM  Keras  N/A N/A Same  N-\n",
            "RMSE , \n",
            "MRE  78% \n",
            "(Temperat\n",
            "ure), 73% \n",
            "(Precipitati\n",
            "on), 2.8% \n",
            "(Solar \n",
            "Radiation)  \n",
            "N-RMSE , \n",
            "1-3% \n",
            "MRE in all  \n",
            "categories  N/A (Sehgal, \n",
            "et al., \n",
            "2017)  \n",
            " 1 \n",
            "  2 53 \n",
            " Appendix  III: Publicly -available datasets related to agriculture . \n",
            "No. Organization/Dataset  Description of dataset  Source  \n",
            "1. Image -Net Dataset  Images of various p lants (trees, vegetables, flowers)  http://image -net.org/explore?wnid=n07707451   \n",
            "2. ImageNet Large Scale Visual \n",
            "Recognition Challenge (ILSVRC)  Images that allow object localization and detection  http://image -net.org/challenges/LSVRC/2017/#det   \n",
            "3. University of Arcansas , Plants  \n",
            "Dataset  Herbicide injury image d atabase  https://plants.uaex.edu/herbicide/   \n",
            "http://www.uaex.edu/yard -garden/resource -library/diseases/   \n",
            "4. EPFL,  Plant Village  Dataset  Images of various c rops and their diseases  https://www.plantvillage.org/en/crops   \n",
            "5. Leafsnap D ataset  Leaves from 185 tree species from the Northeastern \n",
            "United States . http://leafsnap.com/dataset/   \n",
            "6. LifeCLEF  Dataset  Identity, geographic distribution and uses of plants  http://www.imageclef.org/2014/lifeclef/plant   \n",
            "7. PASCAL Visual Object Classes \n",
            "Dataset  Images of various animals  (birds, cats, cow s, dog s, \n",
            "horse s, sheep  etc.) http://host.robots.ox.ac.uk/pascal/VOC/   \n",
            "8. Africa Soil Information Service \n",
            "(AFSIS) dataset  Continent -wide digital soil maps for sub -Saharan Africa  http://africasoils.net/services/data/   \n",
            "9. UC Merced Land Use Dataset  A 21 class land use image dataset  http://vision.ucmerced.edu/datasets /landuse.html   \n",
            "10. MalayaKew Dataset  Scan-like images of leaves from 44 species classes.  http://web.fsktm.um.edu.my/~cschan/downloads_MKLeaf_d\n",
            "ataset.html   \n",
            "11. Crop/Weed Field Image Dataset  Field images, vegetation segmentation masks and \n",
            "crop/weed plant type annotations . https://github.com/cwfid/dataset   \n",
            "https://pdfs.semanticscholar.org/58a0/9b1351ddb447e6abd\n",
            "ede7233a4794d538155.pdf   \n",
            "12. University of Bonn  \n",
            "Photogrammetry , IGG  Sugar beets d ataset  for plant classification as well as \n",
            "localization and mapping . http://www.ipb.uni -bonn.de/data/   \n",
            "13. Flavia leaf dataset  Leaf image s of 32 plant s. http://flavia.sourceforge.net/   \n",
            "14. Syngenta Crop Challenge 2017  2,267 of corn hybrids in 2,122 of locations between \n",
            "2008 and 2016 , together with weather and soil \n",
            "conditions  https://www.ideaconnection.com/syngenta -crop-\n",
            "challenge/challenge.php   \n",
            " 1 \n",
            "Deep learning review and discussion of its future \n",
            "development  \n",
            "Zhiying  Hao* \n",
            "University of Electronic Science and Technology of China, No.4, Block 2, North Jianshe Road, \n",
            "Chenghua District, Chengdu, Sichuan, China  \n",
            "Abstract . This paper is a summary of the algorithms for deep learning and \n",
            "a brief discussion of its future development. In the first part, the concept of \n",
            "deep learning and the advantages and disadvantages of deep learning are \n",
            "introduced. The second part demonstrates several algorithms for deep \n",
            "learning. The third part introduces the application areas of deep learning. \n",
            "Then combines the above algorithms and applications to explore the subsequent development of deep learning. The last part makes a summary \n",
            "of the full paper.  \n",
            "1 Introduction  \n",
            "As early as 1952, IBM's Arthur Samuel designed a program for learning checkers. It can \n",
            "build new models by observing the moves of the pieces and use them to improve their playing \n",
            "skills. In 1959, the concept of machine learning was proposed as a field of study that  could \n",
            "give a machine a certain skill without the need for deterministic programming. In the process \n",
            "of machine  learning development, various machine learning models have been proposed, \n",
            "including deep learning. Due to its complicated structure and the need  for a large amount of \n",
            "calculation, the computing cost is very high, so it had not been paid attention to at the \n",
            "beginning. However, with the great improvement in computer performance, the excellent \n",
            "performance of deep learning makes it rose rapidly and ha s become one of the hottest \n",
            "research areas. In this paper, the main deep learning models will be briefly summarized and \n",
            "the development prospects of deep learning will be analyzed and discussed at the end.  \n",
            "2 Introduction to deep learning  \n",
            "2.1 What is Deep L earning  \n",
            "Deep learning is a branch of machine learning  [1]. It is an algorithm that attempts to use the \n",
            "high-level abstraction of data using multiple processing layers consisting of complex \n",
            "structures or multiple nonlinear transforms. In machine learning, deep learning is an \n",
            "algorithm based on characterizing learning data. The concept of deep learning is relative to \n",
            "shallow learning. Shallow machine learning models such as Support Vector Machines and \n",
            "                                                      \n",
            "* Corresponding author: zhiyinghao@std.uestc.edu.cn  \n",
            "© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons \n",
            "Attribution License 4.0 (http://creativecommons.org/licenses/by/4.0/).MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "Logistic Regression were introduced in the 1990s. These shallo w machine learning models \n",
            "have only one layer or no hidden layer nodes , as shown in Fig  1. Deep learning is based on \n",
            "multiple hidden layer nodes. The essence of deep learning is multi- layer neural network. \n",
            "Deep learning uses the input of the previous layer as the output of t he next layer to learn \n",
            "highly abstract data features.  \n",
            " \n",
            "Fig. 1. A single-layer neural network  \n",
            "Like machine learning, deep learning can be categorized into supervised learning, semi -\n",
            "supervised learning, and unsupervised learning. At present, the classical deep learning \n",
            "framework includes Convolutional Neural Networks, Restricted Boltzmann Machines [2], \n",
            "Deep Belief Networks [3], and Generative Adversarial Networks [4]. In the next section, \n",
            "these algorithms will be introduced briefly.  \n",
            "2.2 Advantages and disad vantages of deep learning  \n",
            "Deep learning has shown better performance than traditional neural networks. After a deep neural network is trained and properly adjusted for certain task like image classification, it saves a lot of calculations, and can complete  a lot of work in a short time. . Deep learning is \n",
            "also malleable. Usually, for traditional algorithms, if you need to adjust the model, you may \n",
            "need to make copious changes to the code. For the determined network framework used for \n",
            "deep learning, if you n eed to adjust the model, you only need to adjust the parameters, thus \n",
            "deep learning has great flexibility. The deep learning framework can be continuously \n",
            "improved and then reached the almost perfect state. Deep learning is also more general, it \n",
            "can be mod elled based on problems, not limited to a fixed problem.  \n",
            "Deep learning has some shortcomings as well. First of all, its training cost is relatively \n",
            "high. Now, the performance of computer hardware has been improved a lot, and some simple \n",
            "neural networks can  be trained on some of the common computing modules. However, the \n",
            "training of some more complex neural networks still requires relatively expensive high -\n",
            "performance computing modules. Although the price of such modules has been greatly \n",
            "reduced compared wit h the previous ones, the demand for such hardware still makes the \n",
            "training cost of deep learning relatively high. At the same time, not only the economic cost, \n",
            "the training of neural networks requires a large amount of data to be trained to achieve a \n",
            "satisfactory level, but it is often difficult to obtain a sufficient amount of data. Secondly, deep \n",
            "learning can't directly learn knowledge. Although models such as AlphaGo Zero can learn without prior knowledge have emerged, most deep learning frameworks still  need to rely on \n",
            "manual feature marking for training. The workload is enormous for marking large -scale \n",
            "datasets, which also increases the training cost of deep learning. Another point is that deep \n",
            "learning lacks sufficient theoretical support. Although dee p learning has achieved good \n",
            "2\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "Logistic Regression were introduced in the 1990s. These shallo w machine learning models \n",
            "have only one layer or no hidden layer nodes , as shown in Fig  1. Deep learning is based on \n",
            "multiple hidden layer nodes. The essence of deep learning is multi -layer neural network. \n",
            "Deep learning uses the input of the previous layer as the output of t he next layer to learn \n",
            "highly abstract data features.  \n",
            " \n",
            "Fig. 1. A single -layer neural network  \n",
            "Like machine learning, deep learning can be categorized into supervised learning, semi -\n",
            "supervised learning, and unsupervised learning. At present, the classical deep learning \n",
            "framework includes Convolutional Neural Networks, Restricted Boltzmann Machines [2], \n",
            "Deep Belief Networks [3], and Generative Adversarial Networks [4]. In the next section, \n",
            "these algorithms will be introduced briefly.  \n",
            "2.2 Advantages and disad vantages of deep learning  \n",
            "Deep learning has shown better performance than traditional neural networks. After a deep \n",
            "neural network is trained and properly adjusted for certain task like image classification, it \n",
            "saves a lot of calculations, and can complete  a lot of work in a short time. . Deep learning is \n",
            "also malleable. Usually, for traditional algorithms, if you need to adjust the model, you may \n",
            "need to make copious changes to the code. For the determined network framework used for \n",
            "deep learning, if you n eed to adjust the model, you only need to adjust the parameters, thus \n",
            "deep learning has great flexibility. The deep learning framework can be continuously \n",
            "improved and then reached the almost perfect state. Deep learning is also more general, it \n",
            "can be mod elled based on problems, not limited to a fixed problem.  \n",
            "Deep learning has some shortcomings as well. First of all, its training cost is relatively \n",
            "high. Now, the performance of computer hardware has been improved a lot, and some simple \n",
            "neural networks can  be trained on some of the common computing modules. However, the \n",
            "training of some more complex neural networks still requires relatively expensive high -\n",
            "performance computing modules. Although the price of such modules has been greatly \n",
            "reduced compared wit h the previous ones, the demand for such hardware still makes the \n",
            "training cost of deep learning relatively high. At the same time, not only the economic cost, \n",
            "the training of neural networks requires a large amount of data to be trained to achieve a \n",
            "satisfactory level, but it is often difficult to obtain a sufficient amount of data. Secondly, deep \n",
            "learning can't directly learn knowledge. Although models such as AlphaGo Zero can learn \n",
            "without prior knowledge have emerged, most deep learning frameworks still  need to rely on \n",
            "manual feature marking for training. The workload is enormous for marking large -scale \n",
            "datasets, which also increases the training cost of deep learning. Another point is that deep \n",
            "learning lacks sufficient theoretical support. Although dee p learning has achieved good  \n",
            " \n",
            " \n",
            "results in various application fields, there is still no complete and rigorous theoretical \n",
            "derivation to explain the deep learning model at this stage, which limits the follow -up study \n",
            "and the improvement of deep learning.  \n",
            "3 Ma in Deep Learning Algorithm Introduction  \n",
            "3.1 Convolutional Neural Network  \n",
            "The convolutional neural network , as seen in Fig 2,  is a feedforward neural network whose \n",
            "convolution operation allows its neurons to cover peripheral units within the convolution \n",
            "kernel and has excelle nt performance in large image processing. A convolutional neural \n",
            "network typically consists of one or more convolutional layers and a fully connected layer, \n",
            "which also includes a pooling layer for integration. Convolutional neural networks give better \n",
            "results in terms of image and speech recognition. It requires fewer parameters to consider \n",
            "than other deep neural networks. The advantages of convolutional neural networks make it \n",
            "one of the most commonly used deep learning models. The basic structure of the \n",
            "convolutional neural network is briefly introduced below.  \n",
            " \n",
            "Fig. 2. Convolutional Neural Network, LeNet -5[5]  \n",
            "3.1.1 Convolutional layer.  \n",
            "The convolutional neural network convolves data using multiple convolution kernels in the \n",
            "convolutional layer to generate a plurality of feature maps corresponding to the convolution \n",
            "kernel.  \n",
            "The convolution operation has the following advantages:  \n",
            "1. The weight sharing mechanism on the same feature map reduces the number of \n",
            "parameters;  \n",
            "2. Local connectivity enables co nvolutional neural networks to take into account the \n",
            "characteristics of adjacent pixels when processing images;  \n",
            "3. There is no object in the image recognition due to the position of the object on the \n",
            "image.  \n",
            "These advantages also make it possible to use a c onvolutional layer instead of a fully \n",
            "connected layer in some models to speed up the training process.  \n",
            "3.1.2 Pooling layer  \n",
            "After obtaining the features by convolution, we hope to use these features to do the \n",
            "classification. However, the amount of data that  is often obtained is very large, and it is prone \n",
            "to over -fitting. Therefore, we aggregate statistics on features at different locations. This \n",
            "aggregation operation is called pooling. In the convolutional neural network, the pooling \n",
            "3\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "layer is used for featu re filtering after image convolution to improve the operability of the \n",
            "classification.  \n",
            "3.1.3 Fully connected layer  \n",
            "After pooling layer is the fully connected layer, its role is to pull the feature map into a one -\n",
            "dimensional vector. The working mode of the  fully connected layer is similar to that of a \n",
            "traditional neural network. The fully connected layer contains parameters in approximately \n",
            "90% of the convolutional neural network, which allows us to map the neural network forward into a vector of fixed leng th. We can grant this vector to a particular image class or use it as \n",
            "a feature vector in subsequent processes.  \n",
            "3.2 Deep Belief Network  \n",
            "The deep belief network is a probability generation model. Compared with the neural network \n",
            "which is a traditional discr iminative model, the generated model is to establish a joint \n",
            "distribution between observation data and labels, and to evaluate both P (Observation|Label) \n",
            "and P (Label|Observation) while the discriminative model has only evaluated the latter, that \n",
            "is, P (La bel|Observation).  \n",
            "The deep confidence network consists of multiple restricted Boltzmann layers, a typical \n",
            "neural network type as shown. These networks are \"restricted\" to a visible layer and a hidden \n",
            "layer, with connections between the layers, but there ar e no connections between the cells \n",
            "within the layer. The hidden layer unit is trained to capture the correlation of higher order \n",
            "data represented in the visible layer.  \n",
            "3.3 Restricted Boltzmann Machine  \n",
            "A Restricted Boltzmann Machine is a randomly generated neural network that can learn the \n",
            "probability distribution through the input data set. It is a Boltzmann Machine's problem, but \n",
            "the qualified model must be a bipartite graph. The model contains visible cells corresponding to the input parameters and hidden  cells corresponding to the training results. Each edge of \n",
            "the figure must be connected to a visible unit and a hidden unit. In contrast, the Boltzmann machine (unrestricted) contains the edges between hidden cells, making it a recurrent neural \n",
            "network. Th is limitation of the constrained Boltzmann machine makes it possible to have a \n",
            "more efficient training algorithm than the general Boltzmann machine, especially for the \n",
            "gradient divergence algorithm.  \n",
            "The Boltzmann machine and its model have been successfull y applied to tasks such as \n",
            "collaborative filtering, classification, dimensionality reduction, image retrieval, information \n",
            "retrieval, language processing, automatic speech recognition, time series modeling, and \n",
            "information processing. Restricted Boltzmann machines have been used in dimensionality \n",
            "reduction, classification, collaborative filtering, feature learning, and topic modeling. \n",
            "Depending on the task, the restricted Boltzmann machine can be trained using supervised \n",
            "learning or unsupervised learning.  \n",
            "3.4 Generative Adversarial Network  \n",
            "The Generated Adversarial Network was proposed in 2014. The Generative Adversarial \n",
            "Network uses two models, a generative model and a discriminative model. The discriminative model determines whether the given picture is a real picture, and the \n",
            "generative model creates a picture as close to the ground truth as possible. The generated \n",
            "model is designed to generate a picture that can spoof the discriminative model, and the \n",
            "discriminative model distinguishes the picture generat ed by the generated model from the \n",
            "4\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "layer is used for featu re filtering after image convolution to improve the operability of the \n",
            "classification.  \n",
            "3.1.3 Fully connected layer  \n",
            "After pooling layer is the fully connected layer, its role is to pull the feature map into a one -\n",
            "dimensional vector. The working mode of the  fully connected layer is similar to that of a \n",
            "traditional neural network. The fully connected layer contains parameters in approximately \n",
            "90% of the convolutional neural network, which allows us to map the neural network forward \n",
            "into a vector of fixed leng th. We can grant this vector to a particular image class or use it as \n",
            "a feature vector in subsequent processes.  \n",
            "3.2 Deep Belief Network  \n",
            "The deep belief network is a probability generation model. Compared with the neural network \n",
            "which is a traditional discr iminative model, the generated model is to establish a joint \n",
            "distribution between observation data and labels, and to evaluate both P (Observation|Label) \n",
            "and P (Label|Observation) while the discriminative model has only evaluated the latter, that \n",
            "is, P (La bel|Observation).  \n",
            "The deep confidence network consists of multiple restricted Boltzmann layers, a typical \n",
            "neural network type as shown. These networks are \"restricted\" to a visible layer and a hidden \n",
            "layer, with connections between the layers, but there ar e no connections between the cells \n",
            "within the layer. The hidden layer unit is trained to capture the correlation of higher order \n",
            "data represented in the visible layer.  \n",
            "3.3 Restricted Boltzmann Machine  \n",
            "A Restricted Boltzmann Machine is a randomly generated neural network that can learn the \n",
            "probability distribution through the input data set. It is a Boltzmann Machine's problem, but \n",
            "the qualified model must be a bipartite graph. The model contains visible cells corresponding \n",
            "to the input parameters and hidden  cells corresponding to the training results. Each edge of \n",
            "the figure must be connected to a visible unit and a hidden unit. In contrast, the Boltzmann \n",
            "machine (unrestricted) contains the edges between hidden cells, making it a recurrent neural \n",
            "network. Th is limitation of the constrained Boltzmann machine makes it possible to have a \n",
            "more efficient training algorithm than the general Boltzmann machine, especially for the \n",
            "gradient divergence algorithm.  \n",
            "The Boltzmann machine and its model have been successfull y applied to tasks such as \n",
            "collaborative filtering, classification, dimensionality reduction, image retrieval, information \n",
            "retrieval, language processing, automatic speech recognition, time series modeling, and \n",
            "information processing. Restricted Boltzmann machines have been used in dimensionality \n",
            "reduction, classification, collaborative filtering, feature learning, and topic modeling. \n",
            "Depending on the task, the restricted Boltzmann machine can be trained using supervised \n",
            "learning or unsupervised learning.  \n",
            "3.4 Generative Adversarial Network  \n",
            "The Generated Adversarial Network was proposed in 2014. The Generative Adversarial \n",
            "Network uses two models, a generative model and a discriminative model. The \n",
            "discriminative model determines whether the given picture is a real picture, and the \n",
            "generative model creates a picture as close to the ground truth as possible. The generated \n",
            "model is designed to generate a picture that can spoof the discriminative model, and the \n",
            "discriminative model distinguishes the picture generat ed by the generated model from the  \n",
            " \n",
            " \n",
            "real picture. The two models are trained at the same time, and the performance of the two \n",
            "models becomes stronger and stronger in the confrontation process between the two models, \n",
            "and will eventually reach a steady state.  \n",
            "The use of generating a network is very versatile, not only for the generation and \n",
            "discrimination of images, but also for other kinds of data.  \n",
            "4 Deep Learning Application  \n",
            "4.1 Image processing  \n",
            "Manually selecting features is a very laborious approach. Its adjustment takes a lot of time. \n",
            "Due to the instability of manual selection, we consider letting the computer automatically \n",
            "learn the features. The automatic learning of the computer can be realized by deep learning.  \n",
            "In image recognition, deep learning util izes patterns of multi -layer neural networks to \n",
            "pre-process, feature extract, and feature processing images.  \n",
            "Taking the convolutional neural network as an example, the convolutional neural network \n",
            "establishes a multi -layer neural network, and uses the conv olutional layer to perform \n",
            "convolution operations to extract feature values, and then performs data processing and \n",
            "training through the pooling layer and the fully connected layer. The detailed process is \n",
            "explained in detail in the Technical Implementation  section of 2.2 Neural Network below.  \n",
            "Although neural network image recognition can't reach the accuracy of the human eye at \n",
            "present, the neural network can process a large amount of image data, and the efficiency is much better than manual recognition. Fa cing huge amount of data that cannot be processed \n",
            "manually, using the neural network method will lead to magnificent improvement.  \n",
            "In addition, deep learning provides an idea for face recognition technology. Face \n",
            "recognition is a biometric recognition tech nology based on human facial feature information \n",
            "for identification. Face recognition products have been widely used in finance, justice, \n",
            "military, public security, border inspection, government, aerospace, electric power, factories, \n",
            "education, medical car e and many enterprises and institutions. And with the further maturity \n",
            "of technology and the improvement of social recognition, face recognition technology will be applied in more fields and has an expectable development prospects. The characteristics \n",
            "of the neural network make it possible to avoid overly complex feature extraction when \n",
            "applied to face recognition, which is beneficial to hardware implementation.  \n",
            "4.2 Audio data processing  \n",
            "Deep learning has a profound impact on speech processing. Almost every  solution in the field \n",
            "of speech recognition may contain one or more embedding algorithms based on neural \n",
            "models.  \n",
            "Speech recognition is basically divided into three main parts, namely signal level, noise \n",
            "level and language level. The signal level extracts the speech signal and enhances the signal, \n",
            "or performs appropriate pre -processing, cleaning, and feature extraction. The noise level \n",
            "divides the different features into different sounds. The language level combines the sounds \n",
            "into words and then combines t hem into sentences.  \n",
            "In the signal level, there are different techniques for extracting and enhancing the speech \n",
            "itself from the signal based on the neural model. At the same time, it is able to replace the \n",
            "classical feature extraction method with a more co mplex and efficient neural network -based \n",
            "method, which greatly improves the efficiency and accuracy. Noise and language levels also \n",
            "include a variety of different depth learning techniques, and different types of neural model -\n",
            "based architectures are used i n both sound level classification and language level \n",
            "classification.  \n",
            "5\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "5 Discussion of the future development of deep learning  \n",
            "5.1 Representation Learning  \n",
            "The core of deep learning is the abstraction and understanding of features. Therefore, feature \n",
            "learning  plays a very important role in deep learning. Since the essence of deep learning is a \n",
            "multi -layer neural network, some useful information is lost in the process of extracting \n",
            "features and transmitting to the lower layer. However, if too much image feature s are \n",
            "extracted, it may lead to over -fitting. Therefore, the study of representational learning may \n",
            "be one of the core research issues in deep learning research studying how to accurately extract \n",
            "the required features while avoiding over -fitting. The resea rch progress on this problem will \n",
            "be of great help to the classification and generalization task of neural networks.  \n",
            "5.2 Unsupervised Learning  \n",
            "As mentioned above, training a supervised neural network requires a large amount of labelled \n",
            "data. The workload i s very large, adding a lot of extra cost to the training of the neural \n",
            "network. Therefore, if the machine completes the work instead of human, the cost of network \n",
            "training will be greatly reduced. Unsupervised learning can also be used not only for the classification of markers, but also for the Go evaluation program such as AlphaGo Zero. The \n",
            "emergence of AlphaGo Zero proves that in some applications, even without the foundation of human prior knowledge, machines can achieve excellent training results. In t his case, the \n",
            "application of unsupervised learning in some areas to automate the learning of the machine \n",
            "from the human knowledge base without the limitations of current state of the art, may \n",
            "contribute to the updating and breakthrough of technology in the se fields. Moreover, the \n",
            "current research on unsupervised learning is not intensive now  while most people's research \n",
            "focuses on supervised learning. Therefore, unsupervised learning has a rich research potential. In fact, unsupervised learning has recently  become one of the hottest research areas. In my \n",
            "opinion, unsupervised learning is also one of the most valuable directions for deep learning in the future.  \n",
            "5.3 Theory Complement  \n",
            "One of the shortcomings of deep learning is that there is no complete theoret ical support, \n",
            "which brings a lot of controversy to deep learning and the lack of theory does hinder the \n",
            "development of deep learning. With the increasing attention of deep learning this year, the \n",
            "research on deep learning has become more and more in -depth,  and the theory of deep \n",
            "learning is constantly improving.     \n",
            "Nevertheless, the theory behind it is still not enough to rigorously prove the inner \n",
            "principles of deep learning. At present, the research relies on part of the theory combined with the actual t est and the experiment -based research method. While the theory can't get \n",
            "further breakthroughs, it only depends on adjusting parameters to improve the models \n",
            "performance, which may easily lead to the bottleneck of the research . \n",
            "Therefore, it seems that for  the future development of deep learning, it is very important \n",
            "to obtain complete theoretical support. In the process of research, the theory of deep learning \n",
            "needs to be continuously improved, and finally reach a level sufficient to explain the structure \n",
            "of the inner principle.  \n",
            "5.4 Perspective of Deep Learning Application  \n",
            "6\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018 \n",
            " \n",
            " \n",
            "5 Discussion of the future development of deep learning  \n",
            "5.1 Representation Learning  \n",
            "The core of deep learning is the abstraction and understanding of features. Therefore, feature \n",
            "learning  plays a very important role in deep learning. Since the essence of deep learning is a \n",
            "multi -layer neural network, some useful information is lost in the process of extracting \n",
            "features and transmitting to the lower layer. However, if too much image feature s are \n",
            "extracted, it may lead to over -fitting. Therefore, the study of representational learning may \n",
            "be one of the core research issues in deep learning research studying how to accurately extract \n",
            "the required features while avoiding over -fitting. The resea rch progress on this problem will \n",
            "be of great help to the classification and generalization task of neural networks.  \n",
            "5.2 Unsupervised Learning  \n",
            "As mentioned above, training a supervised neural network requires a large amount of labelled \n",
            "data. The workload i s very large, adding a lot of extra cost to the training of the neural \n",
            "network. Therefore, if the machine completes the work instead of human, the cost of network \n",
            "training will be greatly reduced. Unsupervised learning can also be used not only for the \n",
            "classification of markers, but also for the Go evaluation program such as AlphaGo Zero. The \n",
            "emergence of AlphaGo Zero proves that in some applications, even without the foundation \n",
            "of human prior knowledge, machines can achieve excellent training results. In t his case, the \n",
            "application of unsupervised learning in some areas to automate the learning of the machine \n",
            "from the human knowledge base without the limitations of current state of the art, may \n",
            "contribute to the updating and breakthrough of technology in the se fields. Moreover, the \n",
            "current research on unsupervised learning is not intensive now  while most people's research \n",
            "focuses on supervised learning. Therefore, unsupervised learning has a rich research potential. \n",
            "In fact, unsupervised learning has recently  become one of the hottest research areas. In my \n",
            "opinion, unsupervised learning is also one of the most valuable directions for deep learning \n",
            "in the future.  \n",
            "5.3 Theory Complement  \n",
            "One of the shortcomings of deep learning is that there is no complete theoret ical support, \n",
            "which brings a lot of controversy to deep learning and the lack of theory does hinder the \n",
            "development of deep learning. With the increasing attention of deep learning this year, the \n",
            "research on deep learning has become more and more in -depth,  and the theory of deep \n",
            "learning is constantly improving.     \n",
            "Nevertheless, the theory behind it is still not enough to rigorously prove the inner \n",
            "principles of deep learning. At present, the research relies on part of the theory combined \n",
            "with the actual t est and the experiment -based research method. While the theory can't get \n",
            "further breakthroughs, it only depends on adjusting parameters to improve the models \n",
            "performance, which may easily lead to the bottleneck of the research . \n",
            "Therefore, it seems that for  the future development of deep learning, it is very important \n",
            "to obtain complete theoretical support. In the process of research, the theory of deep learning \n",
            "needs to be continuously improved, and finally reach a level sufficient to explain the structure \n",
            "of the inner principle.  \n",
            "5.4 Perspective of Deep Learning Application   \n",
            " \n",
            " \n",
            "The fourth part of this paper referred to two application areas of deep learning, image \n",
            "recognition and speech processing, which are the two main application areas of deep learning. \n",
            "In ad dition, recent deep learning has also been used in natural language processing.  \n",
            "Specific applications include, for example, autonomous driving, intelligent dialogue \n",
            "robots such as Siri, image classification, medical image processing, etc. Different deep learning frameworks tend to have slightly different application scenarios, such as \n",
            "convolutional neural networks, which are mainly used for image processing. In the work of \n",
            "medical image processing, for example, the use of convolutional neural networks for b rain \n",
            "tumour segmentation has achieved an accuracy of more than 90%. Also, in medical \n",
            "applications, convolutional neural networks can be used to recognize Alzheimer's disease \n",
            "brain image, and more accurate diagnostic results can be obtained combined with ma nual \n",
            "judgement. Medicine is only a part of deep learning applications. In the application of deep \n",
            "learning, well -trained machines can often calculate some details that are hard to be obtained \n",
            "by human, saving people's workload and improving the quality of results. For example, in \n",
            "order to prevent the traffic light violation at the intersection, the usual way is to take picture \n",
            "by the camera and then manually recognize the license plate to give subsequent punishment. \n",
            "Manually viewing the images and recording  them is a very boring job and is not so efficient. \n",
            "If the captured image is recognized by the deep neural network, and the license plate number \n",
            "entry system is automatically extracted, it not only saves manpower, but also improves the \n",
            "efficiency greatly.  \n",
            "And so on, I believe that deep learning applications will be developed in the future in \n",
            "transportation, medical, language, automation, etc., not to mention examples here. Although it seems that it can't completely replace human work now, deep learning and artificial \n",
            "combination can greatly improve the work efficiency.  \n",
            "6 Conclusion  \n",
            "In this paper, we introduced some of the main algorithms and some conjectures for future development of deep learning. Deep learning has already had in -depth research and a wide \n",
            "range of application scenarios, and has been put into practical use in real life with excellent performance. However, there’s still a lot to exploit in the area of deep learning and neural \n",
            "network and it has great follow -up research space and considerable application potential.  \n",
            "References  \n",
            "1. Guo Y , Liu Y , Oerlemans A , et al. Deep learning for visual understanding: A review[J]. \n",
            "Neurocomputing, 2016, 187(C):27 -48. \n",
            "2. Chun -Xia Z , Nan -Nan J I , Guan -Wei W . Restricted Boltzmann Machines[J]. Chinese \n",
            "Journal of Engineering Mathematics, 2015.  \n",
            "3. Hinton G E , Osindero S , Teh Y W . A Fast Learning Algorithm for Deep Belief Nets[J]. \n",
            "Neural Computation, 2014, 18(7):1527 -1554.  \n",
            "4. Goodfellow I J , Pouget -Abadie J , Mirza M , et al. Generative Adversarial Networks[J]. \n",
            "Advances in Neural Information Processing Systems, 2014, 3:2672 -2680.  \n",
            "5. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. (1998). “Gradient based learning \n",
            "applied to  document recognition”. Proceedings of the IEEE, 86(11):2278 –2324.  \n",
            "7\n",
            "MATEC Web of Conferences 277, 02035 (2019)  https://doi.org/10.1051/matecconf/201927702035\n",
            "JCMME 2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.create_documents([all_text])"
      ],
      "metadata": {
        "id": "XCor_MZ-8Sev"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "directory = 'index_store'\n",
        "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
        "vector_index.save_local(directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze6ccED28dHN",
        "outputId": "6823f9ee-1243-4e27-93be-1f11fa2aa2cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
        "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
        "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(),\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever,\n",
        "                                           return_source_documents=True\n",
        "                                           )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K931Zna48hOo",
        "outputId": "bb5a9a6b-ee53-42de-c01c-236c14525a46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question):\n",
        "  response_object = qa_interface(question)\n",
        "  question = response_object['query']\n",
        "  response = response_object['result']\n",
        "  # print(f\"Query: {question}\")\n",
        "  # print(f\"Response: {response}\")\n",
        "  return response\n",
        "query = 'Summarize this research paper'\n",
        "get_response(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "CCiXfR6j8k_Y",
        "outputId": "41bff664-4375-4914-a52e-9491a3edbcee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This research paper focuses on the application of Deep Learning (DL) in agriculture to address various agricultural problems, such as food security, sustainability, productivity, and environmental impact. The study collected and analyzed research papers published between 2016 and early 2022 from various sources like Research Gate, IEEE Explore, Springer, Elsevier, Google Scholar, Frontier, and Science Direct. The paper discusses the agricultural challenges, the use of DL-based models, data sources, classes and labels modeled, data preprocessing techniques, and overall model performance metrics. The research highlights the increasing popularity and potential of DL in agriculture, with a focus on image classification, obstacle detection, fruit counting, crop prediction, and other agricultural applications. The study also emphasizes the importance of using DL tools for model development in the agriculture sector.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompts and Responses**\n",
        "\n",
        "The section below shows how we can use this to answer questions regarding items that are within the text. When we ask about a random question about a budget speech, you will see the LLM respons that it does not have the ability to create a summary of a specific budget speech."
      ],
      "metadata": {
        "id": "Ud1IzKlg8oi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"What are the two key factors of reward functions?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QSlkw20i8uOU",
        "outputId": "943ebdcd-4bf1-450f-ce6d-d5c07658d085"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The two key factors of reward functions are exploration and exploitation. Balancing these factors is crucial in reinforcement learning to maximize long-term rewards and effectively learn a policy model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"What is Multi task learning?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lu8p259E8wTk",
        "outputId": "7742b5c8-48fa-4f30-fddf-d1c1605f0037"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-task learning is a machine learning approach where a model is trained to perform multiple tasks simultaneously, rather than training separate models for each task. This approach aims to improve the performance of each individual task by leveraging the shared knowledge and relationships between them during training. It allows the model to generalize better and learn more efficiently, especially when the tasks are related or have overlapping features.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = ''' Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias\n",
        "for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users’ viewership\n",
        "patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings.\n",
        "In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias\n",
        "in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation\n",
        "through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we\n",
        "demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed\n",
        "techniques. Noticeably, we show improved relative gain of up to 65.27% in PR-AUC metric. A case study is presented to demonstrate\n",
        "the advantages of our methods in attenuating the popularity bias of global items'''\n",
        "get_response(Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "NslnMI5k827k",
        "outputId": "417d694d-2457-4f41-b86f-4dac0a1fb6d5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper proposes a multi-task learning (MTL) technique along with an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. They aim to address data imbalances that arise in a personalized recommender system across different geographic locations. By enriching training examples with active user representations through upsampling and leveraging MTL to learn geographic-based user embeddings, the framework shows improved relative gains of up to 65.27% in PR-AUC metric compared to a baseline that does not incorporate these techniques. The study demonstrates how their methods effectively attenuate the popularity bias of globally prevalent items in multi-territory recommendations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conlusion**\n",
        "In this portion of the assignment, I succesfully used the langchain model to perform RAG on 4 research papers."
      ],
      "metadata": {
        "id": "BanyKEII89Oi"
      }
    }
  ]
}